{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaldeo/CS598_DLH_Project/blob/CNN_DDI/Pytorch_CNN_DDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(1)\n"
      ],
      "metadata": {
        "id": "KoIAAbuFhrs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EVENT_NUM = 65\n",
        "DROPRATE = 0.3\n",
        "VECTOR_SIZE = 572\n"
      ],
      "metadata": {
        "id": "EjUb60oPjzpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, vector_size=VECTOR_SIZE, event_num=EVENT_NUM, droprate=DROPRATE):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(vector_size * 2, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.dropout1 = nn.Dropout(droprate)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.dropout2 = nn.Dropout(droprate)\n",
        "        self.fc3 = nn.Linear(256, event_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(torch.rel(self.fc1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.bn2(torch.relu(self.fc2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3dZ0MtNSh_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_DDI(nn.Module):\n",
        "    def __init__(self, vector_size=VECTOR_SIZE, event_num=EVENT_NUM, droprate=DROPRATE):\n",
        "        super(CNN_DDI, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(vector_size*2, 64, kernel_size=3, padding='same')\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding='same')\n",
        "        self.conv3_1 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
        "        self.conv3_2 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
        "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, padding='same')\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(256 * vector_size, 267)  # Assuming the input length is 572\n",
        "        self.fc2 = nn.Linear(267, event_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lrelu(self.conv1(x))\n",
        "        x = self.lrelu(self.conv2(x))\n",
        "        residual = x\n",
        "        x = self.lrelu(self.conv3_1(x))\n",
        "        x = self.lrelu(self.conv3_2(x))\n",
        "        x += residual\n",
        "        x = self.lrelu(self.conv4(x))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "vJsafpx9jPGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming 'features' and 'labels' are your data and labels as numpy arrays\n",
        "# Convert data to PyTorch tensors\n",
        "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Create a dataset and data loader for batching\n",
        "dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train, y_train, X_val, y_val are your data in NumPy arrays\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n"
      ],
      "metadata": {
        "id": "UTGwq4GNmb9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DNN(VECTOR_SIZE*2, EVENT_NUM, DROPRATE)\n",
        "# For CNN model, use:\n",
        "# model = CNN_DDI(VECTOR_SIZE*2, EVENT_NUM, DROPRATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "lP2tToEVnuB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print(f'Validation accuracy: {100 * correct / total}%')\n"
      ],
      "metadata": {
        "id": "_l23gtBLnwTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, epochs=100, criterion, optimizer, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Nk6G8utoqax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from NLPProcess import NLPProcess\n",
        "import csv\n",
        "import sqlite3\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import auc, roc_auc_score, accuracy_score, recall_score, f1_score, precision_score, precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from numpy.random import seed\n",
        "from tqdm import tqdm\n",
        "\n",
        "random_seed=1\n",
        "seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "event_num = 65\n",
        "droprate = 0.3\n",
        "vector_size = 572\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "CV = 2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(vector_size * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(droprate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(droprate),\n",
        "            nn.Linear(256, event_num),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "\n",
        "class CNN_DDI(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_DDI, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(2, 64, kernel_size=3, padding='same')\n",
        "        self.lrelu = nn.LeakyReLU(0.2)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding='same')\n",
        "        self.conv3_1 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
        "        self.conv3_2 = nn.Conv1d(128, 128, kernel_size=3, padding='same')\n",
        "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, padding='same')\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(256 * vector_size, 267)\n",
        "        self.fc2 = nn.Linear(267, event_num)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.lrelu(self.conv1(x))\n",
        "        conv2_out = self.lrelu(self.conv2(x))\n",
        "        x = self.lrelu(self.conv3_1(conv2_out))\n",
        "        x = self.lrelu(self.conv3_2(x))\n",
        "        x += conv2_out  # Adding the residual connection\n",
        "        x = self.lrelu(self.conv4(x))\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = x.transpose(1, 2)\n",
        "    #     x = self.lrelu(self.conv1(x))\n",
        "    #     x = self.lrelu(self.conv2(x))\n",
        "    #     residual = x\n",
        "    #     x = self.lrelu(self.conv3_1(x))\n",
        "    #     x = self.lrelu(self.conv3_2(x))\n",
        "    #     x += residual\n",
        "    #     x = self.lrelu(self.conv4(x))\n",
        "    #     x = self.flatten(x)\n",
        "    #     x = torch.relu(self.fc1(x))\n",
        "    #     x = self.softmax(self.fc2(x))\n",
        "    #     return x\n",
        "\n",
        "\n",
        "def prepare(df_drug, feature_list, vector_size,mechanism,action,drugA,drugB):\n",
        "    d_label = {}\n",
        "    d_feature = {}\n",
        "    # Transfrom the interaction event to number\n",
        "    # Splice the features\n",
        "    d_event=[]\n",
        "    for i in range(len(mechanism)):\n",
        "        d_event.append(mechanism[i]+\" \"+action[i])\n",
        "    label_value = 0\n",
        "    count={}\n",
        "    for i in d_event:\n",
        "        if i in count:\n",
        "            count[i]+=1\n",
        "        else:\n",
        "            count[i]=1\n",
        "    list1 = sorted(count.items(), key=lambda x: x[1],reverse=True)\n",
        "    for i in range(len(list1)):\n",
        "        d_label[list1[i][0]]=i\n",
        "    vector = np.zeros((len(np.array(df_drug['name']).tolist()), 0), dtype=float)\n",
        "    for i in feature_list:\n",
        "        vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))\n",
        "    # Transfrom the drug ID to feature vector\n",
        "    for i in range(len(np.array(df_drug['name']).tolist())):\n",
        "        d_feature[np.array(df_drug['name']).tolist()[i]] = vector[i]\n",
        "    # Use the dictionary to obtain feature vector and label\n",
        "    new_feature = []\n",
        "    new_label = []\n",
        "    name_to_id = {}\n",
        "    for i in range(len(d_event)):\n",
        "        new_feature.append(np.hstack((d_feature[drugA[i]], d_feature[drugB[i]])))\n",
        "        new_label.append(d_label[d_event[i]])\n",
        "    new_feature = np.array(new_feature)\n",
        "    new_label = np.array(new_label)\n",
        "    return (new_feature, new_label, event_num)\n",
        "\n",
        "\n",
        "def feature_vector(feature_name, df, vector_size):\n",
        "    # df are the 572 kinds of drugs\n",
        "    # Jaccard Similarity\n",
        "    def Jaccard(matrix):\n",
        "        matrix = np.mat(matrix)\n",
        "        numerator = matrix * matrix.T\n",
        "        denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n",
        "        return numerator / denominator\n",
        "\n",
        "    all_feature = []\n",
        "    drug_list = np.array(df[feature_name]).tolist()\n",
        "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
        "    for i in drug_list:\n",
        "        for each_feature in i.split('|'):\n",
        "            if each_feature not in all_feature:\n",
        "                all_feature.append(each_feature)  # obtain all the features\n",
        "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
        "    df_feature = DataFrame(feature_matrix, columns=all_feature)  # Consrtuct feature matrices with key of dataframe\n",
        "    for i in range(len(drug_list)):\n",
        "        for each_feature in df[feature_name].iloc[i].split('|'):\n",
        "            df_feature[each_feature].iloc[i] = 1\n",
        "    sim_matrix = Jaccard(np.array(df_feature))\n",
        "\n",
        "    sim_matrix1 = np.array(sim_matrix)\n",
        "    count = 0\n",
        "    pca = PCA(n_components=vector_size)  # PCA dimension\n",
        "    sim_matrix = np.asarray(sim_matrix)\n",
        "    pca.fit(sim_matrix)\n",
        "    sim_matrix = pca.transform(sim_matrix)\n",
        "    return sim_matrix\n",
        "\n",
        "\n",
        "def get_index(label_matrix, event_num, seed, CV):\n",
        "    index_all_class = np.zeros(len(label_matrix))\n",
        "    for j in range(event_num):\n",
        "        index = np.where(label_matrix == j)\n",
        "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
        "        k_num = 0\n",
        "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
        "            index_all_class[index[0][test_index]] = k_num\n",
        "            k_num += 1\n",
        "\n",
        "    return index_all_class\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs, criterion, optimizer, device, accumulation_steps=1, patience=10):\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    # best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    epochs_no_improve = 0\n",
        "    early_stop = False\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels) / accumulation_steps  # Normalize loss to account for accumulation\n",
        "            loss.backward()  # Accumulate gradients\n",
        "\n",
        "            # optimizer.step()\n",
        "            running_loss += loss.item() * accumulation_steps  # Correct loss scaling after normalization\n",
        "            # del inputs, labels, outputs, loss  # Free up memory\n",
        "            # torch.cuda.empty_cache()  # Clear memory cache\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                optimizer.step()  # Perform optimization step after accumulating gradients\n",
        "                optimizer.zero_grad()  # Clear gradients after optimization step\n",
        "\n",
        "        # Perform optimization step if there are any unflushed gradients (for cases where dataset size is not divisible by accumulation_steps)\n",
        "        if len(train_loader) % accumulation_steps != 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}')\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pred_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                pred_list.append(outputs.cpu().numpy())\n",
        "\n",
        "                # Apply torch.max() along dimension 1 to get predicted class indices\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                # Since labels are one-hot encoded, convert them to class indices as well\n",
        "                _, labels_indices = torch.max(labels, 1)\n",
        "\n",
        "                total += labels.size(0)\n",
        "                # print(predicted.shape)\n",
        "                # print(labels.shape)\n",
        "                # print(total)\n",
        "                correct += (predicted == labels_indices).sum().item()\n",
        "\n",
        "            #     del inputs, labels, outputs  # Free up memory\n",
        "            # torch.cuda.empty_cache()  # Clear memory cache\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f'Validation loss: {val_loss}, accuracy: {100 * correct / total}%')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f'Early stopping triggered after {epoch+1} epochs.')\n",
        "                early_stop = True\n",
        "                break\n",
        "\n",
        "    # Load best model weights\n",
        "    # model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return np.concatenate(pred_list, axis=0)\n",
        "\n",
        "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    y_score = np.zeros((0, event_num), dtype=float)\n",
        "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
        "    matrix = []\n",
        "    if type(feature_matrix) != list:\n",
        "        matrix.append(feature_matrix)\n",
        "        feature_matrix = matrix\n",
        "    for k in range(CV):\n",
        "        train_index = np.where(index_all_class != k)\n",
        "        test_index = np.where(index_all_class == k)\n",
        "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
        "        print(len(feature_matrix))\n",
        "        for i in range(len(feature_matrix)):\n",
        "            x_train = feature_matrix[i][train_index]\n",
        "            x_test = feature_matrix[i][test_index]\n",
        "            y_train = label_matrix[train_index]\n",
        "            y_test = label_matrix[test_index]\n",
        "            y_train_one_hot = torch.nn.functional.one_hot(torch.tensor(y_train), num_classes=event_num).float()\n",
        "            y_test_one_hot = torch.nn.functional.one_hot(torch.tensor(y_test), num_classes=event_num).float()\n",
        "            if clf_type == 'DDIMDL':\n",
        "                model = DNN()\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters())\n",
        "                train_dataset = TensorDataset(torch.tensor(x_train).float(), y_train_one_hot)\n",
        "                test_dataset = TensorDataset(torch.tensor(x_test).float(), y_test_one_hot)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "                pred += train_model(model, train_loader, test_loader, NUM_EPOCHS, criterion, optimizer, device)\n",
        "                continue\n",
        "            elif clf_type == 'CNN_DDI':\n",
        "                x_train_reshaped = x_train.reshape(-1, 572, 2)\n",
        "                x_test_reshaped = x_test.reshape(-1, 572, 2)\n",
        "                model = CNN_DDI()\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "                print(x_train_reshaped.shape)\n",
        "                print(x_test_reshaped.shape)\n",
        "                print(y_train_one_hot.shape, y_test_one_hot.shape)\n",
        "                print(model)\n",
        "                train_dataset = TensorDataset(torch.tensor(x_train_reshaped).float(), y_train_one_hot)\n",
        "                test_dataset = TensorDataset(torch.tensor(x_test_reshaped).float(), y_test_one_hot)\n",
        "                train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "                test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "                pred += train_model(model, train_loader, test_loader, NUM_EPOCHS, criterion, optimizer, device)\n",
        "                continue\n",
        "            elif clf_type == 'RF':\n",
        "                clf = RandomForestClassifier(n_estimators=100)\n",
        "            elif clf_type == 'GBDT':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'SVM':\n",
        "                clf = SVC(probability=True)\n",
        "            elif clf_type == 'FM':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'KNN':\n",
        "                clf = KNeighborsClassifier(n_neighbors=4)\n",
        "            else:\n",
        "                clf = LogisticRegression()\n",
        "            clf.fit(x_train, y_train)\n",
        "            pred += clf.predict_proba(x_test)\n",
        "        pred_score = pred / len(feature_matrix)\n",
        "        pred_type = np.argmax(pred_score, axis=1)\n",
        "        y_true = np.hstack((y_true, y_test))\n",
        "        y_pred = np.hstack((y_pred, pred_type))\n",
        "        y_score = np.row_stack((y_score, pred_score))\n",
        "    result_all, result_eve = evaluate(y_pred, y_score, y_true, event_num, set_name)\n",
        "    # =============================================================================\n",
        "    #         a,b=evaluate(pred_type,pred_score,y_test,event_num)\n",
        "    #         for i in range(all_eval_type):\n",
        "    #             result_all[i]+=a[i]\n",
        "    #         for i in range(each_eval_type):\n",
        "    #             result_eve[:,i]+=b[:,i]\n",
        "    #     result_all=result_all/5\n",
        "    #     result_eve=result_eve/5\n",
        "    # =============================================================================\n",
        "    return result_all, result_eve\n",
        "\n",
        "\n",
        "def evaluate(pred_type, pred_score, y_test, event_num, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_one_hot = label_binarize(y_test, classes=np.arange(event_num))\n",
        "    pred_one_hot = label_binarize(pred_type, classes=np.arange(event_num))\n",
        "\n",
        "    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n",
        "\n",
        "    result_all[0] = accuracy_score(y_test, pred_type)\n",
        "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[5] = f1_score(y_test, pred_type, average='micro')\n",
        "    result_all[6] = f1_score(y_test, pred_type, average='macro')\n",
        "    result_all[7] = precision_score(y_test, pred_type, average='micro')\n",
        "    result_all[8] = precision_score(y_test, pred_type, average='macro')\n",
        "    result_all[9] = recall_score(y_test, pred_type, average='micro')\n",
        "    result_all[10] = recall_score(y_test, pred_type, average='macro')\n",
        "    for i in range(event_num):\n",
        "        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n",
        "        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                          average=None)\n",
        "        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                         average=None)\n",
        "        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                    average='binary')\n",
        "        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                           average='binary')\n",
        "        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                        average='binary')\n",
        "    return [result_all, result_eve]\n",
        "\n",
        "\n",
        "def self_metric_calculate(y_true, pred_type):\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred = pred_type.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_pred_c = y_pred.take([0], axis=1).ravel()\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    for i in range(len(y_true_c)):\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 1):\n",
        "            TP += 1\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 0):\n",
        "            FN += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 1):\n",
        "            FP += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 0):\n",
        "            TN += 1\n",
        "    print(\"TP=\", TP, \"FN=\", FN, \"FP=\", FP, \"TN=\", TN)\n",
        "    return (TP / (TP + FP), TP / (TP + FN))\n",
        "\n",
        "\n",
        "def multiclass_precision_recall_curve(y_true, y_score):\n",
        "    y_true = y_true.ravel()\n",
        "    y_score = y_score.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_score.ndim == 1:\n",
        "        y_score = y_score.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_score_c = y_score.take([0], axis=1).ravel()\n",
        "    precision, recall, pr_thresholds = precision_recall_curve(y_true_c, y_score_c)\n",
        "    return (precision, recall, pr_thresholds)\n",
        "\n",
        "\n",
        "def roc_aupr_score(y_true, y_score, average=\"macro\"):\n",
        "    def _binary_roc_aupr_score(y_true, y_score):\n",
        "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _average_binary_score(binary_metric, y_true, y_score, average):  # y_true= y_one_hot\n",
        "        if average == \"binary\":\n",
        "            return binary_metric(y_true, y_score)\n",
        "        if average == \"micro\":\n",
        "            y_true = y_true.ravel()\n",
        "            y_score = y_score.ravel()\n",
        "        if y_true.ndim == 1:\n",
        "            y_true = y_true.reshape((-1, 1))\n",
        "        if y_score.ndim == 1:\n",
        "            y_score = y_score.reshape((-1, 1))\n",
        "        n_classes = y_score.shape[1]\n",
        "        score = np.zeros((n_classes,))\n",
        "        for c in range(n_classes):\n",
        "            y_true_c = y_true.take([c], axis=1).ravel()\n",
        "            y_score_c = y_score.take([c], axis=1).ravel()\n",
        "            score[c] = binary_metric(y_true_c, y_score_c)\n",
        "        return np.average(score)\n",
        "\n",
        "    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)\n",
        "\n",
        "\n",
        "def drawing(d_result, contrast_list, info_list):\n",
        "    column = []\n",
        "    for i in contrast_list:\n",
        "        column.append(i)\n",
        "    df = pd.DataFrame(columns=column)\n",
        "    if info_list[-1] == 'aupr':\n",
        "        for i in contrast_list:\n",
        "            df[i] = d_result[i][:, 1]\n",
        "    else:\n",
        "        for i in contrast_list:\n",
        "            df[i] = d_result[i][:, 2]\n",
        "    df = df.astype('float')\n",
        "    color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray')\n",
        "    df.plot.box(ylim=[0, 1.0], grid=True, color=color)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def save_result(feature_name, result_type, clf_type, result):\n",
        "    with open(feature_name + '_' + result_type + '_' + clf_type+ '.csv', \"w\", newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for i in result:\n",
        "            writer.writerow(i)\n",
        "    return 0\n",
        "\n",
        "# Instead of using argparse for command-line arguments, set the parameters directly.\n",
        "args = {\n",
        "    \"featureList\": [\"smile\", \"target\", \"enzyme\", \"category\"],\n",
        "    \"classifier\": [\"CNN_DDI\"],\n",
        "    \"NLPProcess\": \"read\"\n",
        "}\n",
        "\n",
        "# Main function adjusted for Jupyter Notebook\n",
        "def main(args):\n",
        "    seed = 0\n",
        "    # CV = 5\n",
        "    interaction_num = 10\n",
        "    # Ensure you have the 'event.db' file accessible in your Google Colab environment.\n",
        "    # You might need to upload it or access it from Google Drive.\n",
        "    conn = sqlite3.connect(\"/content/drive/My Drive/CNN_DDI/event.db\")\n",
        "    df_drug = pd.read_sql('select * from drug;', conn)\n",
        "    df_event = pd.read_sql('select * from event_number;', conn)\n",
        "    df_interaction = pd.read_sql('select * from event;', conn)\n",
        "\n",
        "    feature_list = args['featureList']\n",
        "    featureName = \"+\".join(feature_list)\n",
        "    clf_list = args['classifier']\n",
        "    for feature in feature_list:\n",
        "        set_name = feature + '+'\n",
        "    set_name = set_name[:-1]\n",
        "    result_all = {}\n",
        "    result_eve = {}\n",
        "    all_matrix = []\n",
        "    drugList = []\n",
        "    for line in open(\"/content/drive/My Drive/CNN_DDI/DrugList.txt\", 'r'):\n",
        "        drugList.append(line.split()[0])\n",
        "    if args['NLPProcess'] == \"read\":\n",
        "        extraction = pd.read_sql('select * from extraction;', conn)\n",
        "        mechanism = extraction['mechanism']\n",
        "        action = extraction['action']\n",
        "        drugA = extraction['drugA']\n",
        "        drugB = extraction['drugB']\n",
        "    else:\n",
        "        pass\n",
        "        # mechanism, action, drugA, drugB = NLPProcess(drugList, df_interaction)\n",
        "\n",
        "    for feature in feature_list:\n",
        "        print(feature)\n",
        "        new_feature, new_label, event_num = prepare(df_drug, [feature], vector_size, mechanism, action, drugA, drugB)\n",
        "        all_matrix.append(new_feature)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    for clf in clf_list:\n",
        "        print(clf)\n",
        "        all_result, each_result = cross_validation(all_matrix, new_label, clf, event_num, seed, CV, set_name)\n",
        "        save_result(featureName, 'all', clf, all_result)\n",
        "        save_result(featureName, 'each', clf, each_result)\n",
        "        result_all[clf] = all_result\n",
        "        result_eve[clf] = each_result\n",
        "    print(\"time used:\", time.perf_counter() - start)\n",
        "\n",
        "# Call the main function with the predefined arguments\n",
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "LRwhaVSTEpKx",
        "outputId": "943a6a72-96f5-4751-bb14-38f17b7fbcff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "smile\n",
            "target\n",
            "enzyme\n",
            "category\n",
            "CNN_DDI\n",
            "4\n",
            "(18617, 572, 2)\n",
            "(18647, 572, 2)\n",
            "torch.Size([18617, 65]) torch.Size([18647, 65])\n",
            "CNN_DDI(\n",
            "  (conv1): Conv1d(2, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
            "  (lrelu): LeakyReLU(negative_slope=0.2)\n",
            "  (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
            "  (conv3_1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
            "  (conv3_2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same)\n",
            "  (conv4): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=146432, out_features=267, bias=True)\n",
            "  (fc2): Linear(in_features=267, out_features=65, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-63d44e07d0a4>\u001b[0m in \u001b[0;36m<cell line: 537>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;31m# Call the main function with the predefined arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-63d44e07d0a4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mall_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'each'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-63d44e07d0a4>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(feature_matrix, label_matrix, clf_type, event_num, seed, CV, set_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mclf_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'RF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-63d44e07d0a4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, epochs, criterion, optimizer, device, accumulation_steps, patience)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m  \u001b[0;31m# Normalize loss to account for accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Accumulate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-63d44e07d0a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mconv2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv2_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 306\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    307\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGw0ucWZLt41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}