{\rtf1\adeflang1025\ansi\ansicpg1252\uc1\adeff31507\deff0\stshfdbch31505\stshfloch31506\stshfhich31506\stshfbi31507\deflang1033\deflangfe2052\themelang1033\themelangfe0\themelangcs0{\fonttbl{\f0\fbidi \froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\f34\fbidi \froman\fcharset0\fprq2{\*\panose 02040503050406030204}Cambria Math;}
{\f36\fbidi \fnil\fcharset134\fprq2{\*\panose 02010600030101010101}DengXian{\*\falt \'b5\'c8\'cf\'df};}{\f44\fbidi \fswiss\fcharset0\fprq2 Aptos;}{\f46\fbidi \fnil\fcharset134\fprq2{\*\panose 02010600030101010101}@DengXian;}
{\flomajor\f31500\fbidi \froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\fdbmajor\f31501\fbidi \fnil\fcharset134\fprq2{\*\panose 02010600030101010101}DengXian Light{\*\falt \'b5\'c8\'cf\'df Light};}
{\fhimajor\f31502\fbidi \fswiss\fcharset0\fprq2 Aptos Display;}{\fbimajor\f31503\fbidi \froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\flominor\f31504\fbidi \froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}
{\fdbminor\f31505\fbidi \fnil\fcharset134\fprq2{\*\panose 02010600030101010101}DengXian{\*\falt \'b5\'c8\'cf\'df};}{\fhiminor\f31506\fbidi \fswiss\fcharset0\fprq2 Aptos;}
{\fbiminor\f31507\fbidi \froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}{\f47\fbidi \froman\fcharset238\fprq2 Times New Roman CE;}{\f48\fbidi \froman\fcharset204\fprq2 Times New Roman Cyr;}
{\f50\fbidi \froman\fcharset161\fprq2 Times New Roman Greek;}{\f51\fbidi \froman\fcharset162\fprq2 Times New Roman Tur;}{\f52\fbidi \froman\fcharset177\fprq2 Times New Roman (Hebrew);}{\f53\fbidi \froman\fcharset178\fprq2 Times New Roman (Arabic);}
{\f54\fbidi \froman\fcharset186\fprq2 Times New Roman Baltic;}{\f55\fbidi \froman\fcharset163\fprq2 Times New Roman (Vietnamese);}{\f387\fbidi \froman\fcharset238\fprq2 Cambria Math CE;}{\f388\fbidi \froman\fcharset204\fprq2 Cambria Math Cyr;}
{\f390\fbidi \froman\fcharset161\fprq2 Cambria Math Greek;}{\f391\fbidi \froman\fcharset162\fprq2 Cambria Math Tur;}{\f394\fbidi \froman\fcharset186\fprq2 Cambria Math Baltic;}{\f395\fbidi \froman\fcharset163\fprq2 Cambria Math (Vietnamese);}
{\f409\fbidi \fnil\fcharset0\fprq2 DengXian Western{\*\falt \'b5\'c8\'cf\'df};}{\f407\fbidi \fnil\fcharset238\fprq2 DengXian CE{\*\falt \'b5\'c8\'cf\'df};}{\f408\fbidi \fnil\fcharset204\fprq2 DengXian Cyr{\*\falt \'b5\'c8\'cf\'df};}
{\f410\fbidi \fnil\fcharset161\fprq2 DengXian Greek{\*\falt \'b5\'c8\'cf\'df};}{\f487\fbidi \fswiss\fcharset238\fprq2 Aptos CE;}{\f488\fbidi \fswiss\fcharset204\fprq2 Aptos Cyr;}{\f490\fbidi \fswiss\fcharset161\fprq2 Aptos Greek;}
{\f491\fbidi \fswiss\fcharset162\fprq2 Aptos Tur;}{\f494\fbidi \fswiss\fcharset186\fprq2 Aptos Baltic;}{\f495\fbidi \fswiss\fcharset163\fprq2 Aptos (Vietnamese);}{\f509\fbidi \fnil\fcharset0\fprq2 @DengXian Western;}
{\f507\fbidi \fnil\fcharset238\fprq2 @DengXian CE;}{\f508\fbidi \fnil\fcharset204\fprq2 @DengXian Cyr;}{\f510\fbidi \fnil\fcharset161\fprq2 @DengXian Greek;}{\flomajor\f31508\fbidi \froman\fcharset238\fprq2 Times New Roman CE;}
{\flomajor\f31509\fbidi \froman\fcharset204\fprq2 Times New Roman Cyr;}{\flomajor\f31511\fbidi \froman\fcharset161\fprq2 Times New Roman Greek;}{\flomajor\f31512\fbidi \froman\fcharset162\fprq2 Times New Roman Tur;}
{\flomajor\f31513\fbidi \froman\fcharset177\fprq2 Times New Roman (Hebrew);}{\flomajor\f31514\fbidi \froman\fcharset178\fprq2 Times New Roman (Arabic);}{\flomajor\f31515\fbidi \froman\fcharset186\fprq2 Times New Roman Baltic;}
{\flomajor\f31516\fbidi \froman\fcharset163\fprq2 Times New Roman (Vietnamese);}{\fdbmajor\f31520\fbidi \fnil\fcharset0\fprq2 DengXian Light Western{\*\falt \'b5\'c8\'cf\'df Light};}
{\fdbmajor\f31518\fbidi \fnil\fcharset238\fprq2 DengXian Light CE{\*\falt \'b5\'c8\'cf\'df Light};}{\fdbmajor\f31519\fbidi \fnil\fcharset204\fprq2 DengXian Light Cyr{\*\falt \'b5\'c8\'cf\'df Light};}
{\fdbmajor\f31521\fbidi \fnil\fcharset161\fprq2 DengXian Light Greek{\*\falt \'b5\'c8\'cf\'df Light};}{\fhimajor\f31528\fbidi \fswiss\fcharset238\fprq2 Aptos Display CE;}{\fhimajor\f31529\fbidi \fswiss\fcharset204\fprq2 Aptos Display Cyr;}
{\fhimajor\f31531\fbidi \fswiss\fcharset161\fprq2 Aptos Display Greek;}{\fhimajor\f31532\fbidi \fswiss\fcharset162\fprq2 Aptos Display Tur;}{\fhimajor\f31535\fbidi \fswiss\fcharset186\fprq2 Aptos Display Baltic;}
{\fhimajor\f31536\fbidi \fswiss\fcharset163\fprq2 Aptos Display (Vietnamese);}{\fbimajor\f31538\fbidi \froman\fcharset238\fprq2 Times New Roman CE;}{\fbimajor\f31539\fbidi \froman\fcharset204\fprq2 Times New Roman Cyr;}
{\fbimajor\f31541\fbidi \froman\fcharset161\fprq2 Times New Roman Greek;}{\fbimajor\f31542\fbidi \froman\fcharset162\fprq2 Times New Roman Tur;}{\fbimajor\f31543\fbidi \froman\fcharset177\fprq2 Times New Roman (Hebrew);}
{\fbimajor\f31544\fbidi \froman\fcharset178\fprq2 Times New Roman (Arabic);}{\fbimajor\f31545\fbidi \froman\fcharset186\fprq2 Times New Roman Baltic;}{\fbimajor\f31546\fbidi \froman\fcharset163\fprq2 Times New Roman (Vietnamese);}
{\flominor\f31548\fbidi \froman\fcharset238\fprq2 Times New Roman CE;}{\flominor\f31549\fbidi \froman\fcharset204\fprq2 Times New Roman Cyr;}{\flominor\f31551\fbidi \froman\fcharset161\fprq2 Times New Roman Greek;}
{\flominor\f31552\fbidi \froman\fcharset162\fprq2 Times New Roman Tur;}{\flominor\f31553\fbidi \froman\fcharset177\fprq2 Times New Roman (Hebrew);}{\flominor\f31554\fbidi \froman\fcharset178\fprq2 Times New Roman (Arabic);}
{\flominor\f31555\fbidi \froman\fcharset186\fprq2 Times New Roman Baltic;}{\flominor\f31556\fbidi \froman\fcharset163\fprq2 Times New Roman (Vietnamese);}{\fdbminor\f31560\fbidi \fnil\fcharset0\fprq2 DengXian Western{\*\falt \'b5\'c8\'cf\'df};}
{\fdbminor\f31558\fbidi \fnil\fcharset238\fprq2 DengXian CE{\*\falt \'b5\'c8\'cf\'df};}{\fdbminor\f31559\fbidi \fnil\fcharset204\fprq2 DengXian Cyr{\*\falt \'b5\'c8\'cf\'df};}
{\fdbminor\f31561\fbidi \fnil\fcharset161\fprq2 DengXian Greek{\*\falt \'b5\'c8\'cf\'df};}{\fhiminor\f31568\fbidi \fswiss\fcharset238\fprq2 Aptos CE;}{\fhiminor\f31569\fbidi \fswiss\fcharset204\fprq2 Aptos Cyr;}
{\fhiminor\f31571\fbidi \fswiss\fcharset161\fprq2 Aptos Greek;}{\fhiminor\f31572\fbidi \fswiss\fcharset162\fprq2 Aptos Tur;}{\fhiminor\f31575\fbidi \fswiss\fcharset186\fprq2 Aptos Baltic;}
{\fhiminor\f31576\fbidi \fswiss\fcharset163\fprq2 Aptos (Vietnamese);}{\fbiminor\f31578\fbidi \froman\fcharset238\fprq2 Times New Roman CE;}{\fbiminor\f31579\fbidi \froman\fcharset204\fprq2 Times New Roman Cyr;}
{\fbiminor\f31581\fbidi \froman\fcharset161\fprq2 Times New Roman Greek;}{\fbiminor\f31582\fbidi \froman\fcharset162\fprq2 Times New Roman Tur;}{\fbiminor\f31583\fbidi \froman\fcharset177\fprq2 Times New Roman (Hebrew);}
{\fbiminor\f31584\fbidi \froman\fcharset178\fprq2 Times New Roman (Arabic);}{\fbiminor\f31585\fbidi \froman\fcharset186\fprq2 Times New Roman Baltic;}{\fbiminor\f31586\fbidi \froman\fcharset163\fprq2 Times New Roman (Vietnamese);}}
{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;
\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;\red0\green0\blue0;\red0\green0\blue0;}{\*\defchp \fs24\kerning2\loch\af31506\hich\af31506\dbch\af31505 }{\*\defpap 
\ql \li0\ri0\sa160\sl278\slmult1\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 }\noqfpromote {\stylesheet{\ql \li0\ri0\sa160\sl278\slmult1\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \rtlch\fcs1 
\af31507\afs24\alang1025 \ltrch\fcs0 \fs24\lang1033\langfe2052\kerning2\loch\f31506\hich\af31506\dbch\af31505\cgrid\langnp1033\langfenp2052 \snext0 \sqformat \spriority0 Normal;}{\*\cs10 \additive \ssemihidden \sunhideused \spriority1 
Default Paragraph Font;}{\*\ts11\tsrowd\trftsWidthB3\trpaddl108\trpaddr108\trpaddfl3\trpaddft3\trpaddfb3\trpaddfr3\trcbpat1\trcfpat1\tblind0\tblindtype3\tsvertalt\tsbrdrt\tsbrdrl\tsbrdrb\tsbrdrr\tsbrdrdgl\tsbrdrdgr\tsbrdrh\tsbrdrv 
\ql \li0\ri0\sa160\sl278\slmult1\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \rtlch\fcs1 \af31507\afs24\alang1025 \ltrch\fcs0 
\fs24\lang1033\langfe2052\kerning2\loch\f31506\hich\af31506\dbch\af31505\cgrid\langnp1033\langfenp2052 \snext11 \ssemihidden \sunhideused Normal Table;}}{\*\rsidtbl \rsid15089610\rsid15682265}{\mmathPr\mmathFont34\mbrkBin0\mbrkBinSub0\msmallFrac0
\mdispDef1\mlMargin0\mrMargin0\mdefJc1\mwrapIndent1440\mintLim0\mnaryLim1}{\info{\operator Jinyu Rao}{\creatim\yr2024\mo4\dy27\hr12\min23}{\revtim\yr2024\mo4\dy27\hr12\min23}{\version2}{\edmins0}{\nofpages37}{\nofwords7858}{\nofchars44795}
{\nofcharsws52548}{\vern93}}{\*\xmlnstbl {\xmlns1 http://schemas.microsoft.com/office/word/2003/wordml}}\paperw12240\paperh15840\margl1440\margr1440\margt1440\margb1440\gutter0\ltrsect 
\widowctrl\ftnbj\aenddoc\trackmoves0\trackformatting1\donotembedsysfont0\relyonvml0\donotembedlingdata1\grfdocevents0\validatexml0\showplaceholdtext0\ignoremixedcontent0\saveinvalidxml0\showxmlerrors0\horzdoc\dghspace120\dgvspace120\dghorigin1701
\dgvorigin1984\dghshow0\dgvshow3\jcompress\viewkind1\viewscale130\rsidroot15089610 \fet0{\*\wgrffmtfilter 2450}\ilfomacatclnup0\ltrpar \sectd \ltrsect\linex0\sectdefaultcl\sftnbj {\*\pnseclvl1\pnucrm\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl2
\pnucltr\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl3\pndec\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl4\pnlcltr\pnstart1\pnindent720\pnhang {\pntxta )}}{\*\pnseclvl5\pndec\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl6
\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl7\pnlcrm\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl8\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl9\pnlcrm\pnstart1\pnindent720\pnhang 
{\pntxtb (}{\pntxta )}}\pard\plain \ltrpar\ql \li0\ri0\sa160\sl278\slmult1\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid15089610 \rtlch\fcs1 \af31507\afs24\alang1025 \ltrch\fcs0 
\fs24\lang1033\langfe2052\kerning2\loch\af31506\hich\af31506\dbch\af31505\cgrid\langnp1033\langfenp2052 {\rtlch\fcs1 \af31507 \ltrch\fcs0 \insrsid15089610 \hich\af31506\dbch\af31505\loch\f31506 (H:\\CondaEnv\\CNNDDIV4) PS H:\\CodeV3\\
CS598_DLH_Project-CNN_DDI\\CS598_DLH_Project-CNN_DDI> pdm run python CNN_DDI_final.py -c CNN_DDI -s Cosine
\par \hich\af31506\dbch\af31505\loch\f31506 Inside an active virtualenv H:\\CondaEnv\\CNNDDIV4, reusing it.
\par \hich\af31506\dbch\af31505\loch\f31506 Set env var PDM_IGNORE_ACTIVE_VENV to ignore it.
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:03.061741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.249732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.274840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
\par \hich\af31506\dbch\af31505\loch\f31506 pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti compu\hich\af31506\dbch\af31505\loch\f31506 teCapability: 7.5
\par \hich\af31506\dbch\af31505\loch\f31506 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.275026: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.279067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.281522: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully open\hich\af31506\dbch\af31505\loch\f31506 ed dynamic library cufft64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.282347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.285294: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.286741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.292999: I tensorflow/s\hich\af31506\dbch\af31505\loch\f31506 tream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.293134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
\par \hich\af31506\dbch\af31505\loch\f31506 Setting GPU memory limit to 10GB.
\par \hich\af31506\dbch\af31505\loch\f31506 
2024-04-27 11:50:04.293597: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
\par \hich\af31506\dbch\af31505\loch\f31506 To enable them in \hich\af31506\dbch\af31505\loch\f31506 other operations, rebuild TensorFlow with the appropriate compiler flags.
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.300631: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23e19ebec60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.300733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.300942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
\par \hich\af31506\dbch\af31505\loch\f31506 pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5
\par \hich\af31506\dbch\af31505\loch\f31506 coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301292: I tensorflow/stream_\hich\af31506\dbch\af31505\loch\f31506 executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library\hich\af31506\dbch\af31505\loch\f31506  cusparse64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.301802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.732316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.732467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.732578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.732768: I tensorflow/core/common_runtime/gpu\hich\af31506\dbch\af31505\loch\f31506 
/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10240 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.735413: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23e8337ad20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:04.735561: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecut\hich\af31506\dbch\af31505\loch\f31506 or device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
\par \hich\af31506\dbch\af31505\loch\f31506 1 Physical GPU, 1 Logical GPU(s) with memory limit:10240
\par \hich\af31506\dbch\af31505\loch\f31506 TensorFlow version: 2.3.4
\par \hich\af31506\dbch\af31505\loch\f31506 Num GPUs Available:  1
\par \{\hich\af31506\dbch\af31505\loch\f31506 
'featureList': ['pathway', 'target', 'enzyme', 'category'], 'classifier': ['CNN_DDI'], 'NLPProcess': 'read', 'similarity_measure': 'Cosine', 'num_folds': 5, 'num_epochs': 100, 'batch_size': 128, 'evaluate_only': False, 'save_weights': False, 'loss_fn': '
\hich\af31506\dbch\af31505\loch\f31506 categorical_crossentropy'\}
\par \hich\af31506\dbch\af31505\loch\f31506 pathway
\par \hich\af31506\dbch\af31505\loch\f31506 target
\par \hich\af31506\dbch\af31505\loch\f31506 enzyme
\par \hich\af31506\dbch\af31505\loch\f31506 category
\par \hich\af31506\dbch\af31505\loch\f31506 4
\par \hich\af31506\dbch\af31505\loch\f31506 running cross validation \hich\af31506\dbch\af31505\loch\f31506 for CNN_DDI
\par \hich\af31506\dbch\af31505\loch\f31506 process = 16202.67008 total = 34176.229376 available = 15234.125824 used = 18942.103552 free = 15234.125824 percent = 55.4
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 1, feature 1. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:14.174946: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:14.363495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7\hich\af31506\dbch\af31505\loch\f31506 .dll
\par \hich\af31506\dbch\af31505\loch\f31506 2024-04-27 11:50:15.117777: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
\par \hich\af31506\dbch\af31505\loch\f31506 Relying on driver to perform ptx compilation.
\par \hich\af31506\dbch\af31505\loch\f31506 Modify $PATH to customize ptxas location.
\par \hich\af31506\dbch\af31505\loch\f31506 This message will be only logged once.
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 1.1517 - accuracy: 0.6575 - val_loss: 0.7318 - val_accuracy: 0.7576
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss\hich\af31506\dbch\af31505\loch\f31506 : 0.5773 - accuracy: 0.8020 - val_loss: 0.6067 - val_accuracy: 0.7984
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.4353 - accuracy: 0.8453 - val_loss: 0.5865 - val_accuracy: 0.8085
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.3554 - accuracy: 0.8711 - val_loss: 0.5899 - val_accuracy: 0.7960
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.3032 - accuracy: 0.8914 - val_loss: 0.6034 - val_accuracy: 0.8039
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.2696 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.9010 - val_loss: 0.6389 - val_accuracy: 0.8112
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.2477 - accuracy: 0.9093 - val_loss: 0.6191 - val_accuracy: 0.8099
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.2318 - accuracy: 0.9158 - val_loss: 0.6068 - val_accuracy: 0.8120
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 8s 33ms/step - loss: 0.2206 - accuracy: 0.9179 - val_loss: 0.6411 - val_accuracy: 0.8107
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [======\hich\af31506\dbch\af31505\loch\f31506 ========================] - 7s 30ms/step - loss: 0.2094 - accuracy: 0.9221 - val_loss: 0.6251 - val_accuracy: 0.8126
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.2043 - accuracy: 0.9227 - val_loss: 0.6092 - val_accuracy: 0.8077
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 0.1961 - accuracy: 0.9254 - val_loss: 0.6125 - val_accuracy: 0.8172
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.1916 - accuracy: 0.9257 - \hich\af31506\dbch\af31505\loch\f31506 val_loss: 0.6130 - val_accuracy: 0.8068
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18360.32 total = 34176.229376 available = 13636.960256 used = 20539.26912 free = 13636.960256 percent = 60.1
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 1, feature 2. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 1.1113 - accuracy: 0.6683 - val_loss: 0.7093 - val_accuracy: 0.7744
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.5501 - accuracy: 0.8107 - val_loss: 0.5949 - va\hich\af31506\dbch\af31505\loch\f31506 l_accuracy: 0.7966
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.4099 - accuracy: 0.8557 - val_loss: 0.5724 - val_accuracy: 0.8071
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.3288 - accuracy: 0.8831 - val_loss: 0.5800 - val_accuracy: 0.8176
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.2833 - accuracy: 0.8977 - val_loss: 0.5896 - val_accuracy: 0.8134
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.2453 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.9118 - val_loss: 0.5786 - val_accuracy: 0.8170
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 0.2217 - accuracy: 0.9203 - val_loss: 0.5872 - val_accuracy: 0.8184
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 0.2062 - accuracy: 0.9244 - val_loss: 0.6299 - val_accuracy: 0.8235
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 0.1965 - accuracy: 0.9293 - val_loss: 0.5856 - val_accuracy: 0.8203
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [======\hich\af31506\dbch\af31505\loch\f31506 ========================] - 7s 30ms/step - loss: 0.1840 - accuracy: 0.9325 - val_loss: 0.5795 - val_accuracy: 0.8186
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.1802 - accuracy: 0.9333 - val_loss: 0.6014 - val_accuracy: 0.8198
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.1745 - accuracy: 0.9345 - val_loss: 0.6370 - val_accuracy: 0.8198
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.1697 - accuracy: 0.9362 - \hich\af31506\dbch\af31505\loch\f31506 val_loss: 0.6504 - val_accuracy: 0.8135
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18594.05824 total = 34176.229376 available = 13789.990912 used = 20386.238464 free = 13789.990912 percent = 59.7
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 1, feature 3. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 
  1/233 [..............................] - ETA: 0s - loss: 4.1801 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s). Check your c
\hich\af31506\dbch\af31505\loch\f31506 allbacks.
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 1.6259 - accuracy: 0.5284 - val_loss: 1.2411 - val_accuracy: 0.6259
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 1.0820 - accuracy: 0.6505 - val_loss: 1.1165 - val_accuracy: 0.6452
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.9307 - accuracy: 0.6846 - val_loss: 1.0666 - val_accuracy: 0.6410
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 31ms/step - loss: 0.8461 - accuracy: 0.7047 - val_loss: 1.0387 - val_accuracy: 0.6521
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.7912 - accuracy: 0.7205 - val_loss: 1.0701 - val_accuracy: 0.6445
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.7487 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.7332 - val_loss: 1.0532 - val_accuracy: 0.6599
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.7130 - accuracy: 0.7433 - val_loss: 1.0954 - val_accuracy: 0.6529
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.6965 - accuracy: 0.7491 - val_loss: 1.0527 - val_accuracy: 0.6587
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.6735 - accuracy: 0.7575 - val_loss: 1.0716 - val_accuracy: 0.6552
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [======\hich\af31506\dbch\af31505\loch\f31506 ========================] - 7s 30ms/step - loss: 0.6575 - accuracy: 0.7611 - val_loss: 1.0835 - val_accuracy: 0.6543
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.6453 - accuracy: 0.7635 - val_loss: 1.1056 - val_accuracy: 0.6604
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.6323 - accuracy: 0.7651 - val_loss: 1.0946 - val_accuracy: 0.6579
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 30ms/step - loss: 0.6269 - accuracy: 0.7688 - \hich\af31506\dbch\af31505\loch\f31506 val_loss: 1.1583 - val_accuracy: 0.6507
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6142 - accuracy: 0.7717 - val_loss: 1.1212 - val_accuracy: 0.6594
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18794.520576 total = 34176.229376 available = 13545.263104 used = 20630.966272 free = 13545.263104 percent = 60.4
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 1, feature 4. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506   1/233 [..............................] - ETA: 0s - loss: 4.1743 - accuracy: 0.0078WARNING:tensorflow:Call\hich\af31506\dbch\af31505\loch\f31506 
backs method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s). Check your callbacks.
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 1.0207 - accuracy: 0.7077 - val_loss: 0.5776 - val_accuracy: 0.8195
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.4136 - accuracy: 0.8629 - val_loss: 0.4377 - val_accuracy: 0.8607
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2585 - accuracy: 0.9110 - val_loss: 0.4417 - val_accuracy: 0.8621
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1791 - accuracy: 0.9379 - val_loss: 0.4153 - val_accuracy: 0.8740
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1328 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.9539 - val_loss: 0.4618 - val_accuracy: 0.8794
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1005 - accuracy: 0.9663 - val_loss: 0.4808 - val_accuracy: 0.8843
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0832 - accuracy: 0.9724 - val_loss: 0.5340 - val_accuracy: 0.8799
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0622 - accuracy: 0.9796 - val_loss: 0.5192 - val_accuracy: 0.8845
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 6s 28ms/step - loss: 0.0555 - accuracy: 0.9816 - val_loss: 0.5952 - val_accuracy: 0.8805
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0490 - accuracy: 0.9843 - val_loss: 0.5739 - val_accuracy: 0.8918
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0465 - accuracy: 0.9854 - val_loss: 0.5547 - val_accuracy: 0.8876
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0327 - accuracy: 0.9899 - v\hich\af31506\dbch\af31505\loch\f31506 al_loss: 0.6170 - val_accuracy: 0.8859
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.6187 - val_accuracy: 0.8883
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0392 - accuracy: 0.9876 - val_loss: 0.5967 - val_accuracy: 0.8872
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18232.619008 total = 34176.229376 available = 14046.109696 used = 20130.11968 free = 14046.109696 percent = 58.9
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 2, featur\hich\af31506\dbch\af31505\loch\f31506 e 1. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.2468 - accuracy: 0.6407 - val_loss: 0.7803 - val_accuracy: 0.7446
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.6054 - accuracy: 0.7954 - val_loss: 0.6368 - val_accuracy: 0.7918
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.4552 - accuracy: 0.8401 - val_loss: 0.6042 - val_accuracy: 0.7980
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.3707 - accuracy: 0.8670 - val_loss: 0.5972 - val_accuracy: 0.8036
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.3166 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.8849 - val_loss: 0.6086 - val_accuracy: 0.8073
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2780 - accuracy: 0.8996 - val_loss: 0.5979 - val_accuracy: 0.8047
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2516 - accuracy: 0.9086 - val_loss: 0.6080 - val_accuracy: 0.8049
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2311 - accuracy: 0.9154 - val_loss: 0.6299 - val_accuracy: 0.7997
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 6s 28ms/step - loss: 0.2158 - accuracy: 0.9193 - val_loss: 0.6434 - val_accuracy: 0.8013
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2098 - accuracy: 0.9223 - val_loss: 0.6352 - val_accuracy: 0.8095
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2001 - accuracy: 0.9247 - val_loss: 0.6452 - val_accuracy: 0.8059
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1963 - accuracy: 0.9258 - v\hich\af31506\dbch\af31505\loch\f31506 al_loss: 0.6385 - val_accuracy: 0.8098
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1885 - accuracy: 0.9264 - val_loss: 0.6305 - val_accuracy: 0.8048
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1842 - accuracy: 0.9281 - val_loss: 0.6632 - val_accuracy: 0.8032
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18449.87904 total = 34176.229376 available = 14105.309184 used = 20070.920192 free = 14105.309184 percent = 58.7
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 2, featur\hich\af31506\dbch\af31505\loch\f31506 e 2. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 1.2539 - accuracy: 0.6364 - val_loss: 0.7544 - val_accuracy: 0.7520
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.5740 - accuracy: 0.8071 - val_loss: 0.6258 - val_accuracy: 0.7926
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4163 - accuracy: 0.8551 - val_loss: 0.5711 - val_accuracy: 0.8052
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.3325 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.8809 - val_loss: 0.5866 - val_accuracy: 0.8135
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2794 - accuracy: 0.9033 - val_loss: 0.5904 - val_accuracy: 0.8099
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2438 - accuracy: 0.9134 - val_loss: 0.6147 - val_accuracy: 0.8123
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2220 - accuracy: 0.9222 - val_loss: 0.5912 - val_accuracy: 0.8142
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.2039 - accuracy: 0.9269 - val_loss: 0.5848 - val_accuracy: 0.8148
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1940 - accuracy: 0.9290 - val_loss: 0.6069 - val_accuracy: 0.8158
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1812 - accuracy: 0.9336 - val_loss: 0.6166 - val_accuracy: 0.8143
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1767 - accuracy: 0.9353 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 0.6406 - val_accuracy: 0.8161
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1715 - accuracy: 0.9361 - val_loss: 0.6437 - val_accuracy: 0.8127
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1653 - accuracy: 0.9385 - val_loss: 0.6219 - val_accuracy: 0.8131
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18634.706944 total = 34176.229376 available = 13815.283712 used = 20360.945664 free = 13815.283712 percent = 59.6
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 2, featur\hich\af31506\dbch\af31505\loch\f31506 e 3. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.6196 - accuracy: 0.5291 - val_loss: 1.2541 - val_accuracy: 0.6192
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.0875 - accuracy: 0.6462 - val_loss: 1.1250 - val_accuracy: 0.6395
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.9266 - accuracy: 0.6860 - val_loss: 1.0824 - val_accuracy: 0.6472
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.8436 - accuracy: 0.7075 - val_loss: 1.0664 - val_accuracy: 0.6519
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7892 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.7198 - val_loss: 1.0649 - val_accuracy: 0.6580
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.7484 - accuracy: 0.7324 - val_loss: 1.0688 - val_accuracy: 0.6494
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.7169 - accuracy: 0.7409 - val_loss: 1.0819 - val_accuracy: 0.6547
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6895 - accuracy: 0.7493 - val_loss: 1.1032 - val_accuracy: 0.6568
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.6717 - accuracy: 0.7545 - val_loss: 1.0668 - val_accuracy: 0.6561
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6579 - accuracy: 0.7579 - val_loss: 1.0935 - val_accuracy: 0.6553
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.6401 - accuracy: 0.7645 - val_loss: 1.1302 - val_accuracy: 0.6565
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.6295 - accuracy: 0.7666 - v\hich\af31506\dbch\af31505\loch\f31506 al_loss: 1.1111 - val_accuracy: 0.6530
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6198 - accuracy: 0.7707 - val_loss: 1.1210 - val_accuracy: 0.6519
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.6128 - accuracy: 0.7730 - val_loss: 1.1562 - val_accuracy: 0.6529
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 15/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6068 - accuracy: 0.7736 - val_loss: 1.1756 - val_accuracy: 0.6505
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18792.349696 total = 34176.2\hich\af31506\dbch\af31505\loch\f31506 29376 available = 13727.719424 used = 20448.509952 free = 13727.719424 percent = 59.8
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 2, feature 4. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.0724 - accuracy: 0.6931 - val_loss: 0.5973 - val_accuracy: 0.8123
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.4089 - accuracy: 0.8642 - val_loss: 0.4558 - val_accuracy: 0.8528
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2581 - accuracy: 0.9108 - val_loss: 0.4201 - val_accuracy: 0.8679
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1740 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.9397 - val_loss: 0.4551 - val_accuracy: 0.8708
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1281 - accuracy: 0.9572 - val_loss: 0.4652 - val_accuracy: 0.8800
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0949 - accuracy: 0.9688 - val_loss: 0.4796 - val_accuracy: 0.8779
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0763 - accuracy: 0.9746 - val_loss: 0.5140 - val_accuracy: 0.8814
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.0644 - accuracy: 0.9787 - val_loss: 0.5797 - val_accuracy: 0.8840
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0575 - accuracy: 0.9809 - val_loss: 0.5661 - val_accuracy: 0.8793
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0505 - accuracy: 0.9843 - val_loss: 0.5843 - val_accuracy: 0.8809
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0412 - accuracy: 0.9866 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 0.6229 - val_accuracy: 0.8741
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0474 - accuracy: 0.9846 - val_loss: 0.5917 - val_accuracy: 0.8824
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0346 - accuracy: 0.9896 - val_loss: 0.6799 - val_accuracy: 0.8824
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18274.476032 total = 34176.229376 available = 14110.98624 used = 20065.243136 free = 14110.98624 percent = 58.7
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 3, feature \hich\af31506\dbch\af31505\loch\f31506 1. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 1.1391 - accuracy: 0.6627 - val_loss: 0.7353 - val_accuracy: 0.7604
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.5714 - accuracy: 0.8064 - val_loss: 0.6518 - val_accuracy: 0.7839
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4339 - accuracy: 0.8472 - val_loss: 0.6107 - val_accuracy: 0.7986
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.3494 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.8749 - val_loss: 0.6343 - val_accuracy: 0.7983
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.3058 - accuracy: 0.8911 - val_loss: 0.6285 - val_accuracy: 0.7979
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2703 - accuracy: 0.9024 - val_loss: 0.6430 - val_accuracy: 0.8035
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2459 - accuracy: 0.9103 - val_loss: 0.6538 - val_accuracy: 0.7961
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.2261 - accuracy: 0.9170 - val_loss: 0.6329 - val_accuracy: 0.8046
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2160 - accuracy: 0.9184 - val_loss: 0.6618 - val_accuracy: 0.8035
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2056 - accuracy: 0.9227 - val_loss: 0.6691 - val_accuracy: 0.8019
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1987 - accuracy: 0.9264 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 0.6722 - val_accuracy: 0.7991
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1957 - accuracy: 0.9256 - val_loss: 0.6989 - val_accuracy: 0.8008
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1900 - accuracy: 0.9278 - val_loss: 0.7184 - val_accuracy: 0.8018
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18489.319424 total = 34176.229376 available = 13982.121984 used = 20194.107392 free = 13982.121984 percent = 59.1
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 3, featur\hich\af31506\dbch\af31505\loch\f31506 e 2. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 
  1/233 [..............................] - ETA: 0s - loss: 4.1769 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s). Check your cal
\hich\af31506\dbch\af31505\loch\f31506 lbacks.
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.1859 - accuracy: 0.6518 - val_loss: 0.7376 - val_accuracy: 0.7588
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.5597 - accuracy: 0.8096 - val_loss: 0.6\hich\af31506\dbch\af31505\loch\f31506 261 - val_accuracy: 0.7920
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4101 - accuracy: 0.8556 - val_loss: 0.5872 - val_accuracy: 0.8073
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.3329 - accuracy: 0.8816 - val_loss: 0.5878 - val_accuracy: 0.8113
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2800 - accuracy: 0.8996 - val_loss: 0.6216 - val_accuracy: 0.8027
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] \hich\af31506\dbch\af31505\loch\f31506 - 7s 28ms/step - loss: 0.2440 - accuracy: 0.9126 - val_loss: 0.6122 - val_accuracy: 0.8128
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2185 - accuracy: 0.9218 - val_loss: 0.6294 - val_accuracy: 0.8145
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2033 - accuracy: 0.9257 - val_loss: 0.6644 - val_accuracy: 0.8125
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1901 - accuracy: 0.9304 - val_loss: 0.6561 - val_accura\hich\af31506\dbch\af31505\loch\f31506 cy: 0.8145
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1790 - accuracy: 0.9352 - val_loss: 0.6787 - val_accuracy: 0.8079
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1754 - accuracy: 0.9357 - val_loss: 0.6190 - val_accuracy: 0.8085
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1688 - accuracy: 0.9372 - val_loss: 0.6936 - val_accuracy: 0.8070
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/st\hich\af31506\dbch\af31505\loch\f31506 ep - loss: 0.1650 - accuracy: 0.9393 - val_loss: 0.6583 - val_accuracy: 0.8179
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18669.99808 total = 34176.229376 available = 13692.387328 used = 20483.842048 free = 13692.387328 percent = 59.9
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 3, feature 3. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.6321 - accuracy: 0.5264 - val_loss: 1.2276 - val_accuracy: 0.6060
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.0855 - accuracy: 0.6452 - val_loss: 1.0623 - val_accuracy: 0.6560
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.9297 - accuracy: 0.6828 - val_loss: 1.0223 - val_accuracy: 0.6639
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.8424 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.7056 - val_loss: 1.0300 - val_accuracy: 0.6604
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7889 - accuracy: 0.7215 - val_loss: 1.0283 - val_accuracy: 0.6590
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7494 - accuracy: 0.7315 - val_loss: 1.0712 - val_accuracy: 0.6527
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.7134 - accuracy: 0.7425 - val_loss: 1.0773 - val_accuracy: 0.6590
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.6892 - accuracy: 0.7481 - val_loss: 1.1560 - val_accuracy: 0.6543
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6688 - accuracy: 0.7559 - val_loss: 1.0478 - val_accuracy: 0.6670
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6503 - accuracy: 0.7620 - val_loss: 1.0961 - val_accuracy: 0.6545
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6409 - accuracy: 0.7649 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 1.0729 - val_accuracy: 0.6535
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6349 - accuracy: 0.7656 - val_loss: 1.0993 - val_accuracy: 0.6596
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6202 - accuracy: 0.7708 - val_loss: 1.1077 - val_accuracy: 0.6541
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18814.554112 total = 34176.229376 available = 13663.260672 used = 20512.968704 free = 13663.260672 percent = 60.0
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 3, featur\hich\af31506\dbch\af31505\loch\f31506 e 4. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 
  1/233 [..............................] - ETA: 0s - loss: 4.1729 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s). Check your cal
\hich\af31506\dbch\af31505\loch\f31506 lbacks.
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.0341 - accuracy: 0.7038 - val_loss: 0.5655 - val_accuracy: 0.8222
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4048 - accuracy: 0.8659 - val_loss: 0.4326 - val_accuracy: 0.8606
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2566 - accuracy: 0.9124 - val_loss: 0.4072 - val_accuracy: 0.8713
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1752 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.9393 - val_loss: 0.4741 - val_accuracy: 0.8667
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1239 - accuracy: 0.9577 - val_loss: 0.4727 - val_accuracy: 0.8756
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0977 - accuracy: 0.9664 - val_loss: 0.5709 - val_accuracy: 0.8721
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.0783 - accuracy: 0.9735 - val_loss: 0.5736 - val_accuracy: 0.8753
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 6s 28ms/step - loss: 0.0659 - accuracy: 0.9773 - val_loss: 0.5535 - val_accuracy: 0.8748
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 0.5358 - val_accuracy: 0.8843
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0447 - accuracy: 0.9859 - val_loss: 0.6155 - val_accuracy: 0.8820
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.0382 - accuracy: 0.9884 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 0.6427 - val_accuracy: 0.8787
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0431 - accuracy: 0.9870 - val_loss: 0.5968 - val_accuracy: 0.8822
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.0363 - accuracy: 0.9890 - val_loss: 0.6579 - val_accuracy: 0.8839
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18281.893888 total = 34176.229376 available = 14234.816512 used = 19941.412864 free = 14234.816512 percent = 58.3
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 4, featur\hich\af31506\dbch\af31505\loch\f31506 e 1. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 1.1353 - accuracy: 0.6608 - val_loss: 0.7397 - val_accuracy: 0.7569
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.5754 - accuracy: 0.8027 - val_loss: 0.6287 - val_accuracy: 0.7907
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4311 - accuracy: 0.8500 - val_loss: 0.6026 - val_accuracy: 0.7977
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.3587 - accuracy: 0.8707 - val_loss: 0.5965 - val_accuracy: 0.8041
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.3032 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.8880 - val_loss: 0.6452 - val_accuracy: 0.8053
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2678 - accuracy: 0.9016 - val_loss: 0.6159 - val_accuracy: 0.8082
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.2456 - accuracy: 0.9097 - val_loss: 0.6356 - val_accuracy: 0.8099
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.2320 - accuracy: 0.9158 - val_loss: 0.6670 - val_accuracy: 0.8103
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.2178 - accuracy: 0.9195 - val_loss: 0.6441 - val_accuracy: 0.8086
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2115 - accuracy: 0.9223 - val_loss: 0.6679 - val_accuracy: 0.8094
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2008 - accuracy: 0.9242 - val_loss: 0.6443 - val_accuracy: 0.8133
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.1987 - accuracy: 0.9251 - v\hich\af31506\dbch\af31505\loch\f31506 al_loss: 0.6782 - val_accuracy: 0.8063
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1921 - accuracy: 0.9251 - val_loss: 0.6480 - val_accuracy: 0.8134
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1885 - accuracy: 0.9282 - val_loss: 0.7114 - val_accuracy: 0.8115
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18496.700416 total = 34176.229376 available = 14024.187904 used = 20152.041472 free = 14024.187904 percent = 59.0
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 4, featu\hich\af31506\dbch\af31505\loch\f31506 re 2. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.1732 - accuracy: 0.6509 - val_loss: 0.7406 - val_accuracy: 0.7571
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.5602 - accuracy: 0.8098 - val_loss: 0.6149 - val_accuracy: 0.7947
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.4131 - accuracy: 0.8533 - val_loss: 0.5868 - val_accuracy: 0.8047
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.3337 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.8805 - val_loss: 0.5823 - val_accuracy: 0.8126
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2805 - accuracy: 0.8997 - val_loss: 0.6082 - val_accuracy: 0.8176
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2456 - accuracy: 0.9116 - val_loss: 0.5964 - val_accuracy: 0.8254
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2202 - accuracy: 0.9211 - val_loss: 0.5938 - val_accuracy: 0.8174
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 6s 28ms/step - loss: 0.2023 - accuracy: 0.9273 - val_loss: 0.5951 - val_accuracy: 0.8275
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1936 - accuracy: 0.9288 - val_loss: 0.6322 - val_accuracy: 0.8156
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.1829 - accuracy: 0.9328 - val_loss: 0.5977 - val_accuracy: 0.8177
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1799 - accuracy: 0.9352 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 0.6188 - val_accuracy: 0.8221
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1727 - accuracy: 0.9354 - val_loss: 0.6146 - val_accuracy: 0.8236
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1685 - accuracy: 0.9368 - val_loss: 0.6247 - val_accuracy: 0.8263
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1655 - accuracy: 0.9378 - val_loss: 0.6090 - val_accuracy: 0.8238
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18677.39136 total = 34176.229\hich\af31506\dbch\af31505\loch\f31506 376 available = 13773.328384 used = 20402.900992 free = 13773.328384 percent = 59.7
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 4, feature 3. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.6319 - accuracy: 0.5307 - val_loss: 1.2859 - val_accuracy: 0.6040
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 1.0772 - accuracy: 0.6496 - val_loss: 1.1119 - val_accuracy: 0.6295
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.9257 - accuracy: 0.6823 - val_loss: 1.0830 - val_accuracy: 0.6464
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.8415 - accurac\hich\af31506\dbch\af31505\loch\f31506 y: 0.7057 - val_loss: 1.0616 - val_accuracy: 0.6448
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7823 - accuracy: 0.7239 - val_loss: 1.0845 - val_accuracy: 0.6455
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7385 - accuracy: 0.7380 - val_loss: 1.0503 - val_accuracy: 0.6495
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.7127 - accuracy: 0.7432 - val_loss: 1.0635 - val_accuracy: 0.6503
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [=======\hich\af31506\dbch\af31505\loch\f31506 =======================] - 7s 28ms/step - loss: 0.6869 - accuracy: 0.7527 - val_loss: 1.0673 - val_accuracy: 0.6466
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6653 - accuracy: 0.7578 - val_loss: 1.0819 - val_accuracy: 0.6451
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.6502 - accuracy: 0.7617 - val_loss: 1.1154 - val_accuracy: 0.6510
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.6348 - accuracy: 0.7667 - va\hich\af31506\dbch\af31505\loch\f31506 l_loss: 1.1877 - val_accuracy: 0.6466
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6224 - accuracy: 0.7706 - val_loss: 1.1886 - val_accuracy: 0.6393
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6149 - accuracy: 0.7708 - val_loss: 1.1652 - val_accuracy: 0.6423
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6073 - accuracy: 0.7727 - val_loss: 1.1443 - val_accuracy: 0.6419
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 15/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.6024 - accuracy: 0.7751 - val_loss: 1.2559 - val_accuracy: 0.6479
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 16/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.5990 - accuracy: 0.7761 - val_loss: 1.1413 - val_accuracy: 0.6376
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18823.168 total = 34176.229376 available = 13930.315776 used = 20245.9136 free = 13930.315776 percent = 59.2
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 4, feature 4. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [===========\hich\af31506\dbch\af31505\loch\f31506 ===================] - 7s 29ms/step - loss: 1.1124 - accuracy: 0.6829 - val_loss: 0.5845 - val_accuracy: 0.8177
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 29ms/step - loss: 0.4126 - accuracy: 0.8626 - val_loss: 0.4498 - val_accuracy: 0.8524
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.2587 - accuracy: 0.9120 - val_loss: 0.4090 - val_accuracy: 0.8670
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 7s 28ms/step - loss: 0.1792 - accuracy: 0.9379 - val_loss\hich\af31506\dbch\af31505\loch\f31506 : 0.4588 - val_accuracy: 0.8700
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1315 - accuracy: 0.9538 - val_loss: 0.4573 - val_accuracy: 0.8780
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.1002 - accuracy: 0.9666 - val_loss: 0.4956 - val_accuracy: 0.8788
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.0786 - accuracy: 0.9735 - val_loss: 0.5068 - val_accuracy: 0.8775
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [===========================\hich\af31506\dbch\af31505\loch\f31506 ===] - 6s 28ms/step - loss: 0.0616 - accuracy: 0.9793 - val_loss: 0.5586 - val_accuracy: 0.8827
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0543 - accuracy: 0.9817 - val_loss: 0.5712 - val_accuracy: 0.8819
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.0495 - accuracy: 0.9842 - val_loss: 0.6088 - val_accuracy: 0.8767
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.6047 - val\hich\af31506\dbch\af31505\loch\f31506 _accuracy: 0.8846
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 28ms/step - loss: 0.0407 - accuracy: 0.9874 - val_loss: 0.5954 - val_accuracy: 0.8826
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 233/233 [==============================] - 6s 27ms/step - loss: 0.0330 - accuracy: 0.9902 - val_loss: 0.6689 - val_accuracy: 0.8868
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18290.50368 total = 34176.229376 available = 14401.55648 used = 19774.672896 free = 14401.55648 percent = 57.9
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 5, feature 1. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 1.1767 - accuracy: 0.6544 - val_loss: 0.7190 - val_accuracy: 0.7685
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==========\hich\af31506\dbch\af31505\loch\f31506 ====================] - 7s 28ms/step - loss: 0.5890 - accuracy: 0.8028 - val_loss: 0.6041 - val_accuracy: 0.7894
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.4440 - accuracy: 0.8433 - val_loss: 0.5748 - val_accuracy: 0.8035
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 28ms/step - loss: 0.3613 - accuracy: 0.8681 - val_loss: 0.5508 - val_accuracy: 0.8121
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.3091 - accuracy: 0.8874 - val_los\hich\af31506\dbch\af31505\loch\f31506 s: 0.6375 - val_accuracy: 0.8077
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 28ms/step - loss: 0.2778 - accuracy: 0.8978 - val_loss: 0.5729 - val_accuracy: 0.8174
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.2512 - accuracy: 0.9073 - val_loss: 0.5679 - val_accuracy: 0.8194
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.2339 - accuracy: 0.9154 - val_loss: 0.5695 - val_accuracy: 0.8171
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==========================\hich\af31506\dbch\af31505\loch\f31506 ====] - 7s 28ms/step - loss: 0.2261 - accuracy: 0.9160 - val_loss: 0.6182 - val_accuracy: 0.8092
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.2191 - accuracy: 0.9173 - val_loss: 0.5706 - val_accuracy: 0.8147
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 28ms/step - loss: 0.2143 - accuracy: 0.9192 - val_loss: 0.5845 - val_accuracy: 0.8148
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 28ms/step - loss: 0.1982 - accuracy: 0.9225 - val_loss: 0.6216 - v\hich\af31506\dbch\af31505\loch\f31506 al_accuracy: 0.8154
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.1949 - accuracy: 0.9236 - val_loss: 0.6168 - val_accuracy: 0.8202
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.1926 - accuracy: 0.9250 - val_loss: 0.5978 - val_accuracy: 0.8214
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18505.252864 total = 34176.229376 available = 14107.205632 used = 20069.023744 free = 14107.205632 percent = 58.7
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 5, feature 2. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [======\hich\af31506\dbch\af31505\loch\f31506 ========================] - 7s 28ms/step - loss: 1.1299 - accuracy: 0.6668 - val_loss: 0.6940 - val_accuracy: 0.7681
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.5628 - accuracy: 0.8091 - val_loss: 0.5702 - val_accuracy: 0.8064
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 0.4261 - accuracy: 0.8492 - val_loss: 0.5649 - val_accuracy: 0.8116
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 30ms/step - loss: 0.3404 - accuracy: 0.8766 - val\hich\af31506\dbch\af31505\loch\f31506 _loss: 0.5501 - val_accuracy: 0.8151
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 8s 33ms/step - loss: 0.2851 - accuracy: 0.8969 - val_loss: 0.5494 - val_accuracy: 0.8186
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 32ms/step - loss: 0.2478 - accuracy: 0.9110 - val_loss: 0.6054 - val_accuracy: 0.8162
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.2320 - accuracy: 0.9155 - val_loss: 0.5847 - val_accuracy: 0.8181
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [======================\hich\af31506\dbch\af31505\loch\f31506 ========] - 7s 31ms/step - loss: 0.2311 - accuracy: 0.9185 - val_loss: 0.5609 - val_accuracy: 0.8265
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.2019 - accuracy: 0.9265 - val_loss: 0.5787 - val_accuracy: 0.8257
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1862 - accuracy: 0.9317 - val_loss: 0.5904 - val_accuracy: 0.8255
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1777 - accuracy: 0.9325 - val_loss: 0.5804 \hich\af31506\dbch\af31505\loch\f31506 - val_accuracy: 0.8298
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1734 - accuracy: 0.9353 - val_loss: 0.6193 - val_accuracy: 0.8242
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1677 - accuracy: 0.9360 - val_loss: 0.6148 - val_accuracy: 0.8241
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1724 - accuracy: 0.9345 - val_loss: 0.6263 - val_accuracy: 0.8182
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 15/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.1654 - accuracy: 0.9368 - val_loss: 0.6273 - val_accuracy: 0.8275
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18686.521344 total = 34176.229376 available = 13891.74784 used = 20284.481536 fr\hich\af31506\dbch\af31505\loch\f31506 ee = 13891.74784 percent = 59.4
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 5, feature 3. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 
  1/234 [..............................] - ETA: 0s - loss: 4.1750 - accuracy: 0.0078WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s). Check your callbac
\hich\af31506\dbch\af31505\loch\f31506 ks.
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 1.6379 - accuracy: 0.5244 - val_loss: 1.2039 - val_accuracy: \hich\af31506\dbch\af31505\loch\f31506 0.6184
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 1.0992 - accuracy: 0.6445 - val_loss: 1.0293 - val_accuracy: 0.6699
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.9451 - accuracy: 0.6782 - val_loss: 1.0076 - val_accuracy: 0.6655
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.8631 - accuracy: 0.6995 - val_loss: 0.9951 - val_accuracy: 0.6620
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - los\hich\af31506\dbch\af31505\loch\f31506 s: 0.8057 - accuracy: 0.7161 - val_loss: 1.0134 - val_accuracy: 0.6632
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.7683 - accuracy: 0.7240 - val_loss: 1.0170 - val_accuracy: 0.6714
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.7285 - accuracy: 0.7371 - val_loss: 1.0255 - val_accuracy: 0.6676
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6995 - accuracy: 0.7472 - val_loss: 1.0315 - val_accuracy: 0.6679
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6918 - accuracy: 0.7478 - val_loss: 1.0869 - val_accuracy: 0.6675
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6705 - accuracy: 0.7547 - val_loss: 1.0902 - val_accuracy: 0.6738
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6544 - accuracy: 0.7583 - val_loss: 1.0681 - val_accuracy: 0.6630
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6436 - accuracy: 0.7636 - val_loss: 1.0544 - val_accuracy: 0.6644
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 13/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 31ms/step - loss: 0.6334 - accuracy: 0.7641 - val_loss: 1.1323 - val_accuracy: 0.6630
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 14/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 30ms/step - loss: 0.6289 - accu\hich\af31506\dbch\af31505\loch\f31506 racy: 0.7651 - val_loss: 1.0511 - val_accuracy: 0.6618
\par \hich\af31506\dbch\af31505\loch\f31506 process = 18831.19616 total = 34176.229376 available = 13697.122304 used = 20479.107072 free = 13697.122304 percent = 59.9
\par \hich\af31506\dbch\af31505\loch\f31506 No weights found or loading not requested for fold 5, feature 4. Starting training.
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 1/100
\par \hich\af31506\dbch\af31505\loch\f31506 
  1/234 [..............................] - ETA: 0s - loss: 4.1741 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 0.0140s
\hich\af31506\dbch\af31505\loch\f31506 ). Check your callbacks.
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 7s 28ms/step - loss: 1.0389 - accuracy: 0.7023 - val_loss: 0.5411 - val_accuracy: 0.8208
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 2/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.4103 - accuracy: 0.8638 - val_loss: 0.3945 - val_accuracy: 0.8673
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 3/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.2632 - accuracy: 0.9104 - val_loss: 0.3989 - val_accuracy: 0.8720
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 4/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step\hich\af31506\dbch\af31505\loch\f31506  - loss: 0.1872 - accuracy: 0.9365 - val_loss: 0.3959 - val_accuracy: 0.8793
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 5/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.1293 - accuracy: 0.9571 - val_loss: 0.4330 - val_accuracy: 0.8832
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 6/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.1006 - accuracy: 0.9676 - val_loss: 0.4375 - val_accuracy: 0.8853
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 7/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0827 - accuracy: 0.9726 - val_loss: 0.4649 - val_accuracy: 0.8909
\par \hich\af31506\dbch\af31505\loch\f31506 Epo\hich\af31506\dbch\af31505\loch\f31506 ch 8/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0617 - accuracy: 0.9794 - val_loss: 0.4847 - val_accuracy: 0.8837
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 9/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0564 - accuracy: 0.9815 - val_loss: 0.4872 - val_accuracy: 0.8884
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 10/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0430 - accuracy: 0.9868 - val_loss: 0.5399 - val_accuracy: 0.8892
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 11/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.5697 - val_accuracy: 0.8849
\par \hich\af31506\dbch\af31505\loch\f31506 Epoch 12/100
\par \hich\af31506\dbch\af31505\loch\f31506 234/234 [==============================] - 6s 27ms/step - loss: 0.0484 - accuracy: 0.9866 - val_loss: 0.5132 - val_accuracy: 0.8909
\par \hich\af31506\dbch\af31505\loch\f31506 H:\\CondaEnv\\CNNDDIV4\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMe\hich\af31506\dbch\af31505\loch\f31506 
tricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
\par \hich\af31506\dbch\af31505\loch\f31506   'precision', 'predicted', average, warn_for)
\par \hich\af31506\dbch\af31505\loch\f31506 H:\\CondaEnv\\CNNDDIV4\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.
\par \hich\af31506\dbch\af31505\loch\f31506   'precision', 'predicted', average, warn_for)
\par \hich\af31506\dbch\af31505\loch\f31506 H:\\CondaEnv\\CNNDDIV4\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being se\hich\af31506\dbch\af31505\loch\f31506 
t to 0.0 due to no predicted samples.
\par \hich\af31506\dbch\af31505\loch\f31506   'precision', 'predicted', average, warn_for)
\par \hich\af31506\dbch\af31505\loch\f31506 H:\\CondaEnv\\CNNDDIV4\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
\par \hich\af31506\dbch\af31505\loch\f31506   'precision', 'predicted', average, warn_for)
\par \hich\af31506\dbch\af31505\loch\f31506    Accuracy  AUPR (micro-averaged)  AUPR (macro-averaged)  AUC (micro-averaged)  AUC (macro-averaged)  ...  Precision (micro-averaged)  Precision (macro-averaged)  Recall (micro-averaged)  Recall (macro
\hich\af31506\dbch\af31505\loch\f31506 -averaged)     Time (s)
\par \hich\af31506\dbch\af31505\loch\f31506 
0  0.881172                0.93839               0.835026              0.997897              0.987051  ...                    0.881172                    0.849614                 0.881172                 0.737535  1853.873359
\par 
\par \hich\af31506\dbch\af31505\loch\f31506 [1 rows x 12 columns]
\par \hich\af31506\dbch\af31505\loch\f31506 time used for CNN_DDI: 1853.8733587
\par \hich\af31506\dbch\af31505\loch\f31506 Total time used: 1854.0356326
\par \hich\af31506\dbch\af31505\loch\f31506 done}{\rtlch\fcs1 \af31507 \ltrch\fcs0 \insrsid15682265 
\par }{\*\themedata 504b030414000600080000002100e9de0fbfff0000001c020000130000005b436f6e74656e745f54797065735d2e786d6cac91cb4ec3301045f748fc83e52d4a
9cb2400825e982c78ec7a27cc0c8992416c9d8b2a755fbf74cd25442a820166c2cd933f79e3be372bd1f07b5c3989ca74aaff2422b24eb1b475da5df374fd9ad
5689811a183c61a50f98f4babebc2837878049899a52a57be670674cb23d8e90721f90a4d2fa3802cb35762680fd800ecd7551dc18eb899138e3c943d7e503b6
b01d583deee5f99824e290b4ba3f364eac4a430883b3c092d4eca8f946c916422ecab927f52ea42b89a1cd59c254f919b0e85e6535d135a8de20f20b8c12c3b0
0c895fcf6720192de6bf3b9e89ecdbd6596cbcdd8eb28e7c365ecc4ec1ff1460f53fe813d3cc7f5b7f020000ffff0300504b030414000600080000002100a5d6
a7e7c0000000360100000b0000005f72656c732f2e72656c73848fcf6ac3300c87ef85bd83d17d51d2c31825762fa590432fa37d00e1287f68221bdb1bebdb4f
c7060abb0884a4eff7a93dfeae8bf9e194e720169aaa06c3e2433fcb68e1763dbf7f82c985a4a725085b787086a37bdbb55fbc50d1a33ccd311ba548b6309512
0f88d94fbc52ae4264d1c910d24a45db3462247fa791715fd71f989e19e0364cd3f51652d73760ae8fa8c9ffb3c330cc9e4fc17faf2ce545046e37944c69e462
a1a82fe353bd90a865aad41ed0b5b8f9d6fd010000ffff0300504b0304140006000800000021006b799616830000008a0000001c0000007468656d652f746865
6d652f7468656d654d616e616765722e786d6c0ccc4d0ac3201040e17da17790d93763bb284562b2cbaebbf600439c1a41c7a0d29fdbd7e5e38337cedf14d59b
4b0d592c9c070d8a65cd2e88b7f07c2ca71ba8da481cc52c6ce1c715e6e97818c9b48d13df49c873517d23d59085adb5dd20d6b52bd521ef2cdd5eb9246a3d8b
4757e8d3f729e245eb2b260a0238fd010000ffff0300504b03041400060008000000210094a9528694070000c7200000160000007468656d652f7468656d652f
7468656d65312e786d6cec595f8b1bc9117f0fe43b0cf32e6b66248da4c5f2a1bfdeb3776d63c90ef7d82bb566dadb333d4cb7762d8e83e07bca4b207077e421
0779cb430839c8418ebce4c3186c92cb874875cf68d42db5bc7f30c184dd7dd1b47e55fd9baaeaaa52f5fdcf5e27d4b9c039272cedb9fe3dcf75703a670b9246
3df7c56c52ebb80e17285d20ca52dc73d798bb9f3df8e52feea32311e3043b209ff223d4736321b2a37a9dcf6119f17b2cc3297cb7647982043ce6517d91a34b
d09bd07ae079613d4124759d1425a0f6e97249e6d8994995ee838df23185c75470b930a7f954aac68684c22ece7d89e06b3ea4b9738168cf857d16ec72865f0b
d7a1880bf8a2e77aeacfad3fb85f4747a51015076435b989fa2be54a81c579a0f6cca3b36a536f1c749a7ea55f01a8d8c78d3bf2bfd2a700683e87372db8e83a
fd56e8758212ab818a8f16ddddb6df30f19afec61e67bf1b0e82a6a15f810afdcd3dbc37e98e472d03af4005beb587ef7bc1a0db30f00a54e0c33d7c73dc6f07
6303af403125e9f93e3a6c773a6189ae204b468fadf06e187aed5109dfa2201aaae8925b2c592a0ec55a825eb17c020009a44890d411eb0c2fd11ca2b89f09c6
9d11e119456bd7c950ca382c7b81ef43e835bda0fa571647471869d2921730e17b4b928fc3e739c944cf7d045a5d0df2eea79fdebef9f1ed9bbfbffdfaebb76f
feea9c902816852a43ee18a5912ef7f39f7ef79fef7fedfcfb6f7ffcf99b6fed78aee3dfffe537effff1cf0fa987a3b635c5bbef7e78ffe30fef7effdb7ffdf9
1b8bf67e8ece74f88c24983b4ff0a5f39c25f082ca14267f7c96df4c621623a24bf4d388a314c95d2cfac72236d04fd688220b6e804d3bbecc21d5d8800f57af
0cc2d3385f0962d1f8384e0ce0296374c072ab151ecbbd3433cf566964df3c5fe9b8e7085dd8f61ea2d4f0f27895418e253695c3181b349f51940a14e1140b47
7ec7ce31b6bcdd178418763d25f39c71b614ce17c419206235c98c9c19d1b4153a2609f8656d2308fe366c73fad219306a7beb11be3091703610b5909f616a98
f1215a0994d854ce504275839f2011db484ed7f95cc78db9004f47983267bcc09cdb649ee6f0be9ad31f23c86e56b79fd27562227341ce6d3a4f10633a72c4ce
87314a321b764ad258c77ececf214491f38c091bfc949927443e831f507ad0dd2f0936dc7d75367801594ea7b40d10f9cd2ab7f8f2216646fc4ed77489b02dd5
f4f3c448b1fd9c58a363b08a8cd03ec198a24bb4c0d879f1b985c1806586cdb7a41fc590558eb12db01e213356e5738a39f44ab2b9d9cf9327841b213bc5113b
c0e774bd9378d6284d507e48f313f0ba6ef3f1590e87d142e1299d9febc027047a408817ab519e72d0a105f741adcf62641430f9ccedf1bace0dff5de78cc1b9
7c65d0b8c6b904197c631948ecbacc076d3343d4d8601b3033449c135bba0511c3fd5b11595c95d8ca2ab7340fedd60dd01d194d4f42d22b3aa0ff5de703fdc5
bb3f7c6f09c18fd3edd8151ba9ea867dcea15472bcd3dd1cc2edf63443962fc8a7dfd28cd02a7d86a18aece7abbb8ee6aea371ffef3b9a43e7f9ae8f39d46ddc
f5312ef417777d4c395af9387dccb67581ae468e178a318f1afa2407673e4b42e954ac293ee16aecc3e1d7cc62028b524ecd3b713503cc62f828cb1c6c60e0a2
1c29192767e25744c4d31865301bf25da924e2a5ea883b19e3303252cb56dd124f57c9295b14a34e355bf28acaca91d8ae7b2d183a15eb30a612053a6c978b92
9f9aa7025fc5365263d60d01297b1312da6626898685447bb37805093935fb382cba16161da97ee3aa3d5300b5ca2bf073db811fe93db7d594846046cee7d09a
2fa49f0a576fbcab9cf9313d7dc8984604c058b1781318ca579eee4aae075f4fbe5d116ad7f0b4414239a5082b9384b28c6af0780c3f82cbe894abd7a171535f
77b72e35e84953a8fd20beb734da9d0fb1b8adaf416e3737d054cf1434752e7b6ed86841c8cc51d673973032868f4906b1c3e52f2e4423b877998bbc38f0b7c9
2c59cec508f1b830b84a3a857b122270ee5092f45cf9fa951b68aa7288e2e60790103e59725d482b9f1a3970bae964bc5ce2b9d0ddaead484b178f90e18b5c61
fd5689df1e2c25d90adc3d8d1797ce195de5cf118458abed4b032e08879b03bfb0e682c0555895c8b6f1b75398cae4afdf45a9182ad611cd625456143d991770
554f2a3aeaa9b281f654be33185433495908cf22596075a31ad5b42a5d05878355f76a2169392d696e6ba6915564d5b4673163874d19d8b1e5ed8abcc66a6362
c8697a852f52f76ecaed6e72dd4e9f505509307865bfdb957e8dda7633839a64bc9f8665ce2e57cddab179c12ba85da74868593fdca8ddb15b5523acdbc1e2ad
2a3fc8ed462d2c2d377da5b2b4ba33d7afb5d9d92b481e23e872575470e54a98ebe608bab2a9ea498ab40147e4b5288f067c725639e9b95f7aad7e7318b48635
afd31ad79a8da657ebb4fa8d5abfd56af8e396ef8d06c1575058449cf8ade2be7e02d717745ddedaabf5bd9bfb647343736fce923a5337f375455cdddcfbc1e1
9b7b8740d2f93218fbcda01f0c6bc3911fd69ac128ac75da8d7e6d1884a3a00f293d9cf4bf729d0b05f607a3d164d20a6ae110704dafdfaaf5078d612dec8c07
c1c41f37471e80cbccf91afa6fb0e9c616f051f17af05f000000ffff0300504b0304140006000800000021000dd1909fb60000001b010000270000007468656d
652f7468656d652f5f72656c732f7468656d654d616e616765722e786d6c2e72656c73848f4d0ac2301484f78277086f6fd3ba109126dd88d0add40384e4350d
363f2451eced0dae2c082e8761be9969bb979dc9136332de3168aa1a083ae995719ac16db8ec8e4052164e89d93b64b060828e6f37ed1567914b284d26245228
2e3198720e274a939cd08a54f980ae38a38f56e422a3a641c8bbd048f7757da0f19b017cc524bd62107bd5001996509affb3fd381a89672f1f165dfe514173d9
850528a2c6cce0239baa4c04ca5bbabac4df000000ffff0300504b01022d0014000600080000002100e9de0fbfff0000001c0200001300000000000000000000
000000000000005b436f6e74656e745f54797065735d2e786d6c504b01022d0014000600080000002100a5d6a7e7c0000000360100000b000000000000000000
00000000300100005f72656c732f2e72656c73504b01022d00140006000800000021006b799616830000008a0000001c00000000000000000000000000190200
007468656d652f7468656d652f7468656d654d616e616765722e786d6c504b01022d001400060008000000210094a9528694070000c720000016000000000000
00000000000000d60200007468656d652f7468656d652f7468656d65312e786d6c504b01022d00140006000800000021000dd1909fb60000001b010000270000
00000000000000000000009e0a00007468656d652f7468656d652f5f72656c732f7468656d654d616e616765722e786d6c2e72656c73504b050600000000050005005d010000990b00000000}
{\*\colorschememapping 3c3f786d6c2076657273696f6e3d22312e302220656e636f64696e673d225554462d3822207374616e64616c6f6e653d22796573223f3e0d0a3c613a636c724d
617020786d6c6e733a613d22687474703a2f2f736368656d61732e6f70656e786d6c666f726d6174732e6f72672f64726177696e676d6c2f323030362f6d6169
6e22206267313d226c743122207478313d22646b3122206267323d226c743222207478323d22646b322220616363656e74313d22616363656e74312220616363
656e74323d22616363656e74322220616363656e74333d22616363656e74332220616363656e74343d22616363656e74342220616363656e74353d22616363656e74352220616363656e74363d22616363656e74362220686c696e6b3d22686c696e6b2220666f6c486c696e6b3d22666f6c486c696e6b222f3e}
{\*\latentstyles\lsdstimax376\lsdlockeddef0\lsdsemihiddendef0\lsdunhideuseddef0\lsdqformatdef0\lsdprioritydef99{\lsdlockedexcept \lsdqformat1 \lsdpriority0 \lsdlocked0 Normal;\lsdqformat1 \lsdpriority9 \lsdlocked0 heading 1;
\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 2;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 3;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 4;
\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 5;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 6;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 7;
\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 8;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority9 \lsdlocked0 heading 9;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 1;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 5;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 6;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 7;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 8;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index 9;
\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 1;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 2;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 3;
\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 4;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 5;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 6;
\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 7;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 8;\lsdsemihidden1 \lsdunhideused1 \lsdpriority39 \lsdlocked0 toc 9;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Normal Indent;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 footnote text;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 annotation text;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 header;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 footer;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 index heading;\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority35 \lsdlocked0 caption;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 table of figures;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 envelope address;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 envelope return;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 footnote reference;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 annotation reference;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 line number;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 page number;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 endnote reference;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 endnote text;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 table of authorities;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 macro;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 toa heading;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Bullet;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Number;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List 3;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Bullet 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Bullet 3;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Bullet 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Bullet 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Number 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Number 3;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Number 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Number 5;\lsdqformat1 \lsdpriority10 \lsdlocked0 Title;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Closing;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Signature;\lsdsemihidden1 \lsdunhideused1 \lsdpriority1 \lsdlocked0 Default Paragraph Font;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text Indent;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Continue;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Continue 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Continue 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Continue 4;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 List Continue 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Message Header;\lsdqformat1 \lsdpriority11 \lsdlocked0 Subtitle;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Salutation;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Date;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text First Indent;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text First Indent 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Note Heading;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text Indent 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Body Text Indent 3;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Block Text;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Hyperlink;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 FollowedHyperlink;\lsdqformat1 \lsdpriority22 \lsdlocked0 Strong;
\lsdqformat1 \lsdpriority20 \lsdlocked0 Emphasis;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Document Map;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Plain Text;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 E-mail Signature;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Top of Form;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Bottom of Form;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Normal (Web);\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Acronym;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Address;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Cite;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Code;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Definition;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Keyboard;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Preformatted;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Sample;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Typewriter;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 HTML Variable;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Normal Table;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 annotation subject;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 No List;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Outline List 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Outline List 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Outline List 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Simple 1;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Simple 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Simple 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Classic 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Classic 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Classic 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Classic 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Colorful 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Colorful 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Colorful 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Columns 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Columns 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Columns 3;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Columns 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Columns 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 6;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 7;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Grid 8;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 4;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 5;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 6;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 7;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table List 8;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table 3D effects 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table 3D effects 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table 3D effects 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Contemporary;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Elegant;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Professional;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Subtle 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Subtle 2;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Web 1;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Web 2;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Web 3;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Balloon Text;\lsdpriority39 \lsdlocked0 Table Grid;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Table Theme;\lsdsemihidden1 \lsdlocked0 Placeholder Text;
\lsdqformat1 \lsdpriority1 \lsdlocked0 No Spacing;\lsdpriority60 \lsdlocked0 Light Shading;\lsdpriority61 \lsdlocked0 Light List;\lsdpriority62 \lsdlocked0 Light Grid;\lsdpriority63 \lsdlocked0 Medium Shading 1;\lsdpriority64 \lsdlocked0 Medium Shading 2;
\lsdpriority65 \lsdlocked0 Medium List 1;\lsdpriority66 \lsdlocked0 Medium List 2;\lsdpriority67 \lsdlocked0 Medium Grid 1;\lsdpriority68 \lsdlocked0 Medium Grid 2;\lsdpriority69 \lsdlocked0 Medium Grid 3;\lsdpriority70 \lsdlocked0 Dark List;
\lsdpriority71 \lsdlocked0 Colorful Shading;\lsdpriority72 \lsdlocked0 Colorful List;\lsdpriority73 \lsdlocked0 Colorful Grid;\lsdpriority60 \lsdlocked0 Light Shading Accent 1;\lsdpriority61 \lsdlocked0 Light List Accent 1;
\lsdpriority62 \lsdlocked0 Light Grid Accent 1;\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 1;\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 1;\lsdpriority65 \lsdlocked0 Medium List 1 Accent 1;\lsdsemihidden1 \lsdlocked0 Revision;
\lsdqformat1 \lsdpriority34 \lsdlocked0 List Paragraph;\lsdqformat1 \lsdpriority29 \lsdlocked0 Quote;\lsdqformat1 \lsdpriority30 \lsdlocked0 Intense Quote;\lsdpriority66 \lsdlocked0 Medium List 2 Accent 1;\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 1;
\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 1;\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 1;\lsdpriority70 \lsdlocked0 Dark List Accent 1;\lsdpriority71 \lsdlocked0 Colorful Shading Accent 1;\lsdpriority72 \lsdlocked0 Colorful List Accent 1;
\lsdpriority73 \lsdlocked0 Colorful Grid Accent 1;\lsdpriority60 \lsdlocked0 Light Shading Accent 2;\lsdpriority61 \lsdlocked0 Light List Accent 2;\lsdpriority62 \lsdlocked0 Light Grid Accent 2;\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 2;
\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 2;\lsdpriority65 \lsdlocked0 Medium List 1 Accent 2;\lsdpriority66 \lsdlocked0 Medium List 2 Accent 2;\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 2;\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 2;
\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 2;\lsdpriority70 \lsdlocked0 Dark List Accent 2;\lsdpriority71 \lsdlocked0 Colorful Shading Accent 2;\lsdpriority72 \lsdlocked0 Colorful List Accent 2;\lsdpriority73 \lsdlocked0 Colorful Grid Accent 2;
\lsdpriority60 \lsdlocked0 Light Shading Accent 3;\lsdpriority61 \lsdlocked0 Light List Accent 3;\lsdpriority62 \lsdlocked0 Light Grid Accent 3;\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 3;\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 3;
\lsdpriority65 \lsdlocked0 Medium List 1 Accent 3;\lsdpriority66 \lsdlocked0 Medium List 2 Accent 3;\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 3;\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 3;\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 3;
\lsdpriority70 \lsdlocked0 Dark List Accent 3;\lsdpriority71 \lsdlocked0 Colorful Shading Accent 3;\lsdpriority72 \lsdlocked0 Colorful List Accent 3;\lsdpriority73 \lsdlocked0 Colorful Grid Accent 3;\lsdpriority60 \lsdlocked0 Light Shading Accent 4;
\lsdpriority61 \lsdlocked0 Light List Accent 4;\lsdpriority62 \lsdlocked0 Light Grid Accent 4;\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 4;\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 4;\lsdpriority65 \lsdlocked0 Medium List 1 Accent 4;
\lsdpriority66 \lsdlocked0 Medium List 2 Accent 4;\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 4;\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 4;\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 4;\lsdpriority70 \lsdlocked0 Dark List Accent 4;
\lsdpriority71 \lsdlocked0 Colorful Shading Accent 4;\lsdpriority72 \lsdlocked0 Colorful List Accent 4;\lsdpriority73 \lsdlocked0 Colorful Grid Accent 4;\lsdpriority60 \lsdlocked0 Light Shading Accent 5;\lsdpriority61 \lsdlocked0 Light List Accent 5;
\lsdpriority62 \lsdlocked0 Light Grid Accent 5;\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 5;\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 5;\lsdpriority65 \lsdlocked0 Medium List 1 Accent 5;\lsdpriority66 \lsdlocked0 Medium List 2 Accent 5;
\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 5;\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 5;\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 5;\lsdpriority70 \lsdlocked0 Dark List Accent 5;\lsdpriority71 \lsdlocked0 Colorful Shading Accent 5;
\lsdpriority72 \lsdlocked0 Colorful List Accent 5;\lsdpriority73 \lsdlocked0 Colorful Grid Accent 5;\lsdpriority60 \lsdlocked0 Light Shading Accent 6;\lsdpriority61 \lsdlocked0 Light List Accent 6;\lsdpriority62 \lsdlocked0 Light Grid Accent 6;
\lsdpriority63 \lsdlocked0 Medium Shading 1 Accent 6;\lsdpriority64 \lsdlocked0 Medium Shading 2 Accent 6;\lsdpriority65 \lsdlocked0 Medium List 1 Accent 6;\lsdpriority66 \lsdlocked0 Medium List 2 Accent 6;
\lsdpriority67 \lsdlocked0 Medium Grid 1 Accent 6;\lsdpriority68 \lsdlocked0 Medium Grid 2 Accent 6;\lsdpriority69 \lsdlocked0 Medium Grid 3 Accent 6;\lsdpriority70 \lsdlocked0 Dark List Accent 6;\lsdpriority71 \lsdlocked0 Colorful Shading Accent 6;
\lsdpriority72 \lsdlocked0 Colorful List Accent 6;\lsdpriority73 \lsdlocked0 Colorful Grid Accent 6;\lsdqformat1 \lsdpriority19 \lsdlocked0 Subtle Emphasis;\lsdqformat1 \lsdpriority21 \lsdlocked0 Intense Emphasis;
\lsdqformat1 \lsdpriority31 \lsdlocked0 Subtle Reference;\lsdqformat1 \lsdpriority32 \lsdlocked0 Intense Reference;\lsdqformat1 \lsdpriority33 \lsdlocked0 Book Title;\lsdsemihidden1 \lsdunhideused1 \lsdpriority37 \lsdlocked0 Bibliography;
\lsdsemihidden1 \lsdunhideused1 \lsdqformat1 \lsdpriority39 \lsdlocked0 TOC Heading;\lsdpriority41 \lsdlocked0 Plain Table 1;\lsdpriority42 \lsdlocked0 Plain Table 2;\lsdpriority43 \lsdlocked0 Plain Table 3;\lsdpriority44 \lsdlocked0 Plain Table 4;
\lsdpriority45 \lsdlocked0 Plain Table 5;\lsdpriority40 \lsdlocked0 Grid Table Light;\lsdpriority46 \lsdlocked0 Grid Table 1 Light;\lsdpriority47 \lsdlocked0 Grid Table 2;\lsdpriority48 \lsdlocked0 Grid Table 3;\lsdpriority49 \lsdlocked0 Grid Table 4;
\lsdpriority50 \lsdlocked0 Grid Table 5 Dark;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful;\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful;\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 1;\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 1;
\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 1;\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 1;\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 1;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 1;
\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 1;\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 2;\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 2;\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 2;
\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 2;\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 2;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 2;\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 2;
\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 3;\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 3;\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 3;\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 3;
\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 3;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 3;\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 3;\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 4;
\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 4;\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 4;\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 4;\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 4;
\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 4;\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 4;\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 5;\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 5;
\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 5;\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 5;\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 5;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 5;
\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 5;\lsdpriority46 \lsdlocked0 Grid Table 1 Light Accent 6;\lsdpriority47 \lsdlocked0 Grid Table 2 Accent 6;\lsdpriority48 \lsdlocked0 Grid Table 3 Accent 6;
\lsdpriority49 \lsdlocked0 Grid Table 4 Accent 6;\lsdpriority50 \lsdlocked0 Grid Table 5 Dark Accent 6;\lsdpriority51 \lsdlocked0 Grid Table 6 Colorful Accent 6;\lsdpriority52 \lsdlocked0 Grid Table 7 Colorful Accent 6;
\lsdpriority46 \lsdlocked0 List Table 1 Light;\lsdpriority47 \lsdlocked0 List Table 2;\lsdpriority48 \lsdlocked0 List Table 3;\lsdpriority49 \lsdlocked0 List Table 4;\lsdpriority50 \lsdlocked0 List Table 5 Dark;
\lsdpriority51 \lsdlocked0 List Table 6 Colorful;\lsdpriority52 \lsdlocked0 List Table 7 Colorful;\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 1;\lsdpriority47 \lsdlocked0 List Table 2 Accent 1;\lsdpriority48 \lsdlocked0 List Table 3 Accent 1;
\lsdpriority49 \lsdlocked0 List Table 4 Accent 1;\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 1;\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 1;\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 1;
\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 2;\lsdpriority47 \lsdlocked0 List Table 2 Accent 2;\lsdpriority48 \lsdlocked0 List Table 3 Accent 2;\lsdpriority49 \lsdlocked0 List Table 4 Accent 2;
\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 2;\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 2;\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 2;\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 3;
\lsdpriority47 \lsdlocked0 List Table 2 Accent 3;\lsdpriority48 \lsdlocked0 List Table 3 Accent 3;\lsdpriority49 \lsdlocked0 List Table 4 Accent 3;\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 3;
\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 3;\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 3;\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 4;\lsdpriority47 \lsdlocked0 List Table 2 Accent 4;
\lsdpriority48 \lsdlocked0 List Table 3 Accent 4;\lsdpriority49 \lsdlocked0 List Table 4 Accent 4;\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 4;\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 4;
\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 4;\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 5;\lsdpriority47 \lsdlocked0 List Table 2 Accent 5;\lsdpriority48 \lsdlocked0 List Table 3 Accent 5;
\lsdpriority49 \lsdlocked0 List Table 4 Accent 5;\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 5;\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 5;\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 5;
\lsdpriority46 \lsdlocked0 List Table 1 Light Accent 6;\lsdpriority47 \lsdlocked0 List Table 2 Accent 6;\lsdpriority48 \lsdlocked0 List Table 3 Accent 6;\lsdpriority49 \lsdlocked0 List Table 4 Accent 6;
\lsdpriority50 \lsdlocked0 List Table 5 Dark Accent 6;\lsdpriority51 \lsdlocked0 List Table 6 Colorful Accent 6;\lsdpriority52 \lsdlocked0 List Table 7 Colorful Accent 6;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Mention;
\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Smart Hyperlink;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Hashtag;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Unresolved Mention;\lsdsemihidden1 \lsdunhideused1 \lsdlocked0 Smart Link;}}{\*\datastore 01050000
02000000180000004d73786d6c322e534158584d4c5265616465722e362e3000000000000000000000060000
d0cf11e0a1b11ae1000000000000000000000000000000003e000300feff090006000000000000000000000001000000010000000000000000100000feffffff00000000feffffff0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
fffffffffffffffffdfffffffeffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
ffffffffffffffffffffffffffffffff52006f006f007400200045006e00740072007900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000016000500ffffffffffffffffffffffff0c6ad98892f1d411a65f0040963251e500000000000000000000000090c9
6d54d898da01feffffff00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff00000000000000000000000000000000000000000000000000000000
00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff0000000000000000000000000000000000000000000000000000
000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ffffffffffffffffffffffff000000000000000000000000000000000000000000000000
0000000000000000000000000000000000000000000000000105000000000000}}