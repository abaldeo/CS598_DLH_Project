{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE00ZgGBSIX1qWpgNrHqYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abaldeo/CS598_DLH_Project/blob/CNN_DDI/CNN_DDI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rycE4UOdrBnS",
        "outputId": "9b3211a7-2bc5-48e8-b1fd-f7fb83a69a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install scikit-learn\n",
        "!pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from NLPProcess import NLPProcess\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "#from tensorflow import set_random_seed\n",
        "#set_random_seed(2)\n",
        "import csv\n",
        "import sqlite3\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n",
        "# from keras.callbacks import EarlyStopping\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "\n",
        "event_num = 65\n",
        "droprate = 0.3\n",
        "vector_size = 572\n",
        "\n",
        "checkpoint_path = \"checkpoints/cp.ckpt\"\n",
        "\n",
        "# Create a ModelCheckpoint callback that saves the model's weights\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                              save_weights_only=True,\n",
        "                              verbose=1)\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath='checkpoints/model_{epoch:02d}-{val_loss:.2f}.hdf5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1)\n",
        "\n",
        "def DNN():\n",
        "    train_input = Input(shape=(vector_size * 2,), name='Inputlayer')\n",
        "    train_in = Dense(512, activation='relu')(train_input)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(256, activation='relu')(train_in)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(event_num)(train_in)\n",
        "    out = Activation('softmax')(train_in)\n",
        "    model = Model(inputs=train_input, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Input, Dense, Conv1D, BatchNormalization, Activation, Flatten, Dropout, Add\n",
        "from tensorflow.keras.layers import Conv1D, Flatten, Add\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "def CNN_DDI():\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=(572, 2), name='InputLayer')\n",
        "\n",
        "    # Convolutional layers as specified in the paper\n",
        "    conv1 = Conv1D(filters=64, kernel_size=3, strides=1, padding='same')(inputs)\n",
        "    conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
        "\n",
        "    conv2 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv1)\n",
        "    conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
        "\n",
        "    # Residual block starts\n",
        "    conv3_1 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv2)\n",
        "    conv3_1 = LeakyReLU(alpha=0.2)(conv3_1)\n",
        "\n",
        "    conv3_2 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv3_1)\n",
        "    conv3_2 = LeakyReLU(alpha=0.2)(conv3_2)\n",
        "\n",
        "    # Add the input of the residual block (conv2) to its output (conv3_2)\n",
        "    res_out = Add()([conv2, conv3_2])\n",
        "    # Residual block ends\n",
        "\n",
        "    conv4 = Conv1D(filters=256, kernel_size=3, strides=1, padding='same')(res_out)\n",
        "    conv4 = LeakyReLU(alpha=0.2)(conv4)\n",
        "\n",
        "    # Flatten the output of the last convolutional layer\n",
        "    flatten = Flatten()(conv4)\n",
        "\n",
        "    # Fully connected layers\n",
        "    fc1 = Dense(267, activation='relu')(flatten)\n",
        "    # fc1 = BatchNormalization()(fc1)\n",
        "    # fc1 = Dropout(droprate)(fc1)\n",
        "\n",
        "    fc2 = Dense(event_num)(fc1)  # Assuming 'num_classes' is the number of DDI event types\n",
        "    out = Activation('softmax')(fc2)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def prepare(df_drug, feature_list, vector_size,mechanism,action,drugA,drugB):\n",
        "    d_label = {}\n",
        "    d_feature = {}\n",
        "    # Transfrom the interaction event to number\n",
        "    # Splice the features\n",
        "    d_event=[]\n",
        "    for i in range(len(mechanism)):\n",
        "        d_event.append(mechanism[i]+\" \"+action[i])\n",
        "    label_value = 0\n",
        "    count={}\n",
        "    for i in d_event:\n",
        "        if i in count:\n",
        "            count[i]+=1\n",
        "        else:\n",
        "            count[i]=1\n",
        "    list1 = sorted(count.items(), key=lambda x: x[1],reverse=True)\n",
        "    for i in range(len(list1)):\n",
        "        d_label[list1[i][0]]=i\n",
        "    vector = np.zeros((len(np.array(df_drug['name']).tolist()), 0), dtype=float)\n",
        "    for i in feature_list:\n",
        "        vector = np.hstack((vector, feature_vector(i, df_drug, vector_size)))\n",
        "    # Transfrom the drug ID to feature vector\n",
        "    for i in range(len(np.array(df_drug['name']).tolist())):\n",
        "        d_feature[np.array(df_drug['name']).tolist()[i]] = vector[i]\n",
        "    # Use the dictionary to obtain feature vector and label\n",
        "    new_feature = []\n",
        "    new_label = []\n",
        "    name_to_id = {}\n",
        "    for i in range(len(d_event)):\n",
        "        new_feature.append(np.hstack((d_feature[drugA[i]], d_feature[drugB[i]])))\n",
        "        new_label.append(d_label[d_event[i]])\n",
        "    new_feature = np.array(new_feature)\n",
        "    new_label = np.array(new_label)\n",
        "    return (new_feature, new_label, event_num)\n",
        "\n",
        "\n",
        "def feature_vector(feature_name, df, vector_size):\n",
        "    # df are the 572 kinds of drugs\n",
        "    # Jaccard Similarity\n",
        "    def Jaccard(matrix):\n",
        "        matrix = np.mat(matrix)\n",
        "        numerator = matrix * matrix.T\n",
        "        denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n",
        "        return numerator / denominator\n",
        "\n",
        "    all_feature = []\n",
        "    drug_list = np.array(df[feature_name]).tolist()\n",
        "    # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
        "    for i in drug_list:\n",
        "        for each_feature in i.split('|'):\n",
        "            if each_feature not in all_feature:\n",
        "                all_feature.append(each_feature)  # obtain all the features\n",
        "    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
        "    df_feature = DataFrame(feature_matrix, columns=all_feature)  # Consrtuct feature matrices with key of dataframe\n",
        "    for i in range(len(drug_list)):\n",
        "        for each_feature in df[feature_name].iloc[i].split('|'):\n",
        "            df_feature[each_feature].iloc[i] = 1\n",
        "    sim_matrix = Jaccard(np.array(df_feature))\n",
        "\n",
        "    sim_matrix1 = np.array(sim_matrix)\n",
        "    count = 0\n",
        "    pca = PCA(n_components=vector_size)  # PCA dimension\n",
        "    sim_matrix = np.asarray(sim_matrix)\n",
        "    pca.fit(sim_matrix)\n",
        "    sim_matrix = pca.transform(sim_matrix)\n",
        "    return sim_matrix\n",
        "\n",
        "\n",
        "def get_index(label_matrix, event_num, seed, CV):\n",
        "    index_all_class = np.zeros(len(label_matrix))\n",
        "    for j in range(event_num):\n",
        "        index = np.where(label_matrix == j)\n",
        "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
        "        k_num = 0\n",
        "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
        "            index_all_class[index[0][test_index]] = k_num\n",
        "            k_num += 1\n",
        "\n",
        "    return index_all_class\n",
        "\n",
        "\n",
        "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    y_score = np.zeros((0, event_num), dtype=float)\n",
        "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
        "    matrix = []\n",
        "    if type(feature_matrix) != list:\n",
        "        matrix.append(feature_matrix)\n",
        "        # =============================================================================\n",
        "        #     elif len(np.shape(feature_matrix))==3:\n",
        "        #         for i in range((np.shape(feature_matrix)[-1])):\n",
        "        #             matrix.append(feature_matrix[:,:,i])\n",
        "        # =============================================================================\n",
        "        feature_matrix = matrix\n",
        "    for k in range(CV):\n",
        "        train_index = np.where(index_all_class != k)\n",
        "        test_index = np.where(index_all_class == k)\n",
        "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
        "        # dnn=DNN()\n",
        "        print(len(feature_matrix))\n",
        "        for i in range(len(feature_matrix)):\n",
        "            x_train = feature_matrix[i][train_index]\n",
        "            x_test = feature_matrix[i][test_index]\n",
        "            y_train = label_matrix[train_index]\n",
        "            # one-hot encoding\n",
        "            y_train_one_hot = np.array(y_train)\n",
        "            y_train_one_hot = (np.arange(y_train_one_hot.max() + 1) == y_train[:, None]).astype(dtype='float32')\n",
        "            y_test = label_matrix[test_index]\n",
        "            # one-hot encoding\n",
        "            y_test_one_hot = np.array(y_test)\n",
        "            y_test_one_hot = (np.arange(y_test_one_hot.max() + 1) == y_test[:, None]).astype(dtype='float32')\n",
        "            if clf_type == 'DDIMDL':\n",
        "                dnn = DNN()\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "                dnn.fit(x_train, y_train_one_hot, batch_size=128, epochs=100, validation_data=(x_test, y_test_one_hot),\n",
        "                        callbacks=[early_stopping])\n",
        "                pred += dnn.predict(x_test)\n",
        "                continue\n",
        "            elif clf_type == 'CNN_DDI':\n",
        "                x_train_reshaped = x_train.reshape(-1, 572, 2)\n",
        "                x_test_reshaped = x_test.reshape(-1, 572, 2)\n",
        "                cnn_ddi = CNN_DDI()\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "                cnn_ddi.fit(x_train_reshaped, y_train_one_hot, batch_size=128, epochs=100, validation_data=(x_test_reshaped, y_test_one_hot),\n",
        "                            callbacks=[early_stopping, cp_callback])\n",
        "                pred += cnn_ddi.predict(x_test_reshaped)\n",
        "                continue\n",
        "            elif clf_type == 'RF':\n",
        "                clf = RandomForestClassifier(n_estimators=100)\n",
        "            elif clf_type == 'GBDT':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'SVM':\n",
        "                clf = SVC(probability=True)\n",
        "            elif clf_type == 'FM':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'KNN':\n",
        "                clf = KNeighborsClassifier(n_neighbors=4)\n",
        "            else:\n",
        "                clf = LogisticRegression()\n",
        "            clf.fit(x_train, y_train)\n",
        "            pred += clf.predict_proba(x_test)\n",
        "        pred_score = pred / len(feature_matrix)\n",
        "        pred_type = np.argmax(pred_score, axis=1)\n",
        "        y_true = np.hstack((y_true, y_test))\n",
        "        y_pred = np.hstack((y_pred, pred_type))\n",
        "        y_score = np.row_stack((y_score, pred_score))\n",
        "    result_all, result_eve = evaluate(y_pred, y_score, y_true, event_num, set_name)\n",
        "    # =============================================================================\n",
        "    #         a,b=evaluate(pred_type,pred_score,y_test,event_num)\n",
        "    #         for i in range(all_eval_type):\n",
        "    #             result_all[i]+=a[i]\n",
        "    #         for i in range(each_eval_type):\n",
        "    #             result_eve[:,i]+=b[:,i]\n",
        "    #     result_all=result_all/5\n",
        "    #     result_eve=result_eve/5\n",
        "    # =============================================================================\n",
        "    return result_all, result_eve\n",
        "\n",
        "\n",
        "def evaluate(pred_type, pred_score, y_test, event_num, set_name):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_one_hot = label_binarize(y_test, classes=np.arange(event_num))\n",
        "    pred_one_hot = label_binarize(pred_type, classes=np.arange(event_num))\n",
        "\n",
        "    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n",
        "\n",
        "    result_all[0] = accuracy_score(y_test, pred_type)\n",
        "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[5] = f1_score(y_test, pred_type, average='micro')\n",
        "    result_all[6] = f1_score(y_test, pred_type, average='macro')\n",
        "    result_all[7] = precision_score(y_test, pred_type, average='micro')\n",
        "    result_all[8] = precision_score(y_test, pred_type, average='macro')\n",
        "    result_all[9] = recall_score(y_test, pred_type, average='micro')\n",
        "    result_all[10] = recall_score(y_test, pred_type, average='macro')\n",
        "    for i in range(event_num):\n",
        "        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n",
        "        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                          average=None)\n",
        "        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                         average=None)\n",
        "        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                    average='binary')\n",
        "        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                           average='binary')\n",
        "        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                        average='binary')\n",
        "    return [result_all, result_eve]\n",
        "\n",
        "\n",
        "def self_metric_calculate(y_true, pred_type):\n",
        "    y_true = y_true.ravel()\n",
        "    y_pred = pred_type.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_pred.ndim == 1:\n",
        "        y_pred = y_pred.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_pred_c = y_pred.take([0], axis=1).ravel()\n",
        "    TP = 0\n",
        "    TN = 0\n",
        "    FN = 0\n",
        "    FP = 0\n",
        "    for i in range(len(y_true_c)):\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 1):\n",
        "            TP += 1\n",
        "        if (y_true_c[i] == 1) and (y_pred_c[i] == 0):\n",
        "            FN += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 1):\n",
        "            FP += 1\n",
        "        if (y_true_c[i] == 0) and (y_pred_c[i] == 0):\n",
        "            TN += 1\n",
        "    print(\"TP=\", TP, \"FN=\", FN, \"FP=\", FP, \"TN=\", TN)\n",
        "    return (TP / (TP + FP), TP / (TP + FN))\n",
        "\n",
        "\n",
        "def multiclass_precision_recall_curve(y_true, y_score):\n",
        "    y_true = y_true.ravel()\n",
        "    y_score = y_score.ravel()\n",
        "    if y_true.ndim == 1:\n",
        "        y_true = y_true.reshape((-1, 1))\n",
        "    if y_score.ndim == 1:\n",
        "        y_score = y_score.reshape((-1, 1))\n",
        "    y_true_c = y_true.take([0], axis=1).ravel()\n",
        "    y_score_c = y_score.take([0], axis=1).ravel()\n",
        "    precision, recall, pr_thresholds = precision_recall_curve(y_true_c, y_score_c)\n",
        "    return (precision, recall, pr_thresholds)\n",
        "\n",
        "\n",
        "def roc_aupr_score(y_true, y_score, average=\"macro\"):\n",
        "    def _binary_roc_aupr_score(y_true, y_score):\n",
        "        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _average_binary_score(binary_metric, y_true, y_score, average):  # y_true= y_one_hot\n",
        "        if average == \"binary\":\n",
        "            return binary_metric(y_true, y_score)\n",
        "        if average == \"micro\":\n",
        "            y_true = y_true.ravel()\n",
        "            y_score = y_score.ravel()\n",
        "        if y_true.ndim == 1:\n",
        "            y_true = y_true.reshape((-1, 1))\n",
        "        if y_score.ndim == 1:\n",
        "            y_score = y_score.reshape((-1, 1))\n",
        "        n_classes = y_score.shape[1]\n",
        "        score = np.zeros((n_classes,))\n",
        "        for c in range(n_classes):\n",
        "            y_true_c = y_true.take([c], axis=1).ravel()\n",
        "            y_score_c = y_score.take([c], axis=1).ravel()\n",
        "            score[c] = binary_metric(y_true_c, y_score_c)\n",
        "        return np.average(score)\n",
        "\n",
        "    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)\n",
        "\n",
        "\n",
        "def drawing(d_result, contrast_list, info_list):\n",
        "    column = []\n",
        "    for i in contrast_list:\n",
        "        column.append(i)\n",
        "    df = pd.DataFrame(columns=column)\n",
        "    if info_list[-1] == 'aupr':\n",
        "        for i in contrast_list:\n",
        "            df[i] = d_result[i][:, 1]\n",
        "    else:\n",
        "        for i in contrast_list:\n",
        "            df[i] = d_result[i][:, 2]\n",
        "    df = df.astype('float')\n",
        "    color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray')\n",
        "    df.plot.box(ylim=[0, 1.0], grid=True, color=color)\n",
        "    return 0\n",
        "\n",
        "\n",
        "def save_result(feature_name, result_type, clf_type, result):\n",
        "    with open(feature_name + '_' + result_type + '_' + clf_type+ '.csv', \"w\", newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for i in result:\n",
        "            writer.writerow(i)\n",
        "    return 0\n",
        "\n",
        "# Instead of using argparse for command-line arguments, set the parameters directly.\n",
        "args = {\n",
        "    \"featureList\": [\"smile\", \"target\", \"enzyme\", \"category\"],\n",
        "    \"classifier\": [\"CNN_DDI\"],\n",
        "    \"NLPProcess\": \"read\"\n",
        "}\n",
        "\n",
        "import os\n",
        "# Main function adjusted for Jupyter Notebook\n",
        "def main(args):\n",
        "    seed = 0\n",
        "    CV = 5\n",
        "    interaction_num = 10\n",
        "    print(os.path.curdir)\n",
        "    # Ensure you have the 'event.db' file accessible in your Google Colab environment.\n",
        "    # You might need to upload it or access it from Google Drive.\n",
        "    conn = sqlite3.connect(\"event.db\")\n",
        "    df_drug = pd.read_sql('select * from drug;', conn)\n",
        "    df_event = pd.read_sql('select * from event_number;', conn)\n",
        "    df_interaction = pd.read_sql('select * from event;', conn)\n",
        "\n",
        "    feature_list = args['featureList']\n",
        "    featureName = \"+\".join(feature_list)\n",
        "    clf_list = args['classifier']\n",
        "    for feature in feature_list:\n",
        "        set_name = feature + '+'\n",
        "    set_name = set_name[:-1]\n",
        "    result_all = {}\n",
        "    result_eve = {}\n",
        "    all_matrix = []\n",
        "    drugList = []\n",
        "    for line in open(\"DrugList.txt\", 'r'):\n",
        "        drugList.append(line.split()[0])\n",
        "    if args['NLPProcess'] == \"read\":\n",
        "        extraction = pd.read_sql('select * from extraction;', conn)\n",
        "        mechanism = extraction['mechanism']\n",
        "        action = extraction['action']\n",
        "        drugA = extraction['drugA']\n",
        "        drugB = extraction['drugB']\n",
        "    else:\n",
        "        pass\n",
        "        # mechanism, action, drugA, drugB = NLPProcess(drugList, df_interaction)\n",
        "\n",
        "    for feature in feature_list:\n",
        "        print(feature)\n",
        "        new_feature, new_label, event_num = prepare(df_drug, [feature], vector_size, mechanism, action, drugA, drugB)\n",
        "        all_matrix.append(new_feature)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "\n",
        "    for clf in clf_list:\n",
        "        print(clf)\n",
        "        all_result, each_result = cross_validation(all_matrix, new_label, clf, event_num, seed, CV, set_name)\n",
        "        save_result(featureName, 'all', clf, all_result)\n",
        "        save_result(featureName, 'each', clf, each_result)\n",
        "        result_all[clf] = all_result\n",
        "        result_eve[clf] = each_result\n",
        "    print(\"time used:\", time.perf_counter() - start)\n",
        "\n",
        "# Call the main function with the predefined arguments\n",
        "main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "LRwhaVSTEpKx",
        "outputId": "46616375-c258-4126-832d-1012fc4b39bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "Num GPUs Available:  0\n",
            ".\n",
            "smile\n",
            "target\n",
            "enzyme\n",
            "category\n",
            "CNN_DDI\n",
            "4\n",
            "Epoch 1/100\n",
            " 17/233 [=>............................] - ETA: 13:40 - loss: 2.6386 - accuracy: 0.2872"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-4184c091b2e8>\u001b[0m in \u001b[0;36m<cell line: 473>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;31m# Call the main function with the predefined arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-4184c091b2e8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0mall_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0msave_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'each'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-4184c091b2e8>\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(feature_matrix, label_matrix, clf_type, event_num, seed, CV, set_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mcnn_ddi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_DDI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 cnn_ddi.fit(x_train_reshaped, y_train_one_hot, batch_size=128, epochs=100, validation_data=(x_test_reshaped, y_test_one_hot),\n\u001b[0m\u001b[1;32m    253\u001b[0m                             callbacks=[early_stopping, cp_callback])\n\u001b[1;32m    254\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcnn_ddi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGw0ucWZLt41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}