{"cells":[{"cell_type":"markdown","metadata":{"id":"6b8qqIE1tPws"},"source":["# CS598 DL4H: Project Notebook - Draft\n","---\n","**Paper Title** - CNN-DDI: a learning-based method for predicting drug–drug interactions using convolution neural networks\n","\n","**Members:**\n","\n","*   Avinash Baldeo (abaldeo2@illinois.edu)\n","*   Jinfeng Wu (jinfeng4@illinois.edu)\n","*   Hao Zhang (haoz8@illinois.edu)\n"]},{"cell_type":"markdown","metadata":{"id":"GRflEXep3Z99"},"source":["# Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["## Background of the Problem\n","\n","\n","### 1. Type of Problem\n","\n","The problem addressed in the paper is the prediction of drug-drug interactions (DDIs). Specifically, the research focuses on developing a computational method to predict whether two drugs will interact and the type of interaction they may have, which can be synergistic, antagonistic, or neutral (no reaction). This is a problem of feature engineering and predictive modeling within the domain of bioinformatics and pharmacology.\n","\n","### 2. Importance of the Problem\n","\n","Predicting DDIs is crucial for pharmaceuticals development for several reasons:\n","\n","- Antagonistic interactions can lead to reduced drug efficacy or increased toxicity, which can harm patients. Predicting such interactions can prevent adverse drug events.\n","-  Early prediction of DDIs can save time and resources in the drug development process by identifying potential issues before clinical trials.\n","- Understanding DDIs is key to customizing drug regimens for individual patients, especially those on multiple medications.\n","- By avoiding adverse drug reactions, we can reduce hospital readmissions and other healthcare costs associated with managing drug complications.\n","- Knowing if two drugs interact is also useful since drugs similar to either of the two are more likely to interact and cause the same effect.\n","\n","### 3. Difficulty of the Problem\n","\n","The prediction of DDIs is challenging due to:\n","\n","- The way drugs interact can be influenced by numerous factors, including genetics, lifestyle, and the presence of other drugs.\n","- Drug data is typically heterogenous and comes from various sources and in different formats, making it difficult to integrate and analyze.\n","- Drugs can be described by many features, such as chemical properties, targets, and pathways, leading to high-dimensional data that is hard to process.\n","- Many potential drug combinations have not been observed or recorded, leading to sparse data and making it difficult to predict interactions for new or less-studied drugs.\n","\n","### 4. State of the Art Methods and Effectiveness\n","\n","The paper reviews mentions traditional and state-of-the-art methods for predicting DDIs:\n","\n","-  Traditional methods include text mining and statistical methods, which have been the foundation for early DDI prediction efforts.\n","- Logistic regression and other classical machine learning models have been used to predict DDIs by analyzing drug features.\n","- More recent studies have applied deep learning methods, such as Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs), to predict DDIs. These methods can automatically learn complex representations of drug features.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xoPQ6DkV3JXh"},"source":["\n","## Paper Explanation\n","\n","\n","### 1. What did the paper propose?\n","\n","The paper proposed a novel algorithm named CNN-DDI, which utilizes a convolutional neural network (CNN) architecture to predict drug-drug interactions (DDIs). The method involves two main components:\n","\n","- This feature selection part of the algorithm calculates the similarity between drug feature vectors using measures like Jaccard similarity. It then optimizes the data from high dimension to low dimension to generate feature vectors as input for the prediction module.\n","- The prediction module uses the feature vectors from the selection module as inputs to predict DDI-associated events. The CNN model consists of five convolutional layers, two fully connected layers, a residual block and a softmax layer.\n","\n","### 2. What are the innovations of the method?\n","\n","The main innovations of the CNN-DDI paper include:\n","\n","- The method uses a combination of drug categories, targets, pathways, and enzymes as feature vectors, which was unique in the context of DDI prediction.\n","-  While deep neural networks (DNNs) have been used in the past for similar tasks, the paper highlights the advantages of CNNs in feature learning and reducing overfitting and dealing with noise in input.\n","- The paper demonstrates that using a combination of multiple drug features leads to more informative and effective predictions than using a single feature type.\n","- The CNN-DDI algorithm is works well for different similarity measures (Jaccard, cosine, and Gaussian), with Jaccard similarity being used in the experiments.\n","\n","### 3. How well the proposed method work (in its own metrics)\n","\n","The proposed CNN-DDI method performed well according to the metrics used in the study. The CNN-DDI algorithm outperforms other methods like Random Forest (RF), Gradient Boosting Decision Tree (GBDT), Logistic Regression (LR), and K-Nearest Neighbor (KNN) across several evaluation metrics. Specifically, CNN-DDI achieved the highest scores in accuracy (ACC), area under the precision-recall curve (AUPR), area under the ROC curve (AUC), F1-score, precision, and recall. The paper also compared CNN-DDI with prior DNN model (DDIMDL) and claimed that CNN-DDI still performed better even when using the same features.\n","\n","### 4. What is the contribution to the research regime?\n","\n","- The paper tries to advance DDI prediction by introducing a novel method that leverages the strengths of CNNs for feature learning and prediction.\n","- By providing a more accurate method for predicting DDIs, the paper contributes to the drug development process, potentially reducing the time and resources spent on identifying adverse drug interactions.\n","- The method could be used to improve patient safety by better predicting potential DDIs, thus preventing adverse effects that could arise from drug combinations.\n","- The CNN-DDI algorithm could be instrumental in the field of personalized medicine, where accurate DDI predictions are essential for tailoring drug regimens to individual patients.\n","\n","The CNN-DDI paper demonstrates the importance of feature learning and the potential of deep learning in improving the prediction of DDIs.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility:\n","\n","The primary hypothesis of the paper is that the CNN-DDI model, which utilizes a feature selection framework and a novel CNN architecture, can accurately predict drug-drug interactions and outperform other the models mentioned in the paper (Random forest, Logistic Regression, K-nearest neighbor, Gradient boosted Decision Tree, & DDIMDL). This hypothesis is tested by implementing the CNN-DDI model according to the details mentioned in the paper and training on the the collected dataset with the same (inferred) hyperparameters settings. Afterwards, we compare the results with table 3 and 4 from the paper to our results.\n","\n","\n","![Table 3.png](https://drive.google.com/uc?export=view&id=1aUzLZji-e5m8w1fwoFG_2ZB5fsBeMVtk)\n","\n","\n","![Table 4.png](https://drive.google.com/uc?export=view&id=1vTVHcCusQn0WMRqyI3E4_s_Fz6yW2CaO)\n"]},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology"]},{"cell_type":"markdown","metadata":{"id":"6VQM3tSQWdVX"},"source":["## Environment\n","\n","Most of the code in this notebook is taken from the [github repo](https://github.com/YifanDengWHU/DDIMDL) for the paper \"A multimodal deep learning framework for predicting drug-drug interaction events\" [2]. This code was written for Python 3.7 and had the following dependencies.\n","\n","* numpy (==1.18.1)\n","* Keras (==2.2.4)\n","* pandas (==1.0.1)\n","* scikit-learn (==0.21.2)\n","* tensorflow (==1.15)\n","\n","The code for this notebook is updated to be run in google colab environment (Python 3.10) and is tested with the following package versions:\n","    \n","\n","* numpy (==1.25.2)\n","* pandas (==2.0.3)\n","* scikit-learn (==1.2.2)\n","* tensorflow (==2.15.0)\n","* tqdm (==4.66.2)\n","* psutil (==5.9.5)\n","* gdown (==4.7.3)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1713150837253,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"TbsGVO_gWmOK","outputId":"988f510a-1020-4265-fa4c-aaebb2830df9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python --version"]},{"cell_type":"markdown","metadata":{"id":"R5P7WfAQWpJ0"},"source":["If not running in Google colab environment, uncomment & run the following commands to install all dependencies.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713150837567,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"ZM4MNoGpWrps"},"outputs":[],"source":["# !pip install numpy==1.25.2\n","# !pip install pandas==2.0.3\n","# !pip install scikit-learn==1.2.2\n","# !pip install tensorflow==2.15.0\n","# !pip install tqdm==4.66.2\n","# !pip install psutil==5.9.5\n","# !pip install gdown==4.7.3"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10940,"status":"ok","timestamp":1713150848505,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"pnbtVsbsPPBk","outputId":"923b230c-85bc-4cac-ca25-b6009547f46c"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.15.0\n","Num GPUs Available:  1\n"]}],"source":["import sys\n","import os\n","import random\n","import csv\n","import sqlite3\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from pandas import DataFrame\n","from tqdm import tqdm\n","\n","# set seed\n","seed = 1\n","# random.seed(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713150848505,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"FQsMfcHUGnzP"},"outputs":[],"source":["def set_max_gpu_mem(size=10,unit=1024):\n","  limit = size * unit\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if gpus:\n","      try:\n","          # Restrict TensorFlow to only allocate specified memory on the first GPU\n","          print(f\"Setting GPU memory limit to {size}GB.\")\n","          tf.config.experimental.set_virtual_device_configuration(\n","              gpus[0],\n","              [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit)])\n","          logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","          print(len(gpus), \"Physical GPU,\", len(logical_gpus), f\"Logical GPU(s) with memory limit:{limit}\")\n","      except RuntimeError as e:\n","          # Virtual devices must be set before GPUs have been initialized\n","          print(e)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713150848505,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"-yiVxkATBqKf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9602ad2-3d77-4ff3-bb65-6244ddbc1bbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting GPU memory limit to 10GB.\n","1 Physical GPU, 1 Logical GPU(s) with memory limit:10240\n"]}],"source":["# SET MEMORY LIMIT TO 10GB AVOID CRASHING DURING TRAINING\n","# ADJUST AS NEEEDED\n","set_max_gpu_mem(10)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1713150848851,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"plkT4QTqcqXB"},"outputs":[],"source":["import psutil\n","PROCESS = psutil.Process(os.getpid())\n","\n","def print_memory_usage(unit_size=10 ** 6):\n","    \"\"\"Prints current memory usage stats.\n","    See: https://stackoverflow.com/a/15495136\n","\n","    :return: None\n","    \"\"\"\n","    total, available, percent, used, free, *_ = psutil.virtual_memory()\n","    total, available, used, free = total / unit_size, available / unit_size, used / unit_size, free / unit_size\n","    proc = PROCESS.memory_info()[1] / unit_size\n","    print('process = %s total = %s available = %s used = %s free = %s percent = %s'\n","          % (proc, total, available, used, free, percent))\n"]},{"cell_type":"markdown","metadata":{"id":"9auBerjAGNyc"},"source":["## Data\n","The  dataset used by CNN-DDI is from the DDIMDL Github repository (https://github.com/YifanDengWHU/DDIMDL). The DDIMDL paper classifies  DDIs’ events into 65 types and includes 572 drugs with more than 70,000 associated events. The data was originally collected from the DrugBank website (https://go.drugbank.com/)  using a web scraper and then processed and stored into a SQLite database (event.db). To utilize this dataset for CNN-DDI, we had to extract the 1622 category types for the drugs in the database from the Drugbank and store it.\n","The database is stored in the public google drive folder and should be automatically downloaded from there. The updated scraper code to include category is our github repo and is purposely left out of this notebook to save time as its not needed to be run again."]},{"cell_type":"markdown","metadata":{"id":"gyg9GA0XmNg_"},"source":["### Data Loading"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1713150848851,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"fO_jiULwAIxV"},"outputs":[],"source":["# Function to check if running on Google Colab\n","def is_running_on_colab():\n","    return 'google.colab' in sys.modules"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32364,"status":"ok","timestamp":1713150881214,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"7E5oSxHQPy_m","outputId":"dd9253f8-8dea-4a79-e46b-f14404d372e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["downloading from public drive folder\n"]},{"output_type":"stream","name":"stderr","text":["Retrieving folder contents\n"]},{"output_type":"stream","name":"stdout","text":["Retrieving folder 1-IKo1Uf3DPJp6br-bKR1V47ZYKbOLEY4 .ipynb_checkpoints\n","Processing file 1XJY2IvPXVUr2mnNSjARSh8YUPzIsAtnk Ablation Plan.png\n","Processing file 1VhwNiJHXzmdARWhGrWx_JX88MaIBYka6 cnn_ddi_model.h5\n","Processing file 1oRINnkMG0ay3myhE2bkbS2-lo148DwYU cp.ckpt.index\n","Processing file 1Jd9gZlXapv8wHyhkJYMEngw6rZPC7yAr DrugList.txt\n","Processing file 1PVEf5gtFZdWDerqgThdXlhSBUajtrXHy event.db\n","Processing file 17OwHb_qH209fo-DkawQs8R7E3OtTmCbr Reproduction Results Comparison.xlsx\n","Processing file 1KVYOLkXTHgUw38q08FyOsKAOL3LElHzY smile+target+enzyme+category_all_CNN_DDI.csv\n","Processing file 1ctCNfuNeSk5j5rI9I_-F2BVWr808QEEE smile+target+enzyme+category_each_CNN_DDI.csv\n","Processing file 16rToBuMrW_eDXfOA2EHqpP8ojPWnbGY5 Table 2 R.png\n","Processing file 1cD-GkXTKiNEwVONa5kzUf5jpVly2oJhx Table 2.png\n","Processing file 11SNLsWjDe9RqzCc90UUiDOieNMprl4AZ Table 3 R.png\n","Processing file 1aUzLZji-e5m8w1fwoFG_2ZB5fsBeMVtk Table 3.png\n","Processing file 13zFOSsoLenVKDi7mRLMJA-7_VwZqc2Ul Table 4 R.png\n","Processing file 1vTVHcCusQn0WMRqyI3E4_s_Fz6yW2CaO Table 4.png\n","Processing file 1n3Ls-m1vIuZqEyUs-1m5wZzQKVsBAGs2 Table 5.PNG\n","Building directory structure completed\n"]},{"output_type":"stream","name":"stderr","text":["Retrieving folder contents completed\n","Building directory structure\n","Downloading...\n","From: https://drive.google.com/uc?id=1XJY2IvPXVUr2mnNSjARSh8YUPzIsAtnk\n","To: /content/CNN_DDI/Ablation Plan.png\n","100%|██████████| 80.9k/80.9k [00:00<00:00, 20.0MB/s]\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1VhwNiJHXzmdARWhGrWx_JX88MaIBYka6\n","From (redirected): https://drive.google.com/uc?id=1VhwNiJHXzmdARWhGrWx_JX88MaIBYka6&confirm=t&uuid=bac5fc17-9233-4fd3-abad-4fc632ebdc40\n","To: /content/CNN_DDI/cnn_ddi_model.h5\n","100%|██████████| 472M/472M [00:06<00:00, 70.7MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1oRINnkMG0ay3myhE2bkbS2-lo148DwYU\n","To: /content/CNN_DDI/cp.ckpt.index\n","100%|██████████| 3.35k/3.35k [00:00<00:00, 8.08MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1Jd9gZlXapv8wHyhkJYMEngw6rZPC7yAr\n","To: /content/CNN_DDI/DrugList.txt\n","100%|██████████| 7.61k/7.61k [00:00<00:00, 16.4MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1PVEf5gtFZdWDerqgThdXlhSBUajtrXHy\n","To: /content/CNN_DDI/event.db\n","100%|██████████| 30.6M/30.6M [00:00<00:00, 48.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=17OwHb_qH209fo-DkawQs8R7E3OtTmCbr\n","To: /content/CNN_DDI/Reproduction Results Comparison.xlsx\n","100%|██████████| 180k/180k [00:00<00:00, 77.0MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1KVYOLkXTHgUw38q08FyOsKAOL3LElHzY\n","To: /content/CNN_DDI/smile+target+enzyme+category_all_CNN_DDI.csv\n","100%|██████████| 220/220 [00:00<00:00, 86.5kB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1ctCNfuNeSk5j5rI9I_-F2BVWr808QEEE\n","To: /content/CNN_DDI/smile+target+enzyme+category_each_CNN_DDI.csv\n","100%|██████████| 6.57k/6.57k [00:00<00:00, 19.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=16rToBuMrW_eDXfOA2EHqpP8ojPWnbGY5\n","To: /content/CNN_DDI/Table 2 R.png\n","100%|██████████| 96.7k/96.7k [00:00<00:00, 31.6MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1cD-GkXTKiNEwVONa5kzUf5jpVly2oJhx\n","To: /content/CNN_DDI/Table 2.png\n","100%|██████████| 234k/234k [00:00<00:00, 86.8MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=11SNLsWjDe9RqzCc90UUiDOieNMprl4AZ\n","To: /content/CNN_DDI/Table 3 R.png\n","100%|██████████| 82.3k/82.3k [00:00<00:00, 44.7MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1aUzLZji-e5m8w1fwoFG_2ZB5fsBeMVtk\n","To: /content/CNN_DDI/Table 3.png\n","100%|██████████| 88.2k/88.2k [00:00<00:00, 88.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=13zFOSsoLenVKDi7mRLMJA-7_VwZqc2Ul\n","To: /content/CNN_DDI/Table 4 R.png\n","100%|██████████| 99.5k/99.5k [00:00<00:00, 62.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1vTVHcCusQn0WMRqyI3E4_s_Fz6yW2CaO\n","To: /content/CNN_DDI/Table 4.png\n","100%|██████████| 95.0k/95.0k [00:00<00:00, 76.1MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1n3Ls-m1vIuZqEyUs-1m5wZzQKVsBAGs2\n","To: /content/CNN_DDI/Table 5.PNG\n","100%|██████████| 53.6k/53.6k [00:00<00:00, 82.4MB/s]\n","Download completed\n"]}],"source":["if is_running_on_colab() and 0: # enable/disable using my drive folder\n","    from google.colab import drive\n","    print('using my drive folder')\n","    # Mount Google Drive\n","    DRIVE_PATH = '/content/drive'\n","    drive.mount(DRIVE_PATH)\n","    BASE_PATH = f\"{DRIVE_PATH}/My Drive/CNN_DDI/\"\n","else:\n","    # use gdown to download from public drive folder\n","    import gdown\n","    print('downloading from public drive folder')\n","    url = \"https://drive.google.com/drive/folders/1ln1ga9J7XzwAnAikKXS-ejXcPLe27jgI?usp=sharing\"\n","    output_folder = 'CNN_DDI'\n","    gdown.download_folder(url, output=output_folder, quiet=False)\n","    BASE_PATH = f\"./{output_folder}/\""]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713150881214,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"AnwdLmszRPps"},"outputs":[],"source":["# Define data and model path\n","EVENT_DATA_PATH = BASE_PATH + \"event.db\"\n","# DRUG_LIST_PATH =  BASE_PATH + \"DrugList.txt\"\n","# WEIGHT_PATH = \"/content/drive/My Drive/CNN_DDI/checkpoints/cp.ckpt.index\""]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713150881215,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"sNfjXJ-g-K4f"},"outputs":[],"source":["VECTOR_SIZE = 572                       # num drugs (model input size)\n","EVENT_NUM = 65                          # num unique event types\n","DROP_RATE = 0.3                         # Default dropout rate\n","conn = sqlite3.connect(EVENT_DATA_PATH)"]},{"cell_type":"markdown","metadata":{"id":"bJ7OjhR59XYk"},"source":["* **df_drug** contains 572 kinds of drugs and their features\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1713150881215,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"aUtD-yqxWOuN","outputId":"a7f4357f-1b2f-47f7-b6fa-8884967f2277"},"outputs":[{"output_type":"stream","name":"stdout","text":["df_drug shape (572, 8)\n","----------------------------------------\n","Columns: ['index', 'id', 'target', 'enzyme', 'pathway', 'smile', 'name', 'category']\n","----------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 572 entries, 0 to 571\n","Data columns (total 8 columns):\n"," #   Column    Non-Null Count  Dtype \n","---  ------    --------------  ----- \n"," 0   index     572 non-null    int64 \n"," 1   id        572 non-null    object\n"," 2   target    572 non-null    object\n"," 3   enzyme    572 non-null    object\n"," 4   pathway   572 non-null    object\n"," 5   smile     572 non-null    object\n"," 6   name      572 non-null    object\n"," 7   category  572 non-null    object\n","dtypes: int64(1), object(7)\n","memory usage: 35.9+ KB\n","----------------------------------------\n","Sample:\n","{'index': 0, 'id': 'DB01296', 'target': 'P14780|Q00653|P01375|P01579|P33673', 'enzyme': 'P33261|P05181', 'pathway': 'hsa:4318|hsa:4791|hsa:7124|hsa:3458', 'smile': '9|10|14|18|19|20|178|181|283|284|285|286|299|308|332|338|339|340|341|344|345|346|347|351|352|365|366|367|380|393|405|406|528|563|566|567|571|582|592|614|615|617|637|638|639|643|661|662|663|679|680|681|682|683|689|690|691|701|703', 'name': 'Glucosamine', 'category': 'DBCAT000338|DBCAT002312|DBCAT002311|DBCAT000085|DBCAT000337|DBCAT002122'}\n","{'index': 1, 'id': 'DB09230', 'target': 'Q02641', 'enzyme': 'P08684', 'pathway': 'hsa:782', 'smile': '9|10|11|12|13|14|15|16|18|19|20|129|131|132|178|182|183|184|185|189|192|196|199|283|284|285|286|299|301|332|333|335|338|339|340|341|344|345|346|351|352|355|356|365|366|370|371|374|375|376|377|380|384|390|391|392|393|395|401|405|416|420|423|430|434|437|440|441|443|446|449|452|454|455|464|470|490|502|514|516|520|524|535|540|545|546|549|552|553|556|558|560|564|566|570|573|578|579|582|584|592|594|595|599|600|603|607|608|613|614|615|618|619|628|633|634|637|640|643|654|656|659|660|664|665|666|668|671|673|677|678|679|680|683|684|688|689|690|692|693|694|696|697|698|700|704|708|709|710|712|737|800', 'name': 'Azelnidipine', 'category': 'DBCAT000328|DBCAT003960|DBCAT000243|DBCAT000244|DBCAT000021|DBCAT003297|DBCAT001211|DBCAT001212|DBCAT002703|DBCAT000574|DBCAT003919|DBCAT002646|DBCAT005101|DBCAT000389|DBCAT000388|DBCAT000227|DBCAT003676'}\n","{'index': 2, 'id': 'DB05812', 'target': 'P05093', 'enzyme': 'P08684|Q06520|P10635|P10632|P05177|P33261|P11712', 'pathway': 'hsa:1586', 'smile': '9|10|11|12|14|18|143|147|178|179|182|183|184|185|186|192|199|283|284|285|286|308|332|333|334|335|339|341|344|345|346|351|352|355|356|358|365|366|370|371|372|373|374|376|384|387|390|396|403|406|416|418|430|434|435|441|442|445|446|447|449|453|464|470|472|482|490|491|495|502|506|516|520|521|523|524|530|538|539|540|545|546|549|552|555|556|564|570|571|576|577|578|582|584|585|592|595|599|600|603|607|608|613|617|618|628|633|634|637|640|641|656|657|660|664|665|668|677|678|679|680|683|688|689|696|697|698|699|708|709|710|711|712|776|777|797|818|839|860', 'name': 'Abiraterone', 'category': 'DBCAT000981|DBCAT000980|DBCAT000024|DBCAT002086|DBCAT000402|DBCAT004503|DBCAT002610|DBCAT000403|DBCAT002640|DBCAT004482|DBCAT000868|DBCAT002644|DBCAT003042|DBCAT000489|DBCAT002636|DBCAT004528|DBCAT000911|DBCAT002625|DBCAT000934|DBCAT003919|DBCAT003232|DBCAT002648|DBCAT004049|DBCAT002646|DBCAT003893|DBCAT004487|DBCAT000394|DBCAT005101|DBCAT003461|DBCAT002090|DBCAT000003|DBCAT004974|DBCAT000144|DBCAT002137|DBCAT000057|DBCAT003931|DBCAT002667|DBCAT000487|DBCAT000309'}\n","{'index': 3, 'id': 'DB01195', 'target': 'Q14524|P35499|Q12809', 'enzyme': 'P10635|P11712', 'pathway': 'hsa:6331|hsa:6329|hsa:3757', 'smile': '9|10|11|12|14|15|18|19|23|24|25|178|180|181|182|185|283|284|285|286|287|299|332|333|338|340|341|344|345|346|351|352|355|356|363|365|366|370|371|381|382|384|390|392|393|405|416|420|430|434|439|441|443|446|451|464|470|476|490|493|498|516|520|524|528|535|540|541|542|548|549|552|553|556|564|565|569|570|573|574|578|579|581|582|584|589|592|594|595|597|599|603|604|606|607|608|611|613|614|618|619|620|623|625|626|628|632|633|634|637|638|640|641|642|643|645|646|651|655|656|660|664|666|668|671|672|677|678|679|680|681|682|683|684|686|688|689|692|698|699|704|708|709|710|719|735|756|782|798|819', 'name': 'Flecainide', 'category': 'DBCAT003297|DBCAT003865|DBCAT002518|DBCAT002210|DBCAT000010|DBCAT002609|DBCAT004029|DBCAT000489|DBCAT004528|DBCAT000911|DBCAT004505|DBCAT002623|DBCAT004031|DBCAT000394|DBCAT005101|DBCAT003950|DBCAT003951|DBCAT004525|DBCAT003957|DBCAT003956|DBCAT000128|DBCAT002690|DBCAT003972|DBCAT005659|DBCAT002668|DBCAT004027|DBCAT000695|DBCAT003823|DBCAT000600|DBCAT000615'}\n","{'index': 4, 'id': 'DB00201', 'target': 'P30542|P29274|Q07343|P21817|BE0004922|P78527|O00329|P42336|P42338|BE0004914|Q13315', 'enzyme': 'P20815|P05177|P24462|P08684|P05181|P10632|P11712|P04798|Q16678|P10635', 'pathway': 'hsa:134|hsa:135|hsa:5142|hsa:6261|hsa:5591|hsa:5293|hsa:5290|hsa:5291|hsa:472', 'smile': '9|10|11|14|15|16|18|19|143|148|149|178|183|184|283|284|285|286|332|340|351|352|355|357|358|359|365|373|374|375|376|377|378|379|381|384|386|387|388|389|390|391|396|397|403|416|418|420|431|437|438|439|441|442|443|447|449|450|451|453|464|472|482|484|485|487|491|493|494|495|499|504|506|519|521|523|530|535|536|538|540|545|547|549|553|555|560|569|572|580|585|593|596|601|602|611|613|621|624|628|636|645|646|647|650|654|657|673|674', 'name': 'Caffeine', 'category': 'DBCAT000443|DBCAT003635|DBCAT002662|DBCAT002675|DBCAT000044|DBCAT000437|DBCAT004212|DBCAT000402|DBCAT002611|DBCAT004503|DBCAT002609|DBCAT002642|DBCAT002634|DBCAT002623|DBCAT002628|DBCAT003919|DBCAT002646|DBCAT000394|DBCAT005101|DBCAT002135|DBCAT002094|DBCAT002112|DBCAT000003|DBCAT004973|DBCAT002148|DBCAT000127|DBCAT000509|DBCAT002156|DBCAT002180|DBCAT000507|DBCAT000506|DBCAT000253|DBCAT000504|DBCAT003636|DBCAT002181|DBCAT001579|DBCAT001034'}\n"]}],"source":["df_drug = pd.read_sql('select * from drug;', conn)\n","print(\"df_drug shape\", df_drug.shape)\n","print(\"-\" * 40)\n","print(\"Columns:\", df_drug.columns.tolist())\n","print(\"-\" * 40)\n","df_drug.info()\n","print(\"-\" * 40)\n","print(\"Sample:\")\n","for index, row in df_drug.head().iterrows():\n","    print(row.to_dict())"]},{"cell_type":"markdown","metadata":{"id":"vaaIRJOo-AfB"},"source":["* **df_event** contains 65 unique event types\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713150881215,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"iyQdt7T6WOl7","outputId":"4daeb4b9-5e81-4002-ee70-d29acff945ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["df_event shape (65, 2)\n","----------------------------------------\n","Columns: ['event', 'number']\n","----------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 65 entries, 0 to 64\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   event   65 non-null     object\n"," 1   number  65 non-null     object\n","dtypes: object(2)\n","memory usage: 1.1+ KB\n","----------------------------------------\n","                                                    event number\n","count                                                  65     65\n","unique                                                 65     52\n","top     name may increase the vasodilatory activities ...     10\n","freq                                                    1      5\n","----------------------------------------\n","Sample:\n","{'event': 'The metabolism of name can be decreased when combined with name.', 'number': '19620'}\n","{'event': 'The risk or severity of adverse effects can be increased when name is combined with name.', 'number': '18992'}\n","{'event': 'The serum concentration of name can be increased when it is combined with name.', 'number': '11292'}\n","{'event': 'The serum concentration of name can be decreased when it is combined with name.', 'number': '4772'}\n","{'event': 'The therapeutic efficacy of name can be decreased when used in combination with name.', 'number': '2624'}\n"]}],"source":["df_event = pd.read_sql('select * from event_number;', conn)\n","print(\"df_event shape\", df_event.shape)\n","print(\"-\" * 40)\n","print(\"Columns:\", df_event.columns.tolist())\n","print(\"-\" * 40)\n","df_event.info()\n","print(\"-\" * 40)\n","print(df_event.describe(include='all'))\n","print(\"-\" * 40)\n","print(\"Sample:\")\n","for index, row in df_event.head().iterrows():\n","    print(row.to_dict())"]},{"cell_type":"markdown","metadata":{"id":"PXhiYeJX_PC2"},"source":["* **df_interaction** contains the 37,264 DDIs between the 572 kinds of drugs\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":357,"status":"ok","timestamp":1713150881569,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"we2jMYrgWObc","outputId":"dd22c507-968e-411c-bed9-c4ca54db8e9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["df_interaction shape (37264, 6)\n","----------------------------------------\n","Columns: ['index', 'id1', 'name1', 'id2', 'name2', 'interaction']\n","----------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 37264 entries, 0 to 37263\n","Data columns (total 6 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   index        37264 non-null  int64 \n"," 1   id1          37264 non-null  object\n"," 2   name1        37264 non-null  object\n"," 3   id2          37264 non-null  object\n"," 4   name2        37264 non-null  object\n"," 5   interaction  37264 non-null  object\n","dtypes: int64(1), object(5)\n","memory usage: 1.7+ MB\n","----------------------------------------\n","          index      id1       name1      id2       name2  \\\n","count   37264.0    37264       37264    37264       37264   \n","unique      NaN      560         560      558         558   \n","top         NaN  DB01118  Amiodarone  DB01149  Nefazodone   \n","\n","                                              interaction  \n","count                                               37264  \n","unique                                              37264  \n","top     The metabolism of Nicardipine can be decreased...  \n","----------------------------------------\n","Sample:\n","{'index': 0, 'id1': 'DB12001', 'name1': 'Abemaciclib', 'id2': 'DB01118', 'name2': 'Amiodarone', 'interaction': 'The risk or severity of adverse effects can be increased when Abemaciclib is combined with Amiodarone'}\n","{'index': 1, 'id1': 'DB12001', 'name1': 'Abemaciclib', 'id2': 'DB11901', 'name2': 'Apalutamide', 'interaction': 'The serum concentration of Abemaciclib can be decreased when it is combined with Apalutamide'}\n","{'index': 2, 'id1': 'DB12001', 'name1': 'Abemaciclib', 'id2': 'DB00673', 'name2': 'Aprepitant', 'interaction': 'The serum concentration of Abemaciclib can be increased when it is combined with Aprepitant'}\n","{'index': 3, 'id1': 'DB12001', 'name1': 'Abemaciclib', 'id2': 'DB00289', 'name2': 'Atomoxetine', 'interaction': 'The metabolism of Abemaciclib can be decreased when combined with Atomoxetine'}\n","{'index': 4, 'id1': 'DB12001', 'name1': 'Abemaciclib', 'id2': 'DB00188', 'name2': 'Bortezomib', 'interaction': 'The metabolism of Abemaciclib can be decreased when combined with Bortezomib'}\n"]}],"source":["df_interaction = pd.read_sql('select * from event;', conn)\n","print(\"df_interaction shape\", df_interaction.shape)\n","print(\"-\" * 40)\n","print(\"Columns:\", df_interaction.columns.tolist())\n","print(\"-\" * 40)\n","df_interaction.info()\n","print(\"-\" * 40)\n","print(df_interaction.describe(include='all').head(3))\n","print(\"-\" * 40)\n","print(\"Sample:\")\n","for index, row in df_interaction.head().iterrows():\n","    print(row.to_dict())"]},{"cell_type":"markdown","metadata":{"id":"OtMaflGFSz5i"},"source":["\n","* **df_extraction** contains the interactions after NLP process was run on the list of drugs to parse extract key information about the interactions. Note, this was already done as data preprocessing step in original DDIMDL paper and was not performed CNN-DDI paper. Hence, we simply load from the event.db database.\n","\n","    * Mechanism: The biological effect of the drugs, such as effects on metabolism,\n","    serum concentration, or therapeutic efficacy.\n","    * Action: The change that occurs as a result of the interaction, typically an increase or decrease in the effect of one or both drugs.\n","    * Drug A: The drug whose efficacy is affected by the interaction.\n","    * Drug B: The other drug involved in the interaction."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1713150881570,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"WUit9BUVBFgl","outputId":"852ff12b-0059-42d7-e873-88832feb2b3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["df_extraction shape (37264, 5)\n","----------------------------------------\n","Columns: ['index', 'mechanism', 'action', 'drugA', 'drugB']\n","----------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 37264 entries, 0 to 37263\n","Data columns (total 5 columns):\n"," #   Column     Non-Null Count  Dtype \n","---  ------     --------------  ----- \n"," 0   index      37264 non-null  int64 \n"," 1   mechanism  37264 non-null  object\n"," 2   action     37264 non-null  object\n"," 3   drugA      37264 non-null  object\n"," 4   drugB      37264 non-null  object\n","dtypes: int64(1), object(4)\n","memory usage: 1.4+ MB\n","----------------------------------------\n","          index       mechanism    action        drugA       drugB\n","count   37264.0           37264     37264        37264       37264\n","unique      NaN              57         4          561         558\n","top         NaN  The metabolism  increase  Atomoxetine  Amiodarone\n","----------------------------------------\n","Sample:\n","{'index': 0, 'mechanism': 'The risk or severity of adverse effects', 'action': 'increase', 'drugA': 'Abemaciclib', 'drugB': 'Amiodarone'}\n","{'index': 1, 'mechanism': 'The serum concentration', 'action': 'decrease', 'drugA': 'Abemaciclib', 'drugB': 'Apalutamide'}\n","{'index': 2, 'mechanism': 'The serum concentration', 'action': 'increase', 'drugA': 'Abemaciclib', 'drugB': 'Aprepitant'}\n","{'index': 3, 'mechanism': 'The metabolism', 'action': 'decrease', 'drugA': 'Abemaciclib', 'drugB': 'Atomoxetine'}\n","{'index': 4, 'mechanism': 'The metabolism', 'action': 'decrease', 'drugA': 'Abemaciclib', 'drugB': 'Bortezomib'}\n"]}],"source":["df_extraction = pd.read_sql('select * from extraction;', conn)\n","print(\"df_extraction shape\", df_extraction.shape)\n","print(\"-\" * 40)\n","print(\"Columns:\", df_extraction.columns.tolist())\n","print(\"-\" * 40)\n","df_extraction.info()\n","print(\"-\" * 40)\n","print(df_extraction.describe(include='all').head(3))\n","print(\"-\" * 40)\n","print(\"Sample:\")\n","for index, row in df_extraction.head().iterrows():\n","    print(row.to_dict())\n","\n","mechanism = df_extraction['mechanism']\n","action = df_extraction['action']\n","drugA = df_extraction['drugA']\n","drugB = df_extraction['drugB']"]},{"cell_type":"markdown","metadata":{"id":"_QgbqgrFGFmc"},"source":["### Data Prepocessing"]},{"cell_type":"markdown","metadata":{"id":"FQOP0iCeZFLQ"},"source":[" For each drug, a binary feature matrix is constructed based on the presence (1) or absence (0) of specific features. This is done by first extracting all unique features across all drugs for a given feature type and then populating a matrix where each row corresponds to a drug and each column to a feature. If a drug has a particular feature, the corresponding cell in the matrix is marked as 1, otherwise as 0.\n"," The Jaccard Similarity between the feature vectors of drugs is computed to measure the similarity between drugs based on their features.\n"," PCA is applied to reduce the dimensionality of the similiariy matrix.\n"," The reduced-dimensionality feature vectors are then used as input for the CNN model to predict DDIs. For each drug-drug pair, the feature vectors of the two drugs are combined to form a single input vector to the model."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1713150881570,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"jh1Af_S_SWnt"},"outputs":[],"source":["def create_feature_set_name(feature_list):\n","    #TODO set_name not needed ,remove\n","    \"\"\"\n","    Create a feature set name from a list of features.\n","    source code adapted from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L338\n","\n","    Args:\n","        feature_list (list): A list of feature names.\n","\n","    Returns:\n","        tuple: A tuple containing two elements:\n","            - feature_name (str): A string of concatenated feature names separated by \"+\".\n","            - set_name (str): A string of feature names separated by \"+\" with a trailing \"+\".\n","    \"\"\"\n","    feature_name = \"+\".join(feature_list)\n","    set_name = \"\"\n","    for feature in feature_list:\n","        set_name = feature + \"+\"\n","    set_name = set_name[:-1]\n","    return feature_name, set_name"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":475,"status":"ok","timestamp":1713150882042,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"Z7K-od0f_Vkk"},"outputs":[],"source":["# Define the Jaccard Similarity function\n","def Jaccard(matrix):\n","    \"\"\"\n","    Calculate the Jaccard similarity between rows of a given matrix.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L89\n","\n","    Args:\n","        matrix (array-like): A 2D array (or matrix) where each row represents a set in binary form (1s and 0s),\n","                             with 1 indicating the presence of an element in the set, and 0 indicating absence.\n","\n","    Returns:\n","        numpy.matrix: A matrix of Jaccard similarity scores between each pair of rows in the input matrix.\n","    \"\"\"\n","    matrix = np.mat(matrix)\n","    numerator = matrix * matrix.T\n","    denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n","    return numerator / denominator"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":665,"status":"ok","timestamp":1713150882705,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"NeeTXDM1_A9_"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","def feature_vector(feature_name, df, vector_size):\n","    \"\"\"\n","    Generates a feature vector for each drug based on the specified feature using Jaccard Similarity and PCA reduction.\n","\n","    This function first constructs a feature matrix for drugs based on the presence or absence of specific features\n","    (e.g., targets, enzymes). It then computes the Jaccard Similarity matrix for these drugs and finally reduces the\n","    dimensionality of this matrix to the specified vector size using PCA.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L86\n","\n","    Args:\n","        feature_name (str): The name of the feature column in the DataFrame `df` to be used for generating feature vectors.\n","        df (DataFrame): A pandas DataFrame containing drug data. Each row corresponds to a drug, and the specified\n","                        feature column contains feature identifiers separated by '|'.\n","        vector_size (int): The target number of dimensions for the feature vectors after PCA reduction.\n","\n","    Returns:\n","        numpy.ndarray: A 2D array where each row represents the reduced-dimensionality feature vector for a drug.\n","    \"\"\"\n","    all_feature = []\n","    drug_list = np.array(df[feature_name]).tolist()\n","    # Extract unique features from the feature column for all drugs\n","    for features in drug_list:\n","        for each_feature in features.split('|'):\n","            if each_feature not in all_feature:\n","                all_feature.append(each_feature)\n","\n","    # Initialize a feature matrix with zeros\n","    feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n","    # Construct a DataFrame for easier manipulation\n","    df_feature = DataFrame(feature_matrix, columns=all_feature)\n","\n","    # Populate the feature matrix with 1s where a drug has a particular feature\n","    for i, features in enumerate(drug_list):\n","        for each_feature in features.split('|'):\n","            df_feature.at[i, each_feature] = 1\n","\n","    # Compute the Jaccard Similarity matrix\n","    sim_matrix = Jaccard(np.array(df_feature))\n","    sim_matrix = np.asarray(sim_matrix)\n","\n","    # Apply PCA to reduce the dimensionality of the similarity matrix\n","    pca = PCA(n_components=vector_size)\n","    pca.fit(sim_matrix)\n","    reduced_sim_matrix = pca.transform(sim_matrix)\n","\n","    return reduced_sim_matrix"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1713150882705,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"87u5NzBi07mD"},"outputs":[],"source":["def prepare(df_drug, feature_list, vector_size, mechanism, action, drugA, drugB):\n","    \"\"\"\n","    Prepares feature vectors and labels for drug interaction events.\n","\n","    This function processes a list of drug interaction features to generate corresponding\n","    feature vectors and labels. It assigns a unique numerical label to each unique\n","    mechanism-action pair and constructs feature vectors for each drug based on the provided\n","    features.\n","\n","    source code adapted from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L50\n","\n","    Args:\n","        df_drug (DataFrame): DataFrame containing drug data, including names.\n","        feature_list (list): List of features to be included in the feature vector.\n","        vector_size (int): The size of the feature vector for each feature.\n","        mechanism (Series): Series of mechanisms involved in drug interactions.\n","        action (Series): Series of actions resulting from drug interactions.\n","        drugA (Series): Series of primary drugs involved in interactions.\n","        drugB (Series): Series of secondary drugs involved in interactions.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - new_feature (numpy.ndarray): Array of feature vectors for drug interactions.\n","            - new_label (numpy.ndarray): Array of labels for each drug interaction event.\n","            - event_num (int): The total number of unique interaction events.\n","    \"\"\"\n","    d_label = {}\n","    d_feature = {}\n","    d_event = []\n","\n","    # Concatenate mechanism and action to form unique interaction events\n","    for i in range(len(mechanism)):\n","        d_event.append(mechanism[i] + \" \" + action[i])\n","\n","    # Count occurrences of each event and assign a unique label\n","    count = {}\n","    for event in d_event:\n","        count[event] = count.get(event, 0) + 1\n","    sorted_events = sorted(count.items(), key=lambda x: x[1], reverse=True)\n","    for i, (event, _) in enumerate(sorted_events):\n","        d_label[event] = i\n","\n","    # Initialize a zero vector for feature aggregation\n","    vector = np.zeros((len(df_drug['name']), 0), dtype=float)\n","\n","    # Aggregate feature vectors for each feature in the list\n","    for feature in feature_list:\n","        vector = np.hstack((vector, feature_vector(feature, df_drug, vector_size)))\n","\n","    # Map drug names to their feature vectors\n","    for i, name in enumerate(df_drug['name']):\n","        d_feature[name] = vector[i]\n","\n","    # Construct feature vectors and labels for each interaction event\n","    new_feature = []\n","    new_label = []\n","    for i in range(len(d_event)):\n","        combined_feature = np.hstack((d_feature[drugA[i]], d_feature[drugB[i]]))\n","        new_feature.append(combined_feature)\n","        new_label.append(d_label[d_event[i]])\n","\n","    new_feature = np.array(new_feature)\n","    new_label = np.array(new_label)\n","    event_num = len(sorted_events)\n","\n","    return (new_feature, new_label, event_num)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1713150882705,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"2NFBDhGr0O_Q"},"outputs":[],"source":["def construct_feature_matrix(feature_list, df_drug, vector_size, mechanism, action, drugA, drugB):\n","    \"\"\"\n","    Processes each feature in the given feature list by preparing and accumulating their corresponding new features.\n","\n","    source code adapted from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L358\n","\n","    Args:\n","        feature_list (list): A list of features to be processed.\n","        df_drug (DataFrame): The DataFrame containing drug data.\n","        vector_size (int): The size of the vector to be used in preparation.\n","        mechanism, action, drugA, drugB: Additional parameters required by the `prepare` function.\n","\n","    Returns:\n","        tuple: A tuple containing three elements:\n","            - all_matrix (list): A list of feature matrices, where each matrix corresponds to a feature in the feature_list.\n","            - new_label (numpy.ndarray): The label matrix corresponding to the feature matrices.\n","            - event_num (int): The total number of unique events or classes in the label matrix.\n","    \"\"\"\n","    all_matrix = []\n","    for feature in feature_list:\n","        #print(feature)\n","        new_feature, new_label, event_num = prepare(df_drug, [feature], vector_size, mechanism, action, drugA, drugB)\n","        all_matrix.append(new_feature)\n","    return all_matrix, new_label, event_num"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1713150882706,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"},"user_tz":300},"id":"oNR-rA4Z7cGE","outputId":"f40c2ef1-e179-4ac6-96d4-a5d10fb4c292"},"outputs":[{"output_type":"stream","name":"stdout","text":["smile+target+enzyme+category\n"]}],"source":["feature_list = [\"smile\", \"target\", \"enzyme\", \"category\"]\n","feature_name, set_name = create_feature_set_name(feature_list)\n","print(feature_name)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"fA08HKhiREvM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713150887993,"user_tz":300,"elapsed":5295,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}},"outputId":"0c2d5488-5137-4ced-ab42-a8eb1b62af28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of feature_matrix: (4, 37264, 1144)\n","Number of label: 37264\n","Number of events: 65\n"]}],"source":["feature_matrix, new_label, event_num = construct_feature_matrix(feature_list, df_drug, VECTOR_SIZE, mechanism, action, drugA, drugB)\n","print(f\"Shape of feature_matrix: ({len(feature_matrix)}, {len(feature_matrix[0])}, {len(feature_matrix[0][0])})\")\n","print(f\"Number of label: {new_label.shape[0]}\")\n","print(f\"Number of events: {event_num}\")"]},{"cell_type":"markdown","metadata":{"id":"6k7X794Q0Ndw"},"source":["# Model\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"avjiPFXZ9FkH","executionInfo":{"status":"ok","timestamp":1713150887994,"user_tz":300,"elapsed":5,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Dropout, Input, Activation, BatchNormalization\n","from tensorflow.keras.layers import Conv1D, Flatten, Add\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model"]},{"cell_type":"markdown","metadata":{"id":"bGL1g6FiUhla"},"source":["**DDIMDL Model**\n","\n","The model consists of an input layer, two dense layers with ReLU activation and dropout for regularization,\n","and an output layer with softmax activation for multi-class classification. It uses the Adam optimizer and Categorial Cross Entropy as the loss function.\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"ZklRzrkEHCIE","executionInfo":{"status":"ok","timestamp":1713150887994,"user_tz":300,"elapsed":5,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["def DNN(vector_size=VECTOR_SIZE, event_num=EVENT_NUM, drop_rate=DROP_RATE):\n","    \"\"\"\n","    A deep neural network (DNN) model for predicting drug-drug interactions.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L35\n","\n","    Args:\n","        vector_size (int): The size of the input feature vector.\n","        event_num (int): The number of unique interaction events (classes) to predict.\n","        drop_rate (float): The dropout rate for regularization.\n","\n","    Returns:\n","        model: A compiled Keras model ready for training.\n","    \"\"\"\n","    # Define the input layer\n","    train_input = Input(shape=(vector_size * 2,), name='Inputlayer')\n","     # First dense layer with 512 units and ReLU activation\n","    train_in = Dense(512, activation='relu')(train_input)\n","    train_in = BatchNormalization()(train_in)\n","    train_in = Dropout(drop_rate)(train_in)\n","    # Second dense layer with 256 units and ReLU activation\n","    train_in = Dense(256, activation='relu')(train_in)\n","    train_in = BatchNormalization()(train_in)\n","    train_in = Dropout(drop_rate)(train_in)\n","    # Output dense layer with 'event_num' units for classification\n","    train_in = Dense(event_num)(train_in)\n","    # Softmax activation to convert logits to probabilities for multi-class classification\n","    out = Activation('softmax')(train_in)\n","    # Create the model\n","    model = Model(inputs=train_input, outputs=out)\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"nRiPlge5HFs8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713150888343,"user_tz":300,"elapsed":353,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}},"outputId":"a67ffb88-d80d-4b4c-9d29-611a6c31f4d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Inputlayer (InputLayer)     [(None, 1144)]            0         \n","                                                                 \n"," dense (Dense)               (None, 512)               586240    \n","                                                                 \n"," batch_normalization (Batch  (None, 512)               2048      \n"," Normalization)                                                  \n","                                                                 \n"," dropout (Dropout)           (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 256)               1024      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 65)                16705     \n","                                                                 \n"," activation (Activation)     (None, 65)                0         \n","                                                                 \n","=================================================================\n","Total params: 737345 (2.81 MB)\n","Trainable params: 735809 (2.81 MB)\n","Non-trainable params: 1536 (6.00 KB)\n","_________________________________________________________________\n"]}],"source":["DDIMDL_model = DNN()\n","DDIMDL_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"XVsT4XreURyW"},"source":["**CNN-DDI Model**\n","\n","The model architecture is based on the CNN-DDI method described in the paper\n","It includes an input layer, multiple convolutional layers with LeakyReLU activation, a residual block, and fully connected layers with a softmax layer for multi-class classification. It uses the Adam optimizer and Categorial Cross Entropy as the loss function.\n","\n","![Table 5.png](https://drive.google.com/uc?export=view&id=1n3Ls-m1vIuZqEyUs-1m5wZzQKVsBAGs2)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"AftoOASFHbZD","executionInfo":{"status":"ok","timestamp":1713150888343,"user_tz":300,"elapsed":12,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["def CNN_DDI(vector_size=VECTOR_SIZE, event_num=EVENT_NUM):\n","    \"\"\"\n","    Convolutional Neural Network (CNN) model for predicting drug-drug interactions (DDIs).\n","\n","    Implementation based on \"CNN-DDI: a learning-based method for predicting drug–drug interactions using convolution neural networks.\"\n","    https://doi.org/10.1186/s12859-022-04612-2\n","\n","    Args:\n","        vector_size (int): The size of the input feature vector for each drug.\n","        event_num (int): The number of unique DDI event types to predict.\n","\n","    Returns:\n","        model: A compiled Keras model ready for training.\n","    \"\"\"\n","    # Define the input layer\n","    inputs = Input(shape=(vector_size, 2), name='InputLayer')\n","\n","    # Convolutional layers as specified in the paper\n","    conv1 = Conv1D(filters=64, kernel_size=3, strides=1, padding='same')(inputs)\n","    conv1 = LeakyReLU(alpha=0.2)(conv1)\n","\n","    conv2 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv1)\n","    conv2 = LeakyReLU(alpha=0.2)(conv2)\n","\n","    # Residual block starts\n","    conv3_1 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv2)\n","    conv3_1 = LeakyReLU(alpha=0.2)(conv3_1)\n","\n","    conv3_2 = Conv1D(filters=128, kernel_size=3, strides=1, padding='same')(conv3_1)\n","    conv3_2 = LeakyReLU(alpha=0.2)(conv3_2)\n","\n","    # Add the input of the residual block (conv2) to its output (conv3_2)\n","    res_out = Add()([conv2, conv3_2])\n","    # Residual block ends\n","\n","    conv4 = Conv1D(filters=256, kernel_size=3, strides=1, padding='same')(res_out)\n","    conv4 = LeakyReLU(alpha=0.2)(conv4)\n","\n","    # Flatten the output of the last convolutional layer\n","    flatten = Flatten()(conv4)\n","\n","    # Fully connected layers\n","    fc1 = Dense(267, activation='relu')(flatten)\n","\n","    fc2 = Dense(event_num)(fc1)  # Assuming 'num_classes' is the number of DDI event types\n","    out = Activation('softmax')(fc2)\n","\n","    # Create the model\n","    model = Model(inputs=inputs, outputs=out)\n","\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"WiPO_muFHv_K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713150888344,"user_tz":300,"elapsed":13,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}},"outputId":"932f6b49-d37e-4ed6-89c1-cfdfa7c0d36b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," InputLayer (InputLayer)     [(None, 572, 2)]             0         []                            \n","                                                                                                  \n"," conv1d (Conv1D)             (None, 572, 64)              448       ['InputLayer[0][0]']          \n","                                                                                                  \n"," leaky_re_lu (LeakyReLU)     (None, 572, 64)              0         ['conv1d[0][0]']              \n","                                                                                                  \n"," conv1d_1 (Conv1D)           (None, 572, 128)             24704     ['leaky_re_lu[0][0]']         \n","                                                                                                  \n"," leaky_re_lu_1 (LeakyReLU)   (None, 572, 128)             0         ['conv1d_1[0][0]']            \n","                                                                                                  \n"," conv1d_2 (Conv1D)           (None, 572, 128)             49280     ['leaky_re_lu_1[0][0]']       \n","                                                                                                  \n"," leaky_re_lu_2 (LeakyReLU)   (None, 572, 128)             0         ['conv1d_2[0][0]']            \n","                                                                                                  \n"," conv1d_3 (Conv1D)           (None, 572, 128)             49280     ['leaky_re_lu_2[0][0]']       \n","                                                                                                  \n"," leaky_re_lu_3 (LeakyReLU)   (None, 572, 128)             0         ['conv1d_3[0][0]']            \n","                                                                                                  \n"," add (Add)                   (None, 572, 128)             0         ['leaky_re_lu_1[0][0]',       \n","                                                                     'leaky_re_lu_3[0][0]']       \n","                                                                                                  \n"," conv1d_4 (Conv1D)           (None, 572, 256)             98560     ['add[0][0]']                 \n","                                                                                                  \n"," leaky_re_lu_4 (LeakyReLU)   (None, 572, 256)             0         ['conv1d_4[0][0]']            \n","                                                                                                  \n"," flatten (Flatten)           (None, 146432)               0         ['leaky_re_lu_4[0][0]']       \n","                                                                                                  \n"," dense_3 (Dense)             (None, 267)                  3909761   ['flatten[0][0]']             \n","                                                          1                                       \n","                                                                                                  \n"," dense_4 (Dense)             (None, 65)                   17420     ['dense_3[0][0]']             \n","                                                                                                  \n"," activation_1 (Activation)   (None, 65)                   0         ['dense_4[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 39337303 (150.06 MB)\n","Trainable params: 39337303 (150.06 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["CNN_DDI_model = CNN_DDI()\n","CNN_DDI_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"c19i5hk3dt-X"},"source":["**Other ML Models**\n","\n","The other models compared against in the paper include\n"," random forest (RF), gradient boosting decision tree (GBDT),\n","logistic regression (LR) and K-nearest neighbor (KNN)."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"VAFXIhNFLPje","executionInfo":{"status":"ok","timestamp":1713150888664,"user_tz":300,"elapsed":323,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def logistic_regression_pred(X_train, Y_train, X_test):\n","    #Logistic Regression model\n","    # original source code from: https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L182\n","    model = LogisticRegression()\n","    model.fit(X_train, Y_train)\n","    pred = model.predict_proba(X_test)\n","    return pred\n","\n","def random_forest_pred(X_train, Y_train, X_test):\n","    #Random Forest Classifier with 100 trees\n","    # original source code from: https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L172\n","    model = RandomForestClassifier(n_estimators=100)\n","    model.fit(X_train, Y_train)\n","    pred = model.predict_proba(X_test)\n","    return pred\n","\n","def gbdt_pred(X_train, Y_train, X_test):\n","    #Gradient Boosting Decision Tree (GBDT) model\n","    # original source code from: https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L174\n","    model = GradientBoostingClassifier()\n","    model.fit(X_train, Y_train)\n","    pred = model.predict_proba(X_test)\n","    return pred\n","\n","def svm_pred(X_train, Y_train, X_test):\n","    #Support Vector Machine (SVM) model with probability estimates\n","    # original source code from: https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L176\n","    model = SVC(probability=True)\n","    model.fit(X_train, Y_train)\n","    pred = model.predict_proba(X_test)\n","    return pred\n","\n","def knn_pred(X_train, Y_train, X_test):\n","    #K-Nearest Neighbors (KNN) classifier with 4 neighbors\n","    # original source code from: https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L180\n","    model = KNeighborsClassifier(n_neighbors=4)\n","    model.fit(X_train, Y_train)\n","    pred = model.predict_proba(X_test)\n","    return pred"]},{"cell_type":"markdown","metadata":{"id":"DyRT88QyPHjV"},"source":["# Metrics"]},{"cell_type":"markdown","metadata":{"id":"OrFYB3_XN5lS"},"source":["CNN-DDI is multi-class classification problem. For evaluation,\n","accuracy (ACC), area under the precision–recall-curve (AUPR), area\n","under the ROC curve (AUC), F1 score and Precision are used as\n","evaluation metrics. Please note,  AUPR & AUC are micro-averaged\n","while other metrics are macro-averaged. This is done since the data classes are imbalanced.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"n35sL2sWPBfy","executionInfo":{"status":"ok","timestamp":1713150888664,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["from sklearn.metrics import auc\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import precision_recall_curve\n","\n","def multiclass_precision_recall_curve(y_true, y_score):\n","    \"\"\"\n","    Calculate the precision-recall curve for the first class in a multiclass classification problem.\n","\n","    This function reshapes the true labels and predicted scores if necessary, and then computes\n","    the precision-recall curve for the first class. It is designed to work with one-vs-rest\n","    multiclass classification models where each class is treated independently.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L265\n","\n","    Args:\n","      y_true: array-like of shape (n_samples,) or (n_samples, n_classes)\n","              True binary labels or binary label indicators for each class.\n","      y_score: array-like of shape (n_samples,) or (n_samples, n_classes)\n","               Target scores, can either be probability estimates of the positive class,\n","               confidence values, or non-thresholded measure of decisions.\n","\n","    Returns:\n","      precision: array of shape (n_thresholds + 1,)\n","                 Precision values such that element i is the precision of predictions with\n","                 score >= thresholds[i] and the last element is 1.\n","      recall: array of shape (n_thresholds + 1,)\n","              Recall values such that element i is the recall of predictions with\n","              score >= thresholds[i] and the last element is 0.\n","      pr_thresholds: array of shape (n_thresholds,)\n","                     Decreasing thresholds on the decision function used to compute\n","                     precision and recall.\n","    \"\"\"\n","    # Ensure the true labels and scores are 1D arrays, reshaping if necessary\n","    y_true = y_true.ravel()\n","    y_score = y_score.ravel()\n","    # Reshape y_true and y_score to 2D arrays if they are 1D\n","    if y_true.ndim == 1:\n","        y_true = y_true.reshape((-1, 1))\n","    if y_score.ndim == 1:\n","        y_score = y_score.reshape((-1, 1))\n","    # Extract the true labels and scores for the first class\n","    y_true_c = y_true.take([0], axis=1).ravel()\n","    y_score_c = y_score.take([0], axis=1).ravel()\n","    # Compute precision, recall, and thresholds for the first class\n","    precision, recall, pr_thresholds = precision_recall_curve(y_true_c, y_score_c)\n","    return (precision, recall, pr_thresholds)\n","\n","\n","def roc_aupr_score(y_true, y_score, average=\"macro\"):\n","    \"\"\"\n","    Calculate the Area Under the Precision-Recall Curve (AUPR) for binary or multiclass classification.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L278\n","\n","    Args:\n","      y_true: array-like of shape (n_samples,) or (n_samples, n_classes)\n","              True binary labels or binary label indicators for multiclass classification.\n","      y_score: array-like of shape (n_samples,) or (n_samples, n_classes)\n","               Target scores, can either be probability estimates of the positive class,\n","               confidence values, or non-thresholded measure of decisions.\n","      average: string, ['micro', 'macro', 'binary'] (default='macro')\n","               If 'binary', calculate AUPR for binary classification problems.\n","               If 'micro', calculate metrics globally by considering each element of the label\n","               indicator matrix as a label.\n","               If 'macro', calculate metrics for each label, and find their unweighted mean.\n","\n","    Returns:\n","      AUPR score: float\n","                  Area Under the Precision-Recall Curve (AUPR) score.\n","    \"\"\"\n","    # Function to calculate AUPR for binary classification\n","    def _binary_roc_aupr_score(y_true, y_score):\n","        precision, recall, pr_thresholds = precision_recall_curve(y_true, y_score)\n","        return auc(recall, precision)\n","\n","    # Function to handle averaging of AUPR scores for multiclass classification\n","    def _average_binary_score(binary_metric, y_true, y_score, average):  # y_true= y_one_hot\n","        if average == \"binary\":\n","            return binary_metric(y_true, y_score)\n","        # Handle micro averaging\n","        if average == \"micro\":\n","            y_true = y_true.ravel()\n","            y_score = y_score.ravel()\n","        # Ensure y_true and y_score are 2D arrays\n","        if y_true.ndim == 1:\n","            y_true = y_true.reshape((-1, 1))\n","        if y_score.ndim == 1:\n","            y_score = y_score.reshape((-1, 1))\n","        n_classes = y_score.shape[1]\n","        score = np.zeros((n_classes,))\n","        # Calculate AUPR for each class and average\n","        for c in range(n_classes):\n","            y_true_c = y_true.take([c], axis=1).ravel()\n","            y_score_c = y_score.take([c], axis=1).ravel()\n","            score[c] = binary_metric(y_true_c, y_score_c)\n","        return np.average(score)\n","\n","    return _average_binary_score(_binary_roc_aupr_score, y_true, y_score, average)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"cvCjVFnsPRrb","executionInfo":{"status":"ok","timestamp":1713150888665,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["from sklearn.preprocessing import label_binarize\n","\n","def evaluate(pred_type, pred_score, y_test, event_num):\n","    \"\"\"\n","    Evaluate the performance of predictions for multi-class classification.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L203\n","\n","    Args:\n","        pred_type (array-like): Predicted labels for each sample.\n","        pred_score (array-like): Predicted scores or probabilities for each class for each sample.\n","        y_test (array-like): True labels for each sample.\n","        event_num (int): Number of distinct events or classes.\n","\n","    Returns:\n","        list: A list containing two numpy arrays. The first array contains overall evaluation metrics for the model,\n","              and the second array contains evaluation metrics for each class.\n","    \"\"\"\n","    # Define the number of evaluation metrics for overall performance\n","    all_eval_type = 11\n","    # Initialize an array to store overall evaluation metrics\n","    result_all = np.zeros((all_eval_type, 1), dtype=float)\n","    # Define the number of evaluation metrics for each class\n","    each_eval_type = 6\n","    # Initialize an array to store evaluation metrics for each class\n","    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n","    # Convert true labels to one-hot encoding\n","    y_one_hot = label_binarize(y_test, classes=np.arange(event_num))\n","    # Convert predicted labels to one-hot encoding\n","    pred_one_hot = label_binarize(pred_type, classes=np.arange(event_num))\n","\n","    # Calculate precision and recall for multi-class classification\n","    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n","\n","    # Calculate overall evaluation metrics\n","    result_all[0] = accuracy_score(y_test, pred_type)\n","    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n","    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n","    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n","    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n","    result_all[5] = f1_score(y_test, pred_type, average='micro')\n","    result_all[6] = f1_score(y_test, pred_type, average='macro')\n","    result_all[7] = precision_score(y_test, pred_type, average='micro')\n","    result_all[8] = precision_score(y_test, pred_type, average='macro')\n","    result_all[9] = recall_score(y_test, pred_type, average='micro')\n","    result_all[10] = recall_score(y_test, pred_type, average='macro')\n","\n","    # Calculate evaluation metrics for each event type\n","    for i in range(event_num):\n","        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n","        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n","                                          average=None)\n","        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n","                                         average=None)\n","        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n","                                    average='binary')\n","        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n","                                           average='binary')\n","        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n","                                        average='binary')\n","    # Return the overall and per-class evaluation metrics\n","\n","    return [result_all, result_eve]"]},{"cell_type":"markdown","metadata":{"id":"xQ721yPpPXgz"},"source":["# Training & Evaluation"]},{"cell_type":"markdown","metadata":{"id":"_Xc6VZfdOrLS"},"source":["For the CNN-DDI reproduction, 5-fold cross validation was used to evaluate the model according to the paper. The data was randomly split into 5 subsets, with 4 used for training and 1 for testing in each fold. The final metrics reported are the average across the 5 folds.\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"tWGQh-VlPdRM","executionInfo":{"status":"ok","timestamp":1713150888665,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["from sklearn.model_selection import KFold\n","\n","def get_index(label_matrix, event_num, seed, CV):\n","    \"\"\"\n","    Generate indices for K-fold cross-validation for each class in the label matrix.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L117\n","\n","    Args:\n","        label_matrix (array-like): A 1D array containing the class labels for each sample.\n","        event_num (int): The number of unique events or classes.\n","        seed (int): Random seed for reproducibility of the shuffle in KFold.\n","        CV (int): The number of folds for the K-fold cross-validation.\n","\n","    Returns:\n","        numpy.ndarray: An array of indices indicating the fold number for each sample.\n","    \"\"\"\n","    # Initialize an array to store the fold indices for all samples\n","    index_all_class = np.zeros(len(label_matrix))\n","    # generate fold indices for each class\n","    for j in range(event_num):\n","        # Find the indices of samples belonging to the current class\n","        index = np.where(label_matrix == j)\n","        # Initialize KFold with the specified number of splits, shuffling, and random seed\n","        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n","        # Initialize a counter for the fold number\n","        k_num = 0\n","        # Get train and test indices for each fold\n","        for train_index, test_index in kf.split(range(len(index[0]))):\n","            # Assign the fold number to the corresponding samples in the overall index array\n","            index_all_class[index[0][test_index]] = k_num\n","            # Increment the fold number\n","            k_num += 1\n","    # Return the array of fold indices\n","    return index_all_class"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"4dEjMcnWPiLM","executionInfo":{"status":"ok","timestamp":1713150888665,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV, num_epochs, batch_size, patience=10):\n","    \"\"\"\n","    Perform K-fold cross-validation to evaluate the performance of specified classifiers on a DDI prediction task.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L130\n","\n","    Args:\n","        feature_matrix (array-like or list of array-like): Feature matrix or list of feature matrices for training and testing.\n","        label_matrix (array-like): Label matrix corresponding to the true class labels.\n","        clf_type (str): Type of classifier to be evaluated. Supported types include 'DDIMDL', 'CNN_DDI', 'RF', 'GBDT', 'SVM', 'FM', 'KNN', and logistic regression.\n","        event_num (int): Number of unique events or classes.\n","        seed (int): Random seed for reproducibility of the shuffle in KFold.\n","        CV (int): Number of folds for the K-fold cross-validation.\n","        num_epochs (int): Number of training epochs for neural network models (DDIMDL and CNN_DDI).\n","        batch_size (int): Batch size used during training of neural network models (DDIMDL and CNN_DDI).\n","        patience (int, optional): Number of epochs with no improvement after which training will be stopped for early stopping. Defaults to 10.\n","\n","    Returns:\n","        tuple: A tuple containing two numpy arrays. The first array contains overall evaluation metrics for the model,\n","               and the second array contains evaluation metrics for each class.\n","    \"\"\"\n","    # Initialize arrays to store evaluation results\n","\n","    all_eval_type = 11\n","    result_all = np.zeros((all_eval_type, 1), dtype=float)\n","    each_eval_type = 6\n","    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n","    y_true = np.array([])\n","    y_pred = np.array([])\n","    y_score = np.zeros((0, event_num), dtype=float)\n","    # Generate indices for K-fold cross-validation\n","    index_all_class = get_index(label_matrix, event_num, seed, CV)\n","    matrix = []\n","    if type(feature_matrix) != list:\n","        matrix.append(feature_matrix)\n","        feature_matrix = matrix\n","    for k in range(CV):\n","        # Split data into training and testing sets based on fold index\n","        train_index = np.where(index_all_class != k)\n","        test_index = np.where(index_all_class == k)\n","        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n","        # Train and predict with each feature matrix (in case of multiple feature matrices)\n","        for i in range(len(feature_matrix)):\n","            x_train = feature_matrix[i][train_index]\n","            x_test = feature_matrix[i][test_index]\n","            y_train = label_matrix[train_index]\n","            # one-hot encoding training labels\n","            y_train_one_hot = np.array(y_train)\n","            y_train_one_hot = (np.arange(event_num) == y_train[:, None]).astype(dtype='float32')\n","            y_test = label_matrix[test_index]\n","            # one-hot encoding of testing labels\n","            y_test_one_hot = np.array(y_test)\n","            y_test_one_hot = (np.arange(event_num) == y_test[:, None]).astype(dtype='float32')\n","            if clf_type == 'DDIMDL':\n","                dnn = DNN()\n","                # print_memory_usage()\n","                early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=0, mode='auto')\n","                dnn.fit(x_train, y_train_one_hot, batch_size=batch_size, epochs=batch_size, validation_data=(x_test, y_test_one_hot),\n","                        callbacks=[early_stopping])\n","                pred += dnn.predict(x_test)\n","            elif clf_type == 'CNN_DDI':\n","                x_train_reshaped = x_train.reshape(-1, VECTOR_SIZE, 2)\n","                x_test_reshaped = x_test.reshape(-1, VECTOR_SIZE, 2)\n","                cnn_ddi = CNN_DDI()\n","                # print_memory_usage()\n","                early_stopping = EarlyStopping(monitor='val_loss', patience=patience, verbose=0, mode='auto')\n","                cnn_ddi.fit(x_train_reshaped, y_train_one_hot, batch_size=batch_size, epochs=num_epochs, validation_data=(x_test_reshaped, y_test_one_hot),\n","                            callbacks=[early_stopping])\n","                pred += cnn_ddi.predict(x_test_reshaped)\n","            elif clf_type == 'RF':\n","                pred += random_forest_pred(x_train, y_train, x_test)\n","            elif clf_type == 'GBDT':\n","                pred += gbdt_pred(x_train, y_train, x_test)\n","            elif clf_type == 'SVM':\n","                pred += svm_pred(x_train, y_train, x_test)\n","            elif clf_type == 'FM':\n","                pred += gbdt_pred(x_train, y_train, x_test)\n","            elif clf_type == 'KNN':\n","                pred += knn_pred(x_train, y_train, x_test)\n","            elif clf_type == 'LR':\n","                pred += logistic_regression_pred(x_train, y_train, x_test)\n","        # Aggregate predictions from all feature matrices and determine predicted class\n","        pred_score = pred / len(feature_matrix)\n","        pred_type = np.argmax(pred_score, axis=1)\n","        # Accumulate true labels, predicted labels, and predicted scores\n","        y_true = np.hstack((y_true, y_test))\n","        y_pred = np.hstack((y_pred, pred_type))\n","        y_score = np.row_stack((y_score, pred_score))\n","    # Evaluate the performance of the classifier\n","    result_all, result_eve = evaluate(y_pred, y_score, y_true, event_num)\n","    return result_all, result_eve"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"-ymqRTa6WzOj","executionInfo":{"status":"ok","timestamp":1713150888665,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["def save_result(feature_name, result_type, clf_type, result, base_path=BASE_PATH):\n","    \"\"\"\n","    Save the evaluation results of a classifier into a CSV file.\n","\n","    original source code from:\n","    https://github.com/YifanDengWHU/DDIMDL/blob/master/DDIMDL.py#L321\n","\n","    Args:\n","        feature_name (str): Name of the feature set used for the classifier.\n","        result_type (str): Type of result being saved (e.g., 'accuracy', 'precision').\n","        clf_type (str): Type of classifier (e.g., 'CNN-DDI', 'RF').\n","        result (list): A list of evaluation results to be saved.\n","        base_path (str, optional): Base path for saving the result file. Defaults to BASE_PATH.\n","\n","    Returns:\n","        int: 0 on successful execution.\n","    \"\"\"\n","    # Construct the file path by combining base path, feature name, result type, and classifier type\n","    file_path = base_path + feature_name + '_' + result_type + '_' + clf_type + '.csv'\n","    with open(file_path, \"w\", newline='') as csvfile:\n","        writer = csv.writer(csvfile)\n","        for i in result:\n","            writer.writerow(i)\n","    return 0"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"qrpmBZA_TfT7","executionInfo":{"status":"ok","timestamp":1713150888665,"user_tz":300,"elapsed":4,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}}},"outputs":[],"source":["def run_cross_validation(clf_list, featureName, featureMatrix, labelMatrix, event_num=EVENT_NUM, seed=0, num_folds=5, num_epochs=100, batch_size=128):\n","    \"\"\"\n","    run the cross-validation for the list of classifiers and save the results.\n","\n","    Args:\n","        clf_list (list): list of classifier model names\n","        featureName (str): A descriptive name for the feature set used, which will be included in the result filenames.\n","        featureMatrix (array-like): The matrix of features used for training and testing the classifiers.\n","        labelMatrix (array-like): The matrix of labels corresponding to the featureMatrix.\n","        event_num (int): The number of unique event types to predict. Defaults to EVENT_NUM.\n","        seed (int): The random seed for reproducibility of cross-validation splits. Defaults to 0.\n","        num_folds (int): The number of folds for K-fold cross-validation. Defaults to 5.\n","        num_epochs (int): The number of epochs for training each classifier. Defaults to 100.\n","        batch_size (int): The batch size for training each classifier. Defaults to 128.\n","\n","    Returns:\n","        dict: Two dictionaries containing the overall and per-event evaluation results for each classifier.\n","    \"\"\"\n","    result_all = {}\n","    result_eve = {}\n","    all_matrix = featureMatrix\n","    new_label = labelMatrix\n","    start = time.perf_counter()\n","    for clf in clf_list:\n","      clf_start = time.perf_counter()\n","      print(f\"running cross validation for {clf}\")\n","      # Perform cross-validation using the specified classifier\n","      all_result, each_result = cross_validation(all_matrix, new_label, clf, event_num, seed, num_folds, num_epochs, batch_size)\n","      # Save the cross-validation results to CSV files\n","      save_result(featureName, 'all', clf, all_result)\n","      save_result(featureName, 'each', clf, each_result)\n","      result_all[clf] = all_result\n","      result_eve[clf] = each_result\n","      print(f\"time used for {clf}:\", time.perf_counter() - clf_start)\n","    print(\"Total time used:\", time.perf_counter() - start)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qZF0FmH-k7A","executionInfo":{"status":"ok","timestamp":1713151081004,"user_tz":300,"elapsed":192342,"user":{"displayName":"Hao Zhang","userId":"14315367380482273094"}},"outputId":"dcd2630d-af61-4edb-fc04-455d00014c78"},"outputs":[{"output_type":"stream","name":"stdout","text":["running cross validation for CNN_DDI\n","146/146 [==============================] - 21s 108ms/step - loss: 1.6217 - accuracy: 0.5335 - val_loss: 0.9514 - val_accuracy: 0.7074\n","583/583 [==============================] - 4s 6ms/step\n","146/146 [==============================] - 16s 92ms/step - loss: 1.3257 - accuracy: 0.6208 - val_loss: 0.8334 - val_accuracy: 0.7343\n","583/583 [==============================] - 3s 6ms/step\n","146/146 [==============================] - 15s 91ms/step - loss: 1.7541 - accuracy: 0.4959 - val_loss: 1.3579 - val_accuracy: 0.5899\n","583/583 [==============================] - 4s 6ms/step\n","146/146 [==============================] - 18s 110ms/step - loss: 1.2568 - accuracy: 0.6485 - val_loss: 0.6927 - val_accuracy: 0.7821\n","583/583 [==============================] - 4s 6ms/step\n","146/146 [==============================] - 18s 108ms/step - loss: 1.5623 - accuracy: 0.5542 - val_loss: 0.9060 - val_accuracy: 0.7283\n","582/582 [==============================] - 4s 6ms/step\n","146/146 [==============================] - 15s 93ms/step - loss: 1.3607 - accuracy: 0.6086 - val_loss: 0.8475 - val_accuracy: 0.7287\n","582/582 [==============================] - 4s 7ms/step\n","146/146 [==============================] - 15s 91ms/step - loss: 1.8193 - accuracy: 0.4898 - val_loss: 1.3810 - val_accuracy: 0.5837\n","582/582 [==============================] - 3s 6ms/step\n","146/146 [==============================] - 18s 106ms/step - loss: 1.3641 - accuracy: 0.6207 - val_loss: 0.7123 - val_accuracy: 0.7767\n","582/582 [==============================] - 4s 6ms/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["time used for CNN_DDI: 192.54022764400003\n","Total time used: 192.540324199\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["classifiers = [\"CNN_DDI\"]\n","num_folds = 2\n","num_epochs = 1\n","batch_size = 128\n","CV_seed = 0\n","\n","run_cross_validation(clf_list=classifiers,featureName=feature_name,featureMatrix=feature_matrix,labelMatrix=new_label,\n","                     event_num=event_num, seed=CV_seed, num_folds=num_folds, num_epochs=num_epochs, batch_size=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Results\n"]},{"cell_type":"markdown","metadata":{"id":"gxpy9q-Ipn0f"},"source":["### Overview\n","\n","The fundamental hypothesis of the paper is that adding category data as the feature can improve the performance of CNN-DDI model. In the paper Table 2, Table 3 and Table 4 are demonstrating the performance about CNN-DDI given various metrics, including ACC, AURP, AUC, F1, Precision, and Recall. We generate three tables, Table 2 Reproduction, Table 3 Reproduction, and Table 4 Reproduction, in our experiment to compare with the paper and verify author's arguments accordingly.\n","\n","###Table 2 Reproduction\n","\n","The goal of Table 2 is to test the effectiveness of adding certain features, especially for the drug category. Features T, P, E, and C are put into model training according to different combinations. Table 2 demonstrates that the performance is improved for CNN-DDI with four feature types. Adding the “category\" feature does provide noticeable improvement.\n","\n","Due to time constraints, we reproduced last 5 combinations and report the results in Table 2 Reproduction. In general, most results in Table 2 Reproduction have about 0~5% difference to the values of Table 2. But the results in the two tables follow similar patterns, demonstrating the successful reproduction for those metrics. The results in Table 2 Reproduction verify that adding \"category\" feature into feature list improves the model performance.\n","\n","![Table 2.png](https://drive.google.com/uc?export=view&id=1cD-GkXTKiNEwVONa5kzUf5jpVly2oJhx)\n","\n","\n","![Table 2 R.png](https://drive.google.com/uc?export=view&id=16rToBuMrW_eDXfOA2EHqpP8ojPWnbGY5)\n","\n","\n","###Table 4 Reproduction:\n","\n","Table 4 compares the outputs of CNN-DDI and DDIMDL with and without \"category\" feature included. Both models use the same source of data. Based on Table 4, it concludes that adding drug category as one of the features is effective.\n","\n","To verify this argument, the reproduced results are documented in Table 4 Reproduction. Again, most data are within 5% difference and follow similar patterns. The reproduction results support the author's argument.\n","However the author also claims that the CNN-DDI model outperforms the DDIMDL when using the same 4 features. We do not observe that to be the case as shown in last row of table 4 reporduction, the DDIMDL is slighty better than CNN-DDI result.\n","\n","![Table 4.png](https://drive.google.com/uc?export=view&id=1vTVHcCusQn0WMRqyI3E4_s_Fz6yW2CaO)\n","\n","\n","![Table 4 R.png](https://drive.google.com/uc?export=view&id=13zFOSsoLenVKDi7mRLMJA-7_VwZqc2Ul)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8EAWAy_LwHlV"},"source":["## Model comparison"]},{"cell_type":"markdown","metadata":{"id":"nlWh0fuu2UWz"},"source":["###Table 3 and Table 3 Reproduction:\n","\n","In the paper Table 3 compares the performance of CNN-DDI with model GBDT, RF, KNN, and LR. The data shows CNN-DDI has stronger performance over those other ML models.\n","\n","We reproduced all the results as Table 3 Reproduction except GBDT due to running issues. The results match the papers in a way that CNN-DDI model performs better than the rest of the models in all metrics.\n","\n","![Table 3.png](https://drive.google.com/uc?export=view&id=1aUzLZji-e5m8w1fwoFG_2ZB5fsBeMVtk)\n","\n","![Table 3 R.png](https://drive.google.com/uc?export=view&id=11SNLsWjDe9RqzCc90UUiDOieNMprl4AZ)\n","\n","###Ablation Plan:\n","\n","For ablation, we will evaluate the model's performance by individually removing drug categories, targets, pathways, and enzyme features. For model ablation, we plan to assess the impact of removing the residual block on the model performance and try different loss functions such as Binary Cross-Entropy Loss.\n","\n","Currently, we have tested the impact of removing features and residual block. The results indicate that the residual block has limited impact on the model’s performance. Moreover, training on all four features does have the highest performance, adding category feature is valid. We will test on different loss function in the next phases for Ablation.\n","![Ablation Plan.png](https://drive.google.com/uc?export=view&id=1XJY2IvPXVUr2mnNSjARSh8YUPzIsAtnk)\n"]},{"cell_type":"markdown","metadata":{"id":"qH75TNU71eRH"},"source":["# Discussion\n","\n","In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n","  * Make assessment that the paper is reproducible or not.\n","  * Explain why it is not reproducible if your results are kind negative.\n","  * Describe “What was easy” and “What was difficult” during the reproduction.\n","  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n","  * What will you do in next phase.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NmCbM-5W0RZp"},"source":["###Reproducibility Assessment\n","This paper is reproducible with certain efforts. The pattern of reproduced results matches well with the results in the paper. Most numbers are within reasonable value differences if not being the same, indicating the success of reproduction.\n","###Issues with Evalution\n","Even though we assert that this paper is reproducible, it is worth mentioning that the we cannot produce the evaluation results without running the entire cross-validation process. The data provided both in the reproduction tables and papers' tables is generated from the results of the training & evaluation process. If we try to save the model at the end of the cross-validation method and then load model and evaluate with the whole dataset, the performance drops significantly for some metrics.\n","\n","###Ease in Reproduction\n","The easy parts in reproduction come from those facts:\n","1. The structure of this CNN model is relatively simple and clear given the information from the paper.\n","2. Most of the data was available and data gathering methods are indicated.\n","3. This model is able to be run in the local machine with RXT 2080ti and complete the cross-validation in approximately 1 hour.\n","\n","###Difficulties in Reproduction\n","1. No access to the source code. This paper is about CNN-DDI while the GitHub repo provided is for an earlier paper for the DDIMDL model. While the available code from DDIMDL did provide the base for our team to work on, it is not guaranteed to be the same as the codebase from the authors of CNN-DDI paper. Even though the overall structure of this CNN model is described in the paper, we were still missing some critical information, such as the number of epochs, whether dropout was used, and the settngs of other hyperparameters that may impact the model performance. The lack of access to the full source code and these crucial details can be among the possible causes of any differences between reproduction data and paper data.\n","2. The provided dataset is incomplete. Adding drug category to the feature list can improve the model performance is the key statement of CNN-DDI paper. However, the category data was not available in the database of the DDIMDL. Thus, we gather the missing data and merge it with the existing dataset. However this data may have changed since the authors of the CNN-DDI paper accessed it.\n","3. Computing resources are limited.\n","It may sound contradictory since we just mentioned it is easy for this project to be able to run on the local machine. Initially, running on local machine fails due to speed and out-of-memory issue. Our team spent some effort switching between different versions of PyTorch and TensorFlow with various settings to make the running successful eventually. Now it takes 1 hour to train the model.\n","\n","###Suggestions for Enhancing Reproducibility\n","For authors of this paper, it would be appreciated if they can release the source code or respond to the technical questions in email about the settings of this CNN-DDI model. Only having source code of other paper can be challenging.\n","For future reproducers, we recommend going through the paper carefully, paying attention to the CNN model structure and understanding the authors' main goal so you are clear what to do. Also suggest using the DDIMDL's resource wisely since you can get hints about overall structure of your coding for CNN-DDI. There are many reusable parts including data processing, result evaluation, etc.\n","\n","###Next Phase\n","1.\tAdjusting model evaluation approaches. As stated above, currently when saving and evaluating the model with the whole dataset, prediction outcome gets lower values compared with the ones from the training process. Current assumptions are either we are not saving the model correctly and evaluated with the appropriate data input, or the trained model is not generalized well. Our team will investigate this problem.\n","2.  Add more data visualizations. Since most of the input data is grid-like text data we found it difficult to have fancy visualizations. We will try to add some for the similiarity matrix.\n","3.\tCompleting the reproduction. There are some results missing compared to the paper, including GBDT model comparision and doing all the different feature combinations for table 2. Our team will make the results section complete for final notebook submission.\n","4.\tFinishing the ablation plan. Our team has tested out different feature combinations and residual blocks for model performance. There are more items to be tested in the list such as different loss functions.\n"]},{"cell_type":"markdown","metadata":{"id":"fTYzflfyGoGt"},"source":["# Public Github Repo\n","\n","All source codes and data are available in our projects repo linked below\n","\n","https://github.com/abaldeo/CS598_DLH_Project/tree/CNN_DDI\n","https://github.com/abaldeo/CS598_DLH_Project/blob/CNN_DDI/DL4H_Team_31.ipynb"]},{"cell_type":"markdown","metadata":{"id":"SHMI2chl9omn"},"source":["# References\n","\n","1.  Zhang, C., Lu, Y. & Zang, T. CNN-DDI: a learning-based method for predicting drug–drug interactions using convolution neural networks. BMC Bioinformatics 23 (Suppl 1), 88 (2022). https://doi.org/10.1186/s12859-022-04612-2\n","\n","2. Deng, Y., Xu, X., Qiu, Y., Xia, J., Zhang, W., & Liu, S. (2020). A multimodal deep learning framework for predicting drug-drug interaction events. Bioinformatics (Oxford, England), 36(15), 4316–4322. https://doi.org/10.1093/bioinformatics/btaa501\n","\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"15DlNEbA9hcBygJHn6oZ04-DuNc-8MqY8","timestamp":1713151157838},{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1711585916361},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
